
# Confusions
1. derived closed form solution. *different* from GD

# Lecture

### Topics
1. **Regression** (predicting continuous values, like stock prices).  
2. **Linear Classifiers** (e.g., perceptrons for binary decisions).  
3. **Logistic Regression** (for probability-based classification).  
4. **Artificial Neural Networks** (multi-layer models inspired by the brain).  
5. **Unsupervised ML ‚Äì Clustering** (grouping data without labels, like K-Means).

---

## Information Theory

### Entropy 
	measures the amount of uncertainty in a Pr ditro
	think of entropy calculations in terms of bits: an entropy value for a unform distro of 4 attributes cannot be 1, since it dcannot represent the data. Instead it must be 2. 

use log_2 in order to match binary computation. 
$$
0*log_0 = 0
$$

### **Lecture Narrative: FIT5047 - Machine Learning Part 1 (Slide 46 Onward)**  

#### **Slide 46: Disorder is Bad, Homogeneity is Good**  
The lecture begins by emphasizing a core principle in machine learning: **homogeneity** (purity) in data partitions is desirable, while **disorder** (impurity) is problematic. This sets the stage for understanding how decision trees split data to maximize class purity.  

#### **Slides 47-50: Entropy ‚Äì Quantifying Uncertainty**  
- **Entropy** is introduced as a measure of uncertainty in a probability distribution.  
- Formula:  
  \[
  H(X) = -\sum_{i=1}^n \Pr(x_i) \log_2 \Pr(x_i)
  \]  
- Example: For a binary class (e.g., "Play ball" with 9 YES and 5 NO instances), entropy is calculated as:  
  \[
  H(S) = -\frac{9}{14} \log_2 \frac{9}{14} - \frac{5}{14} \log_2 \frac{5}{14} = 0.94
  \]  
- Key takeaway: Higher entropy = more disorder (uncertainty); lower entropy = purity.  

#### **Slides 51-54: Information Gain ‚Äì Splitting Criterion**  
- **Information Gain (IG)** measures the reduction in entropy after splitting on an attribute:  
  \[
  IG(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
  \]  
- Example: Splitting on *Wind* (values: Weak, Strong) yields IG = 0.048, while *Outlook* (Sunny, Overcast, Rain) gives IG = 0.246.  
- **Decision**: Split on the attribute with the highest IG (here, *Outlook*).  

#### **Slides 55-58: Building the Decision Tree**  
1. **Root Split**: Choose *Outlook* (highest IG).  
   - Branches: Sunny (mixed YES/NO), Overcast (pure YES), Rain (mixed).  
2. **Recursive Splitting**:  
   - For *Sunny*, further split on *Humidity* (high ‚Üí NO, normal ‚Üí YES).  
   - For *Rain*, split on *Wind* (strong ‚Üí NO, weak ‚Üí YES).  
3. **Stopping Condition**: Stop when no further IG or purity is achieved.  

#### **Slides 59-61: Overfitting in Decision Trees**  
- **Problem**: Complex trees may overfit noise in training data, harming generalization.  
- **Solutions**:  
  - **Pre-pruning**: Stop growth early (e.g., minimum samples per leaf).  
  - **Post-pruning**: Grow full tree, then prune nodes that don‚Äôt improve validation performance.  
  - **Regularization**: Penalize tree complexity (e.g., limit depth).  

#### **Slides 62-65: Handling Continuous Attributes & Rule Extraction**  
- **Continuous Values**: Split points are chosen to maximize IG (e.g., *Humidity ‚â§ 75*).  
- **Rule Extraction**: Convert tree paths to IF-THEN rules (e.g., "IF Outlook=Sunny AND Humidity=High ‚Üí NO").  

#### **Slides 66-78: Na√Øve Bayes Classifier**  
- **Bayes‚Äô Theorem**:  
  \[
  \Pr(C|V) = \alpha \Pr(V|C) \Pr(C)
  \]  
- **Assumption**: Features are conditionally independent given the class.  
- **Parameter Estimation**:  
  - MLE for discrete features (count frequencies).  
  - Normal distribution for continuous features.  
- **Smoothing (ELE)**: Addresses zero probabilities (e.g., Laplace smoothing).  

#### **Slides 79-88: k-Nearest Neighbors (k-NN) & Ensembles**  
- **k-NN**: Classify based on majority vote/mean of *k* closest training instances.  
  - Distance metrics: Euclidean (continuous), Jaccard (categorical).  
- **Ensemble Methods**:  
  - **Bagging** (Bootstrap Aggregating): Reduce variance (e.g., Random Forests).  
  - **Boosting**: Iteratively reweight misclassified samples (e.g., AdaBoost).  

#### **Key Takeaways**  
1. **Decision Trees**: Splits based on IG to maximize purity; prone to overfitting.  
2. **Na√Øve Bayes**: Probabilistic, fast, but assumes feature independence.  
3. **k-NN**: Lazy learner; sensitive to distance metrics and *k*.  
4. **Ensembles**: Combine models to improve robustness (bias-variance trade-off).  

---  
**Next Lecture**: Machine Learning Part 2 (e.g., SVMs, Neural Networks, and deeper ensemble techniques).  

This segment bridges theory (entropy, IG) to practical algorithms (DTs, Na√Øve Bayes), highlighting trade-offs and best practices (e.g., pruning, smoothing). The narrative aligns with the slides‚Äô progression, ensuring clarity for students.

### **Pages 4‚Äì7: Regression**

### motivation

![[Pasted image 20250519121009.png|400]]

There is a set of data 

want to find an objective function -> get the sum squared - model's prediction.





- **What is Regression?**  
  - Predicts a continuous output (e.g., stock prices) from input features (e.g., company profit).  
  - Example: Given data points \((x_1, y_1), ..., (x_m, y_m)\), fit a line \(h_w(x) = w_1x + w_0\) to minimize prediction error.  


for one line the accumulated error between the line and points is minimized among the entire domain in question


- **Loss Function**: Measures error as the squared difference between predicted and actual values:  
  \[
  \text{Loss} = \sum (y_j - (w_1x_j + w_0))^2
  \]
- **Gradient Descent**: An iterative method to find optimal weights (\(w_0, w_1\)) by adjusting them in the direction that reduces loss.
![[Pasted image 20250519122716.png]]


---

## Why do  square the values?

**Squaring ensures smooth, continuous derivatives**:
- The function¬†f(x)=x2f(x)=x2¬†has a simple derivative¬†f‚Ä≤(x)=2xf‚Ä≤(x)=2x, making optimization (e.g., gradient descent) stable.

### **Pages 8‚Äì14: Gradient Descent (GD)**
- **How GD Works**:  
  1. Start with random weights.  
  2. Compute the gradient (slope) of the loss function.  
  3. Update weights:  
     \[
     w_i \leftarrow w_i - \alpha \cdot \frac{\partial \text{Loss}}{\partial w_i}
     \]  
     (\(\alpha\) = learning rate, controls step size).  
  4. Repeat until convergence.  
- **Example**: Fitting a line to 3 data points (Pages 8‚Äì9).  
- **Multivariate Case**: Extends to multiple features (Page 15).

---

### **Pages 16‚Äì21: Linear Classifiers**
- **Goal**: Separate data into classes using a linear boundary (e.g., spam vs. not spam).  
- **Perceptron Rule**: Updates weights based on misclassifications:  
  \[
  w_i \leftarrow w_i + \alpha (y - h_w(x)) \cdot x_i
  \]  
  - If prediction \(h_w(x)\) is wrong, adjust weights to reduce error.  
- **Limitations**:  
  - *Only works for linearly separable data.*  
  - *Predictions are "hard" (0 or 1), not probabilistic.*
	  - *this makes the training very sensitive, and the oscillation is more extreme. Convergence more difficult*

### Note that Naive Bayes is a Linear Classifier

	Due to it being linear in a log space
## Holy Grail of ML!
$$
\text{How to design te model to learn Optimally}
$$

---

consider the function:
![[Pasted image 20250519130141.png]]

	(logistic function = sigmoid function)

We can use this *soft activation function* for applying gradient descent:
![[Pasted image 20250519130326.png]]

allows us to directly compute the gradient.




### **Pages 22‚Äì27: Logistic Regression**
- **Logistic Function**: Maps inputs to probabilities (0‚Äì1):  
  \[
  h_w(x) = \frac{1}{1 + e^{-w \cdot x}}
  \]  
- **Key Idea**: Instead of thresholding, output the probability of belonging to a class.  
- **Training**: Uses gradient descent to minimize log-loss (Pages 25‚Äì26).  
- **Advantage over Perceptron**: Smooth, probabilistic outputs.

---

### **Pages 28‚Äì44: Artificial Neural Networks (ANNs)**
- **Inspiration**: Mimics neurons in the brain.  
- **Basic Unit (McCulloch-Pitts Neuron)**:  
  - Takes weighted inputs, applies an activation function (e.g., sigmoid).  
- **Multi-Layer Networks**:  
  - **Hidden Layers**: Intermediate layers between input and output.  
  - **Backpropagation**: Adjusts weights by propagating errors backward (Pages 44‚Äì46).  
- **Example**: Spam filter with input features (e.g., word counts) and output classes (spam/ham).
- **Feed Forward Networks**: connections made in only one direction. 

---

### **Pages 48‚Äì71: Unsupervised Learning ‚Äì Clustering (K-Means)**

### What is Clustering

	is simply a set of data, and this is the first step towards data mining


- **Goal**: Group similar data points without labels.  
- **K-Means Algorithm**:  
  1. Pick \(K\) initial centroids (randomly).  
  2. Assign each point to the nearest centroid.  
  3. Update centroids as the mean of assigned points.  
  4. Repeat until convergence.  
- **Key Challenges**:  
  - Choosing \(K\) (number of clusters).  
  - Sensitive to outliers and initial centroids.  
- **Example**: Clustering 2D points (Pages 60‚Äì68).

---

### **Pages 72‚Äì73: Tools & Further Reading**
- **WEKA**: A software toolkit for ML (includes K-Means, neural networks, etc.).  
- **Textbook References**: Russell & Norvig‚Äôs *AI: A Modern Approach* covers core concepts.  

---

### **Key Difficult Concepts Clarified**
1. **Gradient Descent**: Imagine rolling a ball down a hill‚Äîit naturally moves toward the lowest point (minimum loss). The learning rate (\(\alpha\)) controls step size.  
2. **Backpropagation**: Like correcting mistakes on a test‚Äîyou adjust weights based on how much they contributed to the error.  
3. **K-Means**: Think of grouping customers by purchasing behavior‚Äîsimilar customers cluster together.  

Let me know if you‚Äôd like deeper explanations on any section!

---
### **Comprehensive Narrative: From Supervised Learning Foundations to Advanced Models**  

---

#### **Phase 1: Foundations of Supervised Learning**  
*(Slides: Fundamentals, Decision Trees, Naive Bayes, k-NN, Ensembles)*  

**Narrative**:  
*"We begin with the core goal of supervised learning: generalizing from labeled examples. Decision Trees (DTs) offer transparency‚Äîsplitting data into branches like a flowchart‚Äîbut their greediness leads to overfitting. Naive Bayes (NB) sidesteps this with probability theory, assuming feature independence for efficiency, though this 'naive' assumption limits its ability to model interactions. k-Nearest Neighbors (k-NN) abandons assumptions entirely, relying on similarity, but scales poorly and falters with high-dimensional data. Ensembles like Random Forests and AdaBoost emerge to harmonize these models, reducing variance (bagging) and bias (boosting). Yet, these methods struggle with continuous outputs and complex boundaries, prompting the need for regression and neural approaches."*  

**Key Transitions**:  
- **From DTs to NB**: When interpretability meets scalability.  
- **From NB to k-NN**: When probabilistic simplicity fails to capture feature relationships.  
- **From k-NN to Ensembles**: When single-model limitations demand collective intelligence.  

---

#### **Phase 2: Regression and Linear Models**  
*(Slides: Regression, Linear Classifiers, Logistic Regression)*  

**Narrative**:  
*"For predicting continuous values (e.g., stock prices), we turn to regression. Univariate linear regression fits a line by minimizing squared error, but real-world problems often involve multiple features (multivariate regression). Gradient descent optimizes weights iteratively, yet linear models are rigid‚Äîthey can‚Äôt classify non-linear data. Linear classifiers (perceptrons) introduce thresholds for binary decisions, but their 'hard' outputs lack probabilistic nuance and fail on non-separable data. Logistic regression bridges this gap with the sigmoid function, estimating class probabilities smoothly. However, all linear models share a critical flaw: they assume linear relationships. When data spirals or clusters irregularly, we need non-linear transformations‚Äîenter neural networks."*  

**Key Transitions**:  
- **From Regression to Perceptrons**: Extending prediction to classification.  
- **From Perceptrons to Logistic Regression**: Softening thresholds into probabilities.  
- **Limitation**: Linearity assumptions break for complex patterns.  

---

#### **Phase 3: Neural Networks and Beyond**  
*(Slides: Neural Networks, Clustering)*  

**Narrative**:  
*"Neural networks (NNs) overcome linearity constraints by stacking layers of weighted transformations (neurons). Each neuron applies a non-linear activation (e.g., sigmoid), enabling the model to learn intricate patterns. Backpropagation propagates errors backward to adjust weights, but NNs demand large data and computational power. Meanwhile, unsupervised learning (e.g., K-Means clustering) groups unlabeled data by similarity, revealing hidden structures. Yet clustering requires predefined ‚ÄòK‚Äô and struggles with outliers, mirroring supervised learning‚Äôs need for careful tuning. Together, these methods form a toolkit where trade-offs‚Äîinterpretability vs. power, supervision vs. discovery‚Äîguide the choice of approach."*  

**Key Transitions**:  
- **From Logistic Regression to NNs**: When non-linearity is essential.  
- **From Supervised to Unsupervised**: When labels are scarce but patterns abound.  

---

### **Unified Conceptual Evolution**  

1. **Simple ‚Üí Complex**:  
   - *DTs/NB/k-NN*: Transparent but limited.  
   - *Linear Models*: Efficient but rigid.  
   - *NNs/Ensembles*: Powerful but opaque.  

2. **Assumptions ‚Üí Flexibility**:  
   - NB assumes feature independence; k-NN and NNs drop this.  
   - Linear models assume additive relationships; NNs model interactions.  

3. **Trade-Offs Drive Innovation**:  
   - **Interpretability**: DTs > Logistic Regression > NNs.  
   - **Scalability**: NB > k-NN > Ensembles.  
   - **Generality**: Linear models fail where NNs excel.  

**Example Workflow**:  
*"To classify handwritten digits: Start with a DT for interpretability. If performance plateaus, switch to an ensemble (Random Forest). For probabilistic confidence, use logistic regression. When the data reveals non-linear boundaries (e.g., spirals), deploy a neural network. If labels are missing, cluster the data first with K-Means to inform supervision."*  

---

### **Why This Progression Matters**  
- **Real-World Implications**:  
  - Medical diagnosis: Logistic regression for probabilistic outcomes, NNs for image analysis.  
  - Finance: Regression for stock trends, ensembles for risk modeling.  
- **Theoretical Insights**:  
  - Bias-variance tradeoff underpins all choices (e.g., pruning DTs vs. adding NN layers).  
  - The "no free lunch" theorem: No model is universally best‚Äîcontext dictates the tool.  

**Final Note**:  
*"Machine learning is a mosaic of compromises. Understanding how models evolve‚Äîfrom the simplicity of linear regression to the depth of neural networks‚Äîequips us to navigate these trade-offs, crafting solutions tailored to the problem‚Äôs constraints."*  

Let me know if you'd like to explore specific applications (e.g., "How would this apply to self-driving cars?") or dive deeper into mathematical foundations! b 





## ML in story form

### **Lecture Narrative: FIT5047 - Machine Learning Part 1 ‚Äì A Story of Learning from Data**  

#### **Prologue: The Quest for Knowledge**  
Our journey begins in the realm of **Machine Learning**, where programs learn from data like apprentices gleaning wisdom from experience. The lecture opens with a warning‚Äîcopyrighted knowledge from Monash University‚Äîreminding us that great power (here, AI) comes with great responsibility.  

#### **Chapter 1: The Foundations**  
We meet our protagonist: **Inductive Learning**, the art of deriving general rules from specific examples. Like a detective piecing together clues, ML algorithms learn from labeled data (*supervised learning*), uncover hidden patterns (*unsupervised learning*), or improve through trial and error (*reinforcement learning*).  

- 
- **Data Types**: The lifeblood of ML.  
  - *Training Data*: The apprentice‚Äôs textbook.  
  - *Validation Data*: The practice exam.  
  - *Test Data*: The final challenge‚Äînever peek!  

#### **Chapter 2: The Perils of Overfitting**  
A dark force emerges: **Overfitting**, where a model memorizes training data like a student who rote-learns without understanding. It aces practice questions but fails the real test. Our heroes‚Äî*regularization* and *validation sets*‚Äîstep in to keep models generalizable.  

- **Ockham‚Äôs Razor**: "The simplest explanation is best." A complex model might fit noise, but a simpler one often generalizes.  

#### **Chapter 3: The Tools of the Trade**  
We embark on practical quests:  

1. **Decision Trees** ‚Äì The branching paths of wisdom:  
   - **Entropy**: A measure of chaos in data. High entropy = confusion; low entropy = purity.  
   - **Information Gain**: The "gold" earned by splitting data (e.g., splitting *Outlook* reduces entropy the most).  
   - **Pruning**: Cutting away overgrown branches (complexity) to avoid overfitting.  

2. **Na√Øve Bayes** ‚Äì The probabilistic sage:  
   - Uses *Bayes‚Äô Theorem* to predict classes, assuming features are independent (a "na√Øve" but effective simplification).  
   - Fights the *Sparse Data Problem* with *smoothing*, ensuring no zero probabilities derail predictions.  

3. **k-Nearest Neighbors (k-NN)** ‚Äì The democratic classifier:  
   - Asks nearby data points (neighbors) to vote on a new instance‚Äôs class.  
   - Uses *Euclidean distance* for continuous features, *Jaccard* for categorical ones.  

4. **Ensemble Methods** ‚Äì The council of elders:  
   - *Bagging* (Random Forests): Many trees vote, reducing variance.  
   - *Boosting* (AdaBoost): Learns from mistakes, focusing on misclassified data.  

#### **Chapter 4: The Battle of Bias and Variance**  
A climactic duel unfolds:  
- **Bias**: The error from oversimplifying (e.g., linear model for complex data).  
- **Variance**: The error from overcomplicating (e.g., a tree that memorizes noise).  
- **The Balance**: Great models tread the golden path between the two.  

#### **Epilogue: The Road Ahead**  
As the lecture concludes, we glimpse the horizon: *Machine Learning Part 2*, where deeper mysteries (SVMs, Neural Networks) await. But for now, our heroes‚ÄîDecision Trees, Na√Øve Bayes, and k-NN‚Äîstand ready to tackle real-world challenges, from spam filtering to medical diagnoses.  

**Moral of the Story**:  
- *Learn from data, but don‚Äôt memorize it.*  
- *Simplicity often beats complexity.*  
- *Always validate‚Äînever trust training performance alone.*  

---  
**Final Slide**: *"The end? No, the journey is just beginning."*  
*(Cue dramatic music for Lecture 7b.)*  

---  
### **Why This Narrative Works**  
1. **Engagement**: Framed as a hero‚Äôs journey, with challenges (overfitting), tools (algorithms), and mentors (Ockham‚Äôs Razor).  
2. **Clarity**: Complex concepts (entropy, bias-variance) are analogized to relatable scenarios (apprentices, democracy).  
3. **Flow**: Mirrors the lecture‚Äôs structure while weaving a cohesive story.  

This approach transforms technical content into an epic tale, making it memorable and engaging for students.



---

# Lab
## weather.nominal data

### Predictor variable: Play
=== Run information ===

Scheme:       weka.classifiers.trees.J48 -C 0.25 -M 2
Relation:     weather.symbolic
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    evaluate on training data

=== Classifier model (full training set) ===

#### J48 pruned tree
------------------

outlook = sunny
|   humidity = high: no (3.0)
|   humidity = normal: yes (2.0)
outlook = overcast: yes (4.0)
outlook = rainy
|   windy = TRUE: no (2.0)
|   windy = FALSE: yes (3.0)

Number of Leaves  : 	5
Size of the tree : 	8
Time taken to build model: 0.01 seconds

=== Evaluation on training set ===

Time taken to test model on training data: 0 seconds

=== Summary === **this uses the whole training set**

Correctly Classified Instances          14              100      %
Incorrectly Classified Instances         0                0      %
Kappa statistic                          1     
Mean absolute error                      0     
Root mean squared error                  0     
Relative absolute error                  0      %
Root relative squared error              0      %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

                TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     yes
                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     no
Weighted Avg.    1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     **this shows a perfect prediction**

=== Confusion Matrix === **errors calculate using cross vlaidation**
 a b   <-- classified as
 9 0 | a = yes
 0 5 | b = no

**we can use cross validation to isolate training segments for the model**

| params  | A   | B   | C   | D   | E   |
| ------- | --- | --- | --- | --- | --- |
| trial 1 | NT  | T   | T   | T   | T   |
| trial 2 | T   | NT  | T   | T   | T   |
| trial 3 |     |     |     |     |     |
**using cross validation, we can choose hyperparameters**

#### can visualize related attributes
	main menu> "visualize all" bottom right

![[Pasted image 20250513122047.png|]]

want to know if var is impotant or not for play

if outlook > Pr play = yes, since outlook is *overcast*
however, if it is *sunny* and *high*, then Pr play = no

So, we can get discrete conditional Pr outcomes of objective sing software's decision trees.



![[Pasted image 20250513123226.png]]

![[Pasted image 20250513123302.png]]

### On using training data set in classifier

method for testing whether model is working or not.

can right-click result and select *visualize tree*

can use hyperparameters  here will result in different models
![[Pasted image 20250513124830.png]]

1. 
2. humidity has a large distribution, and small Pr that play = no


![[Pasted image 20250513124932.png]]
we can see that the number of decision nodes for outlook=overcast is 4


this i an application of naive bayes
=== Run information ===

Scheme:       weka.classifiers.bayes.NaiveBayes 
Relation:     weather.symbolic
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    evaluate on training data

=== Classifier model (full training set) ===

**Naive Bayes Classifier**
**Note: epsilon value = 1**
                Class
Attribute         yes     no
               (0.63) (0.38)
=============================
outlook
  sunny            3.0    4.0
  overcast         5.0    1.0
  rainy            4.0    3.0
  [total]         12.0    8.0

temperature
  hot              3.0    3.0
  mild             5.0    3.0
  cool             4.0    2.0
  [total]         12.0    8.0

humidity
  high             4.0    5.0
  normal           7.0    2.0
  [total]         11.0    7.0

windy
  TRUE             4.0    4.0
  FALSE            7.0    3.0
  [total]         11.0    7.0



Time taken to build model: 0 seconds

=== Evaluation on training set ===

Time taken to test model on training data: 0 seconds

=== Summary ===

Correctly Classified Instances          13               92.8571 %
Incorrectly Classified Instances         1                7.1429 %
Kappa statistic                          0.8372
Mean absolute error                      0.2917
Root mean squared error                  0.3392
Relative absolute error                 62.8233 %
Root relative squared error             70.7422 %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
                 1.000    0.200    0.900      1.000    0.947      0.849    0.922     0.947     yes
                 0.800    0.000    1.000      0.800    0.889      0.849    0.911     0.911     no
Weighted Avg.    0.929    0.129    0.936      0.929    0.926      0.849    0.918     0.934     

=== Confusion Matrix ===

 a b   <-- classified as
 9 0 | a = yes
 1 4 | b = no

X_i: independent random var
**outlook is overcast** = 4
**temp hot** = 4
.
.
.
so we have Pr(play=yes) = 9/14

**final value being 0.94**

now to calculate entropy of play we need 
H(play|humidity=high)
Pr(p=y | H=h) 
Pr(play=no | H=h)
we know that Pr of humidity = high is 1/2
and Pr of humidity = n is 1/2
*basic use of log rules aplied*

can get the numbers in tabular form through main menu> edit:

![[Pasted image 20250513132603.png]]

data
orrectly Classified Instances          13               92.8571 %
Incorrectly Classified Instances         1                7.1429 %
Kappa statistic                          0.8372
Mean absolute error                      0.2917
Root mean squared error                  0.3392
Relative absolute error                 62.8233 %
Root relative squared error             70.7422 %
Total Number of Instances               14     

---

### Looking at Naive Bayes ML Model
objective: want to maximize conditional Pr of P(X|Y), but knowledge of the of attribute is required. 
so we assume that 


![[Pasted image 20250513130836.png]]

using the MLE: get 
probability of playing ball, given that the outlook is overcast, the temperature is hot, the humidity is normal, and it is not windy.
*written as *
$$\begin{align}
[P(p=y | outlook=overcast, t=h, H=n, W=n)] \\ \\
estimated P(p=y)(P(o=o | p=y)P(t=h | p=y)\\P(h=n | p=y)P(w=n | p=y)
\end{align}
$$
**GPT Explanation of NB**

### **Naive Bayes ‚Äì Explained Like You‚Äôre 12!** üç™üê∂  

Imagine you‚Äôre trying to guess if a **cookie is chocolate chip (üç™) or a dog treat (üê∂)** just by looking at its **features**:  
- **Round?** (Yes/No)  
- **Has spots?** (Yes/No)  

#### **Step 1: Learn from Examples (Training Data)**  
You‚Äôve seen 6 cookies before:  

| Cookie | Round? | Has Spots? | Type       |
|--------|--------|------------|------------|
| 1      | Yes    | Yes        | üç™ (Choco) |
| 2      | Yes    | No         | üç™ (Choco) |
| 3      | No     | Yes        | üê∂ (Dog)   |
| 4      | No     | No         | üê∂ (Dog)   |
| 5      | Yes    | Yes        | üç™ (Choco) |
| 6      | No     | Yes        | üê∂ (Dog)   |

#### **Step 2: Count Probabilities**  
- **Probability of Chocolate (üç™)**: 3 out of 6 ‚Üí **50%**  
- **Probability of Dog (üê∂)**: 3 out of 6 ‚Üí **50%**  

Now, for **each feature**:  
1. **Round?**  
   - If üç™, **2 out of 3** are round ‚Üí **66%**  
   - If üê∂, **0 out of 3** are round ‚Üí **0%**  

2. **Has Spots?**  
   - If üç™, **2 out of 3** have spots ‚Üí **66%**  
   - If üê∂, **2 out of 3** have spots ‚Üí **66%**  

#### **Step 3: Make a Prediction!**  
You find a **new cookie**:  
- **Round?** = Yes  
- **Has Spots?** = Yes  

**Calculate:**  
1. **Chocolate (üç™) Chance**:  
   - 50% (üç™) √ó 66% (Round) √ó 66% (Spots) ‚âà **22%**  

2. **Dog (üê∂) Chance**:  
   - 50% (üê∂) √ó 0% (Round) √ó 66% (Spots) = **0%**  

**Winner:** üç™ (Chocolate Chip)!  

---

### **Why "Naive"?**  
It assumes **features (Round/Spots) don‚Äôt affect each other**‚Äîwhich isn‚Äôt always true (real life is messier). But it‚Äôs fast and works surprisingly well!  

#### **Real-World Use**  
- Spam detection (is this email "spam" or "not spam"?).  
- Medical tests (is this disease "yes" or "no"?).  

**Try it in Weka:**  
1. Load your data.  
2. Pick `NaiveBayes` in the `Classify` tab.  
3. Click `Start`‚ÄîWeka does the math for you!  

---
## On KNN

**from classifier:**
=== Run information ===

Scheme:       weka.classifiers.lazy.IBk -K 1 -W 0 -A "weka.core.neighboursearch.LinearNNSearch -A \"weka.core.EuclideanDistance -R first-last\""
Relation:     weather.symbolic
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    evaluate on training data

=== Classifier model (full training set) ===

IB1 instance-based classifier
using 1 nearest neighbour(s) for classification


Time taken to build model: 0 seconds

=== Evaluation on training set ===

Time taken to test model on training data: 0 seconds

=== Summary ===

Correctly Classified Instances          14              100      %
Incorrectly Classified Instances         0                0      %
Kappa statistic                          1     
Mean absolute error                      0.0625
Root mean squared error                  0.0625
Relative absolute error                 13.4615 %
Root relative squared error             13.0347 %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     yes
                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     no
Weighted Avg.    1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     

=== Confusion Matrix ===

 a b   <-- classified as
 9 0 | a = yes
 0 5 | b = no

**with parameter, k, as being the target variable and the k closest data points**

it's important to note that you can use various distance variables (manhatten, euclidean, etc.)

Jaccard coefficient is an example of calculating distance between two data points.









---
# Weka

### **Detailed Guide to Using Weka (Machine Learning Tool)**
Weka (Waikato Environment for Knowledge Analysis) is an open-source Java-based machine learning toolkit developed at the University of Waikato. It provides a **GUI** for easy experimentation and a **command-line/API** for advanced users. Below is a structured breakdown of how it works, how to find variables (attributes), and how to run algorithms.

---

## **1. How Weka Works**
Weka supports:
- **Data Preprocessing** (cleaning, normalization, feature selection).
- **Classification** (e.g., Decision Trees, SVM, Naive Bayes).
- **Regression** (e.g., Linear Regression, Random Forest).
- **Clustering** (e.g., k-Means, EM).
- **Association Rule Mining** (e.g., Apriori).
- **Visualization** (e.g., scatter plots, ROC curves).

### **Key Components**
| Component          | Description |
|--------------------|-------------|
| **Explorer**       | GUI for loading data, preprocessing, and running ML models. |
| **Experimenter**   | Compare multiple algorithms statistically. |
| **Knowledge Flow** | Drag-and-drop pipeline builder. |
| **Command Line**   | Advanced scripting for automation. |

---

## **2. Loading Data & Finding Variables (Attributes)**
### **Step 1: Open Weka**
- Launch Weka (GUI Chooser) ‚Üí Click **"Explorer"**.

### **Step 2: Load Dataset**
- Supported formats: `.arff` (native), `.csv`, `.xlsx`.
- Click **"Open File"** (or use a URL/database).
- Example datasets are in `Weka/data/`.

### **Step 3: Understand Variables (Attributes)**
- After loading data, the **"Preprocess"** tab shows:
  - **Attributes (Variables)**: Columns in the dataset.
  - **Class Variable**: The target (output) variable (selectable in the dropdown).
  - **Statistics**: Mean, min, max, missing values per attribute.

![Weka Preprocess Tab](https://www.cs.waikato.ac.nz/ml/weka/screenshots/preprocess.png)

---

## **3. Running a Machine Learning Algorithm**
### **Step 1: Choose an Algorithm**
- Go to the **"Classify"** tab.
- Click **"Choose"** ‚Üí Select an algorithm (e.g., `J48` for Decision Trees).

### **Step 2: Configure Test Options**
- **Training Set**: Train & test on the same data (not recommended for real-world use).
- **Cross-Validation (Recommended)**: Split data into folds (e.g., 10-fold CV).
- **Percentage Split**: e.g., 70% train, 30% test.

### **Step 3: Run the Model**
- Click **"Start"** ‚Üí Weka trains the model and displays:
  - **Accuracy** (for classification).
  - **Confusion Matrix** (TP, FP, TN, FN).
  - **Model Summary** (rules, weights, etc.).

![Weka Classify Tab](https://www.cs.waikato.ac.nz/ml/weka/screenshots/classify.png)

---

## **4. Example: Running J48 (Decision Tree)**
1. **Load Data**: `weather.nominal.arff` (sample dataset).
2. **Preprocess**: Ensure the **"play"** column is set as the class variable.
3. **Classify Tab** ‚Üí Choose `trees ‚Üí J48`.
4. **Test Options**: 10-fold cross-validation.
5. **Start** ‚Üí View output:
   ```
   Correctly Classified Instances: 9 (64.2857 %)
   Confusion Matrix:
      a b   <-- Predicted
     3 1 | a = yes
     1 6 | b = no
   ```

---

## **5. Evaluating Results**
- **Accuracy**: % of correctly classified instances.
- **ROC Curve**: Right-click result ‚Üí `Visualize threshold curve`.
- **Feature Importance**: Some algorithms (e.g., Random Forest) show attribute rankings.

---

## **6. Exporting & Saving Models**
- **Save Model**: Right-click result ‚Üí `Save model` (`.model` file).
- **Load Model**: Use later for predictions on new data.
- **Export Predictions**: `Right-click ‚Üí Save predictions`.

---

## **7. Advanced: Command Line & Scripting**
```bash
java weka.classifiers.trees.J48 -t data.arff -C 0.25 -M 2
```
- `-t`: Training dataset.
- `-C`: Pruning confidence (for J48).
- `-M`: Minimum instances per leaf.

---

## **8. Weka vs. Alternatives**
| Feature       | Weka | Python (Scikit-learn) | R (caret) |
|--------------|------|----------------------|----------|
| **GUI**      | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |
| **Ease of Use** | ‚úÖ Beginner-friendly | ‚ö†Ô∏è Coding needed | ‚ö†Ô∏è Coding needed |
| **Scalability** | ‚ùå Limited | ‚úÖ High | ‚úÖ High |

---

### **Final Tips**
- **For Beginners**: Use the Explorer GUI.
- **For Large Datasets**: Preprocess in Python/R, then import to Weka.
- **For Research**: Use the **Experimenter** to compare algorithms statistically.

Would you like a step-by-step video tutorial or help with a specific dataset? üöÄ