I just wanted to get clear on how the activation function generates the new blue curve below?

![[Pasted image 20250928203600.png]]

How exactly do we get optimal weights and biases for each edge of the network? Is optimality calculated during FF, BP or both?
![[Pasted image 20250928203900.png]]

What does it mean for the changes in the activation function being proportional to the size of the corresponding weights?
![[Pasted image 20250928204335.png]]

After data is Fed Forward, why are some wights proportionally bigger than others? Why do we only sum them up?

Can an activation function be considered "a function of the weighted sum of a layer of neurons to one neuron"? But what about the other activation functions that do different things (ReLU, tanh, softmax)?

![[Pasted image 20250928204700.png]]