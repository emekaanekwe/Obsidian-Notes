# Deep Learning Assignment Breakdown and Guidance

## Assignment Overview

This assignment has three main parts:

1. **Part 1: Theory and Knowledge Questions** (30 points) - Tests your understanding of fundamental DL concepts
2. **Part 2: Deep Neural Networks** (25 points) - Practical implementation of feedforward networks
3. **Part 3: Convolutional Neural Networks** (45 points) - Image classification with CNNs and advanced techniques

## Part 1: Theory and Knowledge Questions

### Question 1.1: Activation Functions (10 points)

**What to do:**
- For ELU and GELU activation functions:
  1. State their output ranges
  2. Derive their derivatives (show steps)
  3. Plot the activation functions and their derivatives

**How to approach:**

**ELU:**
1. Output range:
   - For x > 0: ELU(x) = x → range (0, ∞)
   - For x ≤ 0: ELU(x) = α(exp(x)-1) → range (-α, 0) where α=0.1 in this case
   - Overall range: (-0.1, ∞)

2. Derivative:
   - For x > 0: dELU/dx = 1
   - For x ≤ 0: dELU/dx = αexp(x)

**GELU:**
1. Output range: (-∞, ∞) since it's xΦ(x) where Φ(x) is Gaussian CDF (0 to 1)

2. Derivative:
   - GELU(x) = xΦ(x)
   - dGELU/dx = Φ(x) + xφ(x) where φ(x) is Gaussian PDF

**Implementation:**
```python
import numpy as np
import matplotlib.pyplot as plt

# ELU
def elu(x, alpha=0.1):
    return np.where(x > 0, x, alpha*(np.exp(x)-1))

def elu_derivative(x, alpha=0.1):
    return np.where(x > 0, 1, alpha*np.exp(x))

# GELU approximation (since exact Φ(x) is complex)
def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3))

def gelu_derivative(x):
    return 0.5 * (1 + np.tanh(np.sqrt(2/np.pi)*(x + 0.044715*x**3)) + \
           0.5*x*(1 - np.tanh(np.sqrt(2/np.pi)*(x + 0.044715*x**3)**2) * \
           np.sqrt(2/np.pi)*(1 + 3*0.044715*x**2)

# Plotting
x = np.linspace(-5, 5, 100)
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(x, elu(x), label='ELU')
plt.plot(x, gelu(x), label='GELU')
plt.title('Activation Functions')
plt.legend()

plt.subplot(1,2,2)
plt.plot(x, elu_derivative(x), label='ELU Derivative')
plt.plot(x, gelu_derivative(x), label='GELU Derivative')
plt.title('Derivatives')
plt.legend()
plt.show()
```

### Question 1.2: Feedforward Network Computation (7 points)

**What to do:**
Given a 3-layer network with ReLU activation and specific weights/biases:
1. Compute latent representations h¹(x), h²(x)
2. Compute logits h³(x)
3. Compute prediction probabilities p(x)
4. Determine predicted label and correctness
5. Compute cross-entropy loss
6. Explain why CE loss is non-negative

**How to approach:**

1. Compute h¹(x) = ReLU(W¹x + b¹)
2. Compute h²(x) = ReLU(W²h¹(x) + b²)
3. Compute h³(x) = W³h²(x) + b³
4. p(x) = softmax(h³(x))
5. CE loss = -log(p_y) where y=2

**Implementation:**
```python
import torch
import torch.nn.functional as F

# Given values
x = torch.tensor([1., 2., -1., 2.])
y = 2

W1 = torch.tensor([[1, -1, 1, -1],
                   [-1, 1, -1, 1],
                   [2, 2, 2, -2]]).float()
b1 = torch.tensor([0., 1., 0.])

W2 = torch.tensor([[1, -1, 1],
                   [1, 1, -1],
                   [-1, 1, -1]]).float()
b2 = torch.tensor([1., 0., 1.])

W3 = torch.tensor([[2, -2],
                   [-2, 2],
                   [2, 1.5]]).float()

# Forward pass
h1 = F.relu(W1 @ x + b1)  # [3,4] @ [4] + [3] → [3]
h2 = F.relu(W2 @ h1 + b2) # [3,3] @ [3] + [3] → [3]
h3 = W3 @ h2              # [2,3] @ [3] → [2]
p = F.softmax(h3, dim=0)

# Results
print(f"h1: {h1}") 
print(f"h2: {h2}")
print(f"h3: {h3}")
print(f"p(x): {p}")
print(f"Predicted label: {torch.argmax(p).item()}")
print(f"CE Loss: {-torch.log(p[y]).item()}")
```

### Question 1.3: Option 2 - Manual FFN Implementation (20 points)

**What to do:**
Implement a feedforward network from scratch in PyTorch including:
1. Custom Linear layer (MyLinear)
2. Complete FFN class (MyFFN) with:
   - Forward pass
   - Loss computation
   - SGD, SGD with momentum, AdaGrad optimizers
3. Train on MNIST and evaluate

**Key Implementation:**
```python
class MyLinear(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.W = torch.nn.Parameter(torch.randn(input_size, output_size)*0.01)
        self.b = torch.nn.Parameter(torch.zeros(output_size))
        
    def forward(self, x):
        return x @ self.W + self.b

class MyFFN(torch.nn.Module):
    def __init__(self, input_size, num_classes, hidden_sizes, act=torch.nn.ReLU()):
        super().__init__()
        self.layers = torch.nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [num_classes]
        
        for i in range(len(sizes)-1):
            self.layers.append(MyLinear(sizes[i], sizes[i+1]))
        self.act = act
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = self.act(layer(x))
        return F.softmax(self.layers[-1](x), dim=1)
    
    def update_SGD(self, x, y, lr=0.01):
        loss = F.cross_entropy(self(x), y)
        loss.backward()
        
        with torch.no_grad():
            for layer in self.layers:
                layer.W -= lr * layer.W.grad
                layer.b -= lr * layer.b.grad
                layer.W.grad.zero_()
                layer.b.grad.zero_()
```

## Part 2: Deep Neural Networks (FashionMNIST)

### Question 2.1: Data Visualization (5 points)

**Implementation:**
```python
def visualize_batch(loader):
    images, labels = next(iter(loader))
    fig, axes = plt.subplots(4, 8, figsize=(12,6))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i].reshape(28,28), cmap='gray')
        ax.set_title(f"Label: {labels[i].item()}")
        ax.axis('off')
    plt.tight_layout()
    plt.show()
```

### Question 2.2: FFN Implementation (5 points)

**Implementation:**
```python
class FashionFFN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 400)
        self.fc2 = nn.Linear(400, 300)
        self.fc3 = nn.Linear(300, 10)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = x.view(-1, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return F.log_softmax(self.fc3(x), dim=1)
```

### Question 2.3: Hyperparameter Tuning (5 points)

**Approach:**
Grid search over:
- n1 ∈ {20,40}
- n2 ∈ {20,40}
- activation ∈ {sigmoid, tanh, relu}

**Implementation:**
```python
best_val = 0
for n1 in [20,40]:
    for n2 in [20,40]:
        for act in [nn.Sigmoid, nn.Tanh, nn.ReLU]:
            model = nn.Sequential(
                nn.Linear(784,n1), act(),
                nn.Linear(n1,n2), act(),
                nn.Linear(n2,10))
            # Train and evaluate
            if val_acc > best_val:
                best_model = model
```

## Part 3: CNNs (Animal Classification)

### Question 3.1: Custom CNN Implementation (12 points)

**Implementation:**
```python
class YourBlock(nn.Module):
    def __init__(self, in_channels, out_channels, drop_rate=0.2, batch_norm=True, use_skip=False):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity()
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity()
        self.pool = nn.MaxPool2d(2)
        self.dropout = nn.Dropout(drop_rate)
        self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=2) if use_skip else None
        
    def forward(self, x):
        identity = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = self.dropout(x)
        
        if self.skip:
            identity = self.skip(identity)
            x = x + identity
        return x
```

### Question 3.5: OVA Loss (4 points)

**Implementation:**
```python
def ova_loss(outputs, labels, num_classes=20):
    sigmoid_outputs = torch.sigmoid(outputs)
    loss = 0
    for i in range(num_classes):
        # For correct class: maximize sigmoid
        # For incorrect classes: minimize sigmoid (maximize 1-sigmoid)
        mask = (labels == i).float()
        term = mask * torch.log(sigmoid_outputs[:,i] + 1e-10) + \
               (1-mask) * torch.log(1 - sigmoid_outputs[:,i] + 1e-10)
        loss -= term.mean()
    return loss
```

### Question 3.6: PGD Attack (4 points)

**Implementation:**
```python
def pgd_attack(model, images, labels, eps=0.0313, alpha=0.002, iters=20):
    orig_images = images.clone().detach()
    
    for i in range(iters):
        images.requires_grad = True
        outputs = model(images)
        loss = F.cross_entropy(outputs, labels)
        loss.backward()
        
        adv_images = images + alpha * images.grad.sign()
        eta = torch.clamp(adv_images - orig_images, min=-eps, max=eps)
        images = torch.clamp(orig_images + eta, 0, 1).detach_()
    
    return images
```

## General Advice

1. **Start early** - This is a comprehensive assignment with many components
2. **Test incrementally** - Verify each function works before moving to the next
3. **Use Colab GPUs** - For faster training of CNN models
4. **Document results** - Keep track of accuracies/losses for each experiment
5. **Follow submission guidelines** - Properly name files and include all required components

Would you like me to elaborate on any specific part of the assignment in more detail?