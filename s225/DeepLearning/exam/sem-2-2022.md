# Part A - Multiple Choice (12 Questions, 25 marks)
## Question 1
Assume that we have 5 classes in {cat = 1, dolphin = 2, monkey = 3, dog = 4, elephant = 5}. Given a data example x with ground-truth label monkey, assume that a feed-forward NN gives discriminative scores to this x as h1=-2, h2=1, h3=5, h4=2, h5=4. What is the probability to predict x as elephant or p(y=elephant ∣ x)?

Select one:
a.
0.5
b.
$$\frac{e^2}{e^{-2}+e^{1}+e^5+e^{2}+e^4}$$
c. 
$$\frac{e^{-2}}{e^{-2}+e^{1}+e^5+e^{2}+e^4}$$
d.
$$log \frac{e^{2}}{e^{-2}+e^{1}+e^5+e^{2}+e^4}$$
e.
$$\frac{e^{4}}{e^{-2}+e^{1}+e^5+e^{2}+e^4}$$
## Question 2
Consider an image classification task with five classes {cat=1, dog=2, lion=3, flower=4, cow=5}. Consider an image x. Assume that a Convolutional Neural Network gives prediction probabilities f(x)= [0.4, 0.2, 0.1, 0.2, 0.1] and categorial ground-truth label of x is flower. What is the cross-entropy loss
suffered by this prediction?

Select one:
a.
$-log 0.2$
b.
$-log 0.3$
c.
$log 0.3$
d.
$-log 0.1$
e.
$log0.2$

## Question 3
Consider the leaky ReLU activation function:
$$\sigma(z)= \begin{cases}
z & if z \ge 0  \\
0.2z & otherwise
\end{cases}$$
Assume that $=\sigma(\stackrel{-}{h}$) with $\stackrel{-}{h}=[-1,2,-3]$. what is the derivative $\frac{\partial h}{\partial \stackrel{-}{h}}$?
Select one:
a.
diag([-1, 2, -3])
b.
diag([0.2, 1 ,0.2])
c.
[0.2, 1 ,0.2]
d.
diag([0, 1 ,0])
e.
[0, 1, 0]

## Question 4
Given the function $f(w)=\frac{1}{1000}\sum^{1000}_{i=1}(w-x_i)^2$
where xi=i,∀i=1,…,1000. We need to solve $min_w$⁡ f(w) using stochastic gradient descent with the learning rate η=0.1. Assume we sample a batch b1=1,b2=3 of indices and at the iteration t, we have $w_t=10$. What is the value of $w_t+1$ at the next iteration?

Select one:
a.
8.3
b.
8.2
c.
8.1
d.
8.4
e.
8.5

## Question 5
Assume that the tensor before the last tensor of a CNN has shape[128, 32, 32, 20] and we apply 30 filters, each of which has the shape [3, 3, 20] and strides= [2,2] with padding = ‘valid’ to obtain the last tensor. We flatten this tensor to a fully connected (FC) layer. What is the number of neurons on this FC layer?

(Single choice)
Select one:
a.
15 x 15 x 30
b.
14 x 14 x 20
c.
128 x15 x 15 x 30
d.
15 x 15 x 20
e.
128 x 16 x 16 x 30

## Question 6
Assume that the tensor before the last tensor of a CNN has shape[32, 64, 64, 10] and we apply 20 filters, each of which has the shape [3, 3, 10] and strides= [2,2] with padding = 'same' to obtain the last tensor. What is the shape of the output tensor?

Select one:
a.
[32, 29, 29, 20]
b.
[32, 32, 32, 20]
c.
[32, 31, 31, 20]
d.
[32, 30, 30, 20]
e.
[32, 33, 33, 20]

## Question 7
According to the following code, what is the shape of h3?

```python
embed_size = 64
vocab_size = 200
x = tf.keras.Input(shape=[5], dtype='int64')
h1 = tf.keras.layers.Embedding(vocab_size, embed_size)(x)
h2 = tf.keras.layers.GRU(16, return_sequences=True)(h1)
h3 = tf.keras.layers.GRU(8, return_sequences=True)(h2)
h4 = tf.keras.layers.GRU(16, return_sequences=True)(h3)
h5 = tf.keras.layers.Flatten()(h4)
h6 = tf.keras.layers.Dense(10, activation='softmax')(h5)
```

Select one:
a.
[None, 5, 10]
b.
[None, 5, 200]
c.
[None, 5, 16]
d.
[None, 5, 64]
e.
[None, 5, 8]

## Question 8
Consider the below seq2seq model. We apply the global attention to compute the context vector $c_t$. What are correct?

```
SEQUENCE DIAGRAM (Left to Right):

Top row (a_t sequence):
- a_1 (blue box, value 0.3) → a_2 (blue box, value 0.1) → a_3 (blue box, value 0.4) → a_4 (blue box, value 0.2) → c_t (dark red box) → ? (yellow box)
- Arrows connect each element sequentially from left to right

Bottom row (h̄ sequence):
- h̄_1 (blue box) → h̄_2 (blue box) → h̄_3 (blue box) → h̄_4 (blue box) → h_t (dark red box) → h_t (dark red box)
- Arrows connect each element sequentially from left to right

Vertical connections:
- Each a_i box has an upward arrow from corresponding h̄_i box below it
- The final c_t and h_t boxes are also vertically connected

Bottom labels (under h̄ boxes):
- Under h̄_1: "I"
- Under h̄_2: "am"
- Under h̄_3: "a"
- Under h̄_4: "student"
- Under first h_t: "_" (underscore/blank)
- Under second h_t: "Je"

Key elements:
- a_t = attention weights (sum to 1.0: 0.3 + 0.1 + 0.4 + 0.2)
- h̄_i = encoder hidden states for input sequence "I am a student"
- c_t = context vector (weighted sum of encoder states)
- h_t = decoder hidden states
- ? = predicted output token
- This represents an attention mechanism in sequence-to-sequence translation (English "I am a student" → French "Je...")
```

Select one or more:
a.
$c_t=0.1\stackrel{-}{h_2}+0.4\stackrel{-}{h_3}+0.2\stackrel{-}{h_3}$
b.
The first word is more important than other words to the generation of the current output word.
c.
$c_t=0.2\stackrel{-}{h_1}+0.4\stackrel{-}{h_2}+0.1\stackrel{-}{h_3}+0.3\stackrel{-}{h_4}$
d.
The third word is more important than other words to the generation of the current output word.
e.
$c_t=0.3\stackrel{-}{h_1}+0.1\stackrel{-}{h_2}+0.4\stackrel{-}{h_3}+0.2\stackrel{-}{h_4}$

## Question 9
Given a CBOW model with vocabulary size 1,000 and embedding size 250, we consider a target word
with index 10 and context words with indices 15, 25, 35, 45 respectively. Let U and V be two weight
matrices connecting input to hidden layers and hidden to output layers. What statements are correct?

Select one or more:
a.
Shape of U is [1000,250] and shape of V is [250,1000]
b.
Input to the network is $\frac{1_{15}+1_{25}+1_{35}+1_{45}}{{4}}$
c.
The hidden value h is the row 10 of the matrix U
d.
Shape of U is [1000,1000] and shape of V is [250,250]
e.
Input to the network is one-hot vector 110
f.
The hidden value h is the average of rows 15, 25, 35, 45 of the matrix U

## Question 10
How to train a denoising auto encoder with encoder $f_{\theta}$ and decoder $g_{\theta}$?
DENOISING AUTOENCODER PIPELINE:

Flow from left to right:

Step 1 - Original Input:
┌─────────────────┐
│ Original Input  │
│ (clean image)   │
│ Shows digit "4" │
│ (white on black)│
└────────┬────────┘
         │
         v
Step 2 - Add Noise:
┌─────────────────────────────┐
│ Noise addition (⊕ operator) │
│ Original + Noise → Noisy    │
└────────┬────────────────────┘
         │
         v
┌─────────────────┐
│ Noisy Input     │
│ (corrupted)     │
│ Shows digit "4" │
│ with salt/pepper│
│ noise added     │
└────────┬────────┘
         │
         v
Step 3 - Encoder:
┌─────────────────┐
│ Encoder         │
│ (blue box)      │
│ Compresses noisy│
│ input to latent │
│ representation  │
└────────┬────────┘
         │
         v
Step 4 - Latent Code:
┌─────────────────┐
│ Latent Code     │
│ (compressed     │
│ representation) │
│ Shows abstract  │
│ black/white     │
│ pattern         │
└────────┬────────┘
         │
         v
Step 5 - Decoder:
┌─────────────────┐
│ Decoder         │
│ (green box)     │
│ Reconstructs    │
│ from latent     │
│ code            │
└────────┬────────┘
         │
         v
Step 6 - Reconstruction:
┌─────────────────┐
│ Reconstruction  │
│ (output image)  │
│ Shows digit "4" │
│ (denoised)      │
│ (white on black)│
└─────────────────┘

Select one or more:
a.
$$\underset{\theta, \phi}{min}E_{x\sim P}[E_{x'\sim N(x,\eta I)}[d(x, g_{\phi}(f_{\theta}(x')))]$$
b.
$$\underset{\theta, \phi}{min}E_{x\sim P}[d(\tilde x, g_{\phi}(f_{\theta}(x)))]$$
c.
$$\underset{\theta, \phi}{min}E_{\epsilon \sim P}[E_{\epsilon \sim N(0,\eta I)}[d(x, g_{\phi}(f_{\theta}(x+\epsilon)))]$$
d.
$$\underset{\theta, \phi}{min}E_{\epsilon \sim P}[E_{\epsilon \sim N(0,\eta I)}[d(x, f_{\theta}(g_{\phi}(x+\epsilon)))]$$
e.
$$\underset{\theta, \phi}{min}E_{x'\sim P}[E_{x'\sim N(x,\eta I)}[d(x, f_{\theta}(g_{\phi}(x')))]$$

## Question 11

How to train GANs?

GAN training loop textual diagram

```
FLOWCHART DIAGRAM:

At the top:
- A diamond decision node labeled "fake ↔ real"
- Arrow points down to node D

Node D (trapezoid shape):
- Has two outputs branching downward
- Left branch: leads to x_fake (with a ghost/spirit image icon)
- Right branch: leads to x_real (with a dog/puppy image icon)

Left path (fake):
- x_fake connects to node G (trapezoid shape)
- G outputs to "z ~ p_z" (sampling from distribution p_z)

Right path (real):
- x_real connects to a cylindrical database labeled "data"
- The database contains multiple real images (appears to show vehicles, dogs, outdoor scenes)

Key elements:
- D = Discriminator
- G = Generator
- x_fake = fake generated samples
- x_real = real data samples
- z ~ p_z = latent variable sampled from prior distribution
- The diagram shows a GAN (Generative Adversarial Network) architecture where:
  * Generator G takes noise z and produces fake samples x_fake
  * Discriminator D evaluates both fake samples and real samples from the data
  * Discriminator decides whether inputs are "fake" or "real"
```
Select one or more:
a. 
$$\underset{G}{min}\underset{D}{max}J(G,D)=E_{x\sim p_{d}(x)}[log(1-D(x))]+E_{z\sim p(z)}[logD(G(z))]$$

b.
$$\underset{\theta, \phi}{min}E_{x\sim P}[d(\stackrel{\sim}{x},g_{\phi}(f_{\theta}(x))]$$
c.
$$\underset{G}{max}\underset{D}{min}J(G,D)=E_{x\sim p_{d}(x)}[logD(x))]+E_{z\sim p(z)}[log(1-D(G(z)))]$$
d.
$$\underset{G}{min}\underset{D}{max}J(G,D)=E_{x\sim p_{d}(x)}[logD(x))]+E_{z\sim p(z)}[log(1-D(G(z)))]$$
## Question 12
Given a DL model $f(x;θ)$ parameterized by θ where $f(x;θ)$ represents the prediction probabilities of x
associated with a ground-truth label y∈{1,…,M}, we find an adversarial example by $x_{adv}=argmax_{x'\in B_{\epsilon}(x)}l(f(x';\theta),y)$

Select one or more:
a.
We maximally decrease the chance to predict x with label y.
b.
It is a targeted attack.
c.
We maximally increase the chance to predict x with label y.
d.
We maximally increase the chance to predict x with any else label y′≠y.
e.
It is an untargeted attack.

# Part B - Short Workout & Knowledge Questions (6 questions, 40 marks)

## Question 13
Consider a feed-forward neural network as shown in the figure for spam email detection with two
labels (spam=1 and non-spam=2). Assume that we feed a feature vector x=[1,1]T with true label y=2 to10
the network.

NEURAL NETWORK DIAGRAM WITH EQUATIONS:

Network Structure (Left side):
- Input layer: 2 nodes (bottom)
- Hidden layer: 3 nodes (middle)
- Output layer: 2 nodes (top, labeled "spam" and "non-spam")
- All nodes are connected with lines showing full connectivity between layers

Mathematical Formulations (Right side):

Output layer (Layer 2):
- h², p = softmax(h²)
- W² = [1  -1]  , b² = [0]
       [0   1]         [1]
       [-1    1]
       [1    1]

Hidden layer (Layer 1):
- h̃¹, h¹ = ReLU(h̃¹)
- W¹ = [ 1   0]  , b¹ = [-2]
       [ 0   -1]         [ 0]
       [-1   -1]         [- 1]
       [-2   1]           [ 1]

Input layer:
- x = [1, 1]ᵀ, y = 2

13a.
What are the formulas and the values of $\bar{h^1}, h^1$? 

13b.
What are the formulas and the values for the logit h2 and the prediction probability p?

13c.
What is the predicted label $\hat y$ incorrect prediction? (yhat) and the cross-entropy loss for this prediction? Is it a correct or incorrect prediction?

## Question 14
Assume that we conduct a Convolution Neural Network (CNN) with the configuration as shown in the
below figure to predict the image dataset Cifar100 with 100 classes. We feed a batch of images with
the shape [16, 32, 32, 3] our CNN. Answer the following questions.

CONVOLUTIONAL NEURAL NETWORK ARCHITECTURE (for CIFAR-100 Classification):

INPUT STAGE:
┌──────────────────────────────┐
│ Input images (batch of 16)   │
│ Shape: [16, 32, 32, 3]       │
│ - Batch size: 16             │
│ - Height: 32 pixels          │
│ - Width: 32 pixels           │
│ - Channels: 3 (RGB)          │
│                              │
│ Visual: Two example images   │
│ shown as [32,32,3] with RGB  │
│ color bands visible on sides │
└──────────┬───────────────────┘
           │
           v
┌──────────────────────────────┐
│ Input Layer (orange box)     │
│ Represents the input tensor  │
└──────────┬───────────────────┘
           │
           v

CONVOLUTIONAL LAYER (Conv2D):
┌────────────────────────────────────────────────────┐
│ Conv2D Layer                                       │
│ - Filters: 20 filters (shown as Filter 1, 2, ..., │
│   Filter 20)                                       │
│ - Filter shape: [5, 5, 3]                          │
│   * 5×5 spatial dimensions                         │
│   * 3 input channels (RGB)                         │
│ - Padding: 'same' (output size = input size)       │
│ - Strides: (2, 2) (downsampling by factor of 2)    │
│                                                    │
│ Visual representation:                             │
│ - 20 blue 3D filter blocks shown                   │
│ - Each filter labeled [5,5,3]                      │
│ - Filter 1, Filter 2, ..., Filter 20               │
└──────────┬─────────────────────────────────────────┘
           │
           v
┌──────────────────────────────┐
│ Feature volume after Conv2D  │
│ Feature maps: [A1,B1,C1,D1]  │
│ (4 orange 3D blocks shown    │
│  representing feature maps)  │
└──────────┬───────────────────┘
           │
           v

ACTIVATION + NORMALIZATION:
┌──────────────────────────────┐
│ Batch Norm + ReLU            │
│ (blue arrow indicates this)  │
└──────────┬───────────────────┘
           │
           v

POOLING LAYER:
┌────────────────────────────────────────────────────┐
│ Pooling Layer                                      │
│ - Type: Not specified (likely MaxPooling)          │
│ - Pool size: (2, 2)                                │
│ - Strides: (2, 2)                                  │
│ - Padding: 'same'                                  │
│ (blue arrow with green text indicates pooling)     │
└──────────┬─────────────────────────────────────────┘
           │
           v
┌──────────────────────────────┐
│ Feature volume after Pooling │
│ Feature maps: [A2,B2,C2,D2]  │
│ (4 orange 3D blocks shown    │
│  with reduced spatial dims)  │
└──────────┬───────────────────┘
           │
           v

FLATTENING + GLOBAL POOLING:
┌────────────────────────────────────────────────────┐
│ Step 1: Flatten                                    │
│ (yellow arrow labeled "1: Flatten")                │
│                                                    │
│ Step 2: Global Average Pooling                     │
│ (yellow arrow labeled "2: Global Average Pooling") │
│                                                    │
│ Converts 3D feature maps → 1D feature vector       │
└──────────┬─────────────────────────────────────────┘
           │
           v

FULLY CONNECTED LAYERS:
┌──────────────────────────────┐
│ FC Layer (blue dots)         │
│ Multiple fully connected     │
│ neurons shown vertically     │
│                              │
│ Top section labeled:         │
│ - C (orange box)             │
│ - D (orange box)             │
│ - E (orange box with dots)   │
│ - F (orange box)             │
└──────────┬───────────────────┘
           │
           v

OUTPUT LAYER:
┌──────────────────────────────┐
│ Softmax activation           │
│ (labeled next to F box)      │
└──────────┬───────────────────┘
           │
           v
┌──────────────────────────────┐
│ Output layer                 │
│ 100 classes of CIFAR-100     │
│ (green text)                 │
│                              │
│ Final prediction vector      │
│ of length 100                │
└──────────────────────────────┘

14a.
What is the shape of the feature maps [A1, B1, C1, D1]? Show the steps of your answer.

14b.
What is the shape of the feature maps [A2, B2, C2, D2]? Show the steps of your answer.

14c.
Assume that we flatten the feature maps [A2, B2, C2, D2] to obtain the next output. What is the shape
of the 2D tensor [C,D]? Explain your answer.

14d.
Assume that we apply global average pooling to the feature maps [A2, B2, C2, D2] to obtain the next
output. What is the shape of the 2D tensor [C,D]? Explain your answer.

14e.
What is the shape of the 2D tensor [E,F]? Explain your answer.

## Information for Questions 15 and 16

Assume that we have [6,6,1] input tensor as shown below. We first apply a Conv2D layer with two
filters (i.e., filter 1 and filter 2), strides = (1,1), and padding = valid to obtain the output 1. We then
apply max pooling with kernel size = (2,2), strides = (2,2), and padding= valid to obtain the output 2.

CONV2D AND MAX POOLING OPERATION DIAGRAM:

INPUT:
┌────────────────────────────────────┐
│ Input Matrix (6×6 single channel)  │
│                                    │
│  [-3, -1,  4,  3, -3,  1]          │
│  [-2,  1,  1, -3,  3,  2]          │
│  [-2,  3,  0, -2,  1,  0]          │
│  [ 3, -2,  6,  0,  1,  2]          │
│  [ 0, -3,  1,  1,  2, -1]          │
│  [-1,  1, -1,  1, -1,  1]          │
│                                    │
│ Shape: [6, 6, 1]                   │
│ (blue grid with numbers)           │
└──────────┬─────────────────────────┘
           │
           v

CONV2D OPERATION:
┌────────────────────────────────────────────────────────────┐
│ Conv2D Layer                                               │
│ Parameters:                                                │
│ - Strides = (1, 1)                                         │
│ - Padding = 'valid' (no padding)                           │
│ - 2 filters applied                                        │
│                                                            │
│ Filter 1 (3×3):          Filter 2 (3×3):                   │
│  [1,  1,  1]              [-1, -1, -1]                     │
│  [1,  1,  1]              [-1, -1, -1]                     │
│  [1,  1,  1]              [-1, -1, -1]                     │
│                                                            │
│ (shown in white grids below Conv2D label)                  │
└──────────┬─────────────────────────────────────────────────┘
           │
           v
┌──────────────────────────────┐
│ Output 1                     │
│ (orange box/arrow label)     │
│                              │
│ After convolution with 2     │
│ filters, output has 2        │
│ feature maps (channels)      │
│                              │
│ Output shape: [4, 4, 2]      │
│ - Height: 4                  │
│ - Width: 4                   │
│ - Channels: 2                │
│                              │
│ (size reduced due to 'valid' │
│  padding: 6×6 → 4×4)         │
└──────────┬───────────────────┘
           │
           v

MAX POOLING OPERATION:
┌────────────────────────────────────────────────────────────┐
│ Max Pooling Layer                                          │
│ Parameters:                                                │
│ - Kernel size = (2, 2)                                     │
│ - Strides = (2, 2)                                         │
│ - Padding = 'valid'                                        │
│                                                            │
│ Takes maximum value in each 2×2 window                     │
│ Stride of 2 means non-overlapping windows                  │
└──────────┬─────────────────────────────────────────────────┘
           │
           v
┌──────────────────────────────┐
│ Output 2                     │
│ (red text label)             │
│                              │
│ After max pooling:           │
│ Output shape: [2, 2, 2]      │
│ - Height: 2                  │
│ - Width: 2                   │
│ - Channels: 2 (preserved)    │
│                              │
│ (spatial dimensions halved:  │
│  4×4 → 2×2)                  │
└──────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════

PIPELINE FLOW:

Input [6×6×1] → Conv2D (2 filters, 3×3, stride=1, valid padding) 
                → Output 1 [4×4×2] 
                → Max Pooling (2×2, stride=2, valid padding) 
                → Output 2 [2×2×2]

Key operations:
1. Conv2D reduces spatial size from 6×6 to 4×4 (valid padding, no border)
2. Conv2D increases channels from 1 to 2 (due to 2 filters)
3. Max Pooling reduces spatial size from 4×4 to 2×2 (downsampling by 2)
4. Max Pooling preserves channel count (2 channels)

### Question 15
What are the values of the feature maps in output 1?

### Question 16
What are the values of the feature maps in output 2?

## Question 17
Read the following code and provide the shapes of the tensors x, h1, h2, h3, h4, h5, h6. Note that the
shapes should contain one dimension with the Value None for batch size

```python
embed_size = 256
vocab_size = 512
x = tf.keras.Input(shape=[20], dtype='int64')
h1 = tf.keras.layers.Embedding(vocab_size, embed_size)(x)
h2 = tf.keras.layers.LSTM(16, return_sequences=True)(h1)
h3 = tf.keras.layers.LSTM(32, return_sequences=True)(h2)
h4 = tf.keras.layers.LSTM(64, return_sequences=False)(h3)
h5 = tf.keras.layers.Flatten()(h4)
h6 = tf.keras.layers.Dense(15, activation='softmax')(h5)
```

## Question 18
Read the following code and provide the shapes of the tensors x, h1, h2, z, hbar2, hbar1, xr.Note that
the shapes should contain one dimension with the Value None for batch size

```python
x = tf.keras.Input(shape=[32,32,3], dtype='float64')
h1 = tf.keras.layers.Conv2D(10, kernel_size=3, strides=[4,4], padding='same', activation='relu')(x)
h2 = tf.keras.layers.Conv2D(20, kernel_size=3, strides=[2,2], padding='same', activation='relu')(h1)
z = tf.keras.layers.Flatten()(h2)
h3 = tf.keras.layers.Reshape([h1.shape[1],h2.shape[2], h2.shape[3]])(h2)
hbar1 = tf.keras.layers.Conv2DTranspose(10, kernel_size=3, strides=[2,2], padding='same', activation='relu')(hbar2)
xr = tf.keras.layers.Conv2DTranspose(3, kernel_size=3, strides=[4,4], padding='same', activation='relu')(hbar1)
```

# Part C - Mixed & Handwritten-Answer Questions (7 questions, 35 marks)

**These questions typically assess the knowledge and understanding of lectures**

## Information for Questions 19, 20, 21, and 22
Consider a deep learning model with the network architecture as shown below.

```
AUTOENCODER ARCHITECTURE DIAGRAM:

Structure from left to right:

═══════════════════════════════════════════════════════════════════════════

INPUT LAYER:
┌──────────────────────────────┐
│ Input (x)                    │
│ 7 input nodes (gray circles) │
│ Vertically stacked           │
│ (red box outline)            │
└──────────┬───────────────────┘
           │
           v

COMPONENT 1 (ENCODER):
┌────────────────────────────────────────────────────────────┐
│ Component 1 - Encoder f_θ (green label)                    │
│                                                            │
│ Architecture (left to right):                              │
│ Layer 1: 7 nodes (input) → 4 nodes (gray circles)         │
│   - Green lines connect all input nodes to layer 1        │
│   - Fully connected                                        │
│                                                            │
│ Layer 2: 4 nodes → narrower layer (fewer nodes)           │
│   - Black/gray lines connect layer 1 to layer 2           │
│   - Fully connected                                        │
│                                                            │
│ Function: f_θ (encoder function with parameters θ)        │
└──────────┬─────────────────────────────────────────────────┘
           │
           v

LATENT CODE (BOTTLENECK):
┌──────────────────────────────┐
│ Latent code (z)              │
│ (blue label at top)          │
│                              │
│ 2 white circles (nodes)      │
│ Compact representation       │
│ Lowest dimensional layer     │
│                              │
│ This is the compressed       │
│ representation of input x    │
└──────────┬───────────────────┘
           │
           v

COMPONENT 2 (DECODER):
┌────────────────────────────────────────────────────────────┐
│ Component 2 - Decoder g_Φ (green label)                    │
│                                                            │
│ Architecture (left to right):                              │
│ Layer 1: 2 nodes (latent) → wider layer (gray circles)    │
│   - Black/gray lines connect latent to layer 1            │
│   - Fully connected                                        │
│                                                            │
│ Layer 2: Expanding to 7 nodes (gray circles)              │
│   - Green lines connect layer 1 to output layer           │
│   - Fully connected                                        │
│                                                            │
│ Function: g_Φ (decoder function with parameters Φ)        │
└──────────┬─────────────────────────────────────────────────┘
           │
           v

RECONSTRUCTION LAYER:
┌──────────────────────────────┐
│ Reconstruction (x̃)           │
│ (blue label at top)          │
│                              │
│ 7 output nodes (gray circles)│
│ Vertically stacked           │
│ (red box outline)            │
│                              │
│ Reconstructed version of     │
│ original input x             │
└──────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════

ARCHITECTURE SUMMARY:

x (input, 7 dims) 
  → f_θ (encoder) 
    → z (latent code, 2 dims) 
      → g_Φ (decoder) 
        → x̃ (reconstruction, 7 dims)

Key Components:
1. Encoder (f_θ): Maps high-dimensional input to low-dimensional latent space
2. Latent code (z): Compressed representation (bottleneck layer)
3. Decoder (g_Φ): Maps latent code back to original input dimensions
4. Reconstruction (x̃): Attempts to recreate original input x

Network Characteristics:
- Symmetrical hourglass/bottleneck architecture
- Green connections: Input layer and output layer connections
- Black/gray connections: Internal hidden layer connections
- Dimensionality: 7 → ... → 2 → ... → 7
- Training objective: minimize reconstruction error ||x - x̃||


```

### Question 19
What are the names of the deep learning model, component 1, and component 2? Write down the objective function to train this deep learning model with the explanations of the meaning of each term in this objective function.

### Question 20
To be able to perform denoising, we add some Gaussian noises to inputs and aim to reconstruct
benign inputs from noisy inputs. Write down the objective function to train this denoising variant with
the explanations of the meaning of each term in this objective function.

### Question 21
To gain sparser latent codes z, we apply a spare regularization term to latent codes z. Write down the
objective function to train this sparse variant with the explanations of the meaning of each term in
this objective function.

### Question 22
To strengthen this model, we leverage with the principle of generative adversarial networks (GAN). To this end, we can view gΦ as a generator and devise a discriminator Dγ (i.e., γ is its parameters) to
discriminate $\tilde x$ and x. Give your further thoughts and comments about this extension. Write down the
purposes of the discriminator Dγ and the component 2 (i.e., $g_Φ$) in this context. Write down the objective function to train Dγ. Write down the objective function to train $f_θ$ and $g_Φ$ when leveraging with
the GAN principle.

## Information for Questions 23 and 24
Consider a Convolution Neural Network (CNN) with the model parameter θ. Specifically, given an
image x with the ground-truth label y, the CNN returns the prediction probabilities f(x;θ) over M classes
and suffers the loss l(f(x;θ),y) where l is a loss function (e.g., the cross-entropy loss).

### Question 23
a) Adversarial examples are a serious issue of CNNs. Give a definition of adversarial examples. Give a
practical example to explain why adversarial examples circumvent the applications of CNNs in reality.
b) Given a benign example x and the ϵ-ball B_ϵ={x′:‖x′-x‖∞≤ϵ}, describe and give the formula to find out
a targeted adversarial example for x. 
c) Given a benign example x and the ϵ-ball B_ϵ={x′:‖x′-x‖∞≤ϵ}, describe and give the formula to find out
a untargeted adversarial example for x. 

### Question 24
a) Given a mini-batch B={(x1,y1 ),…,(xb,yb)} at an iteration, describe how to perform adversarial training
for this mini-batch to improve model robustness. 
b) How adversarial training is similar to data augmentation? How adversarial training is different to
data augmentation?

### Question 25
Given a standard recurrent neural network (RNN) as shown in the following figure, assume that we
feed a sequence x=x_0, x_1, x_2,…,x_L to the RNN.

RECURRENT NEURAL NETWORK (RNN) UNFOLDED THROUGH TIME:

Structure showing temporal sequence from left to right:

═══════════════════════════════════════════════════════════════════════════

```
TIME STEP t=0:
┌─────────────────────────┐
│ Input: x₀ (bottom)      │
│ (blue circle)           │
└───────────┬─────────────┘
            │ U (red arrow, upward)
            v
┌─────────────────────────┐
│ Hidden state: h₀        │
│ (blue circle)           │
└───┬───────┬─────────────┘
    │       │ W (green arrow, right to h₁)
    │       │ V (purple arrow, upward)
    │       v
    │   ┌─────────────────┐
    │   │ Output: ŷ₀      │
    │   │ (blue circle)   │
    │   └─────────────────┘
    │
    v (connects to h₁)

TIME STEP t=1:
┌─────────────────────────┐
│ Input: x₁ (bottom)      │
│ (blue circle)           │
└───────────┬─────────────┘
            │ U (red arrow, upward)
            v
┌─────────────────────────┐
│ Hidden state: h₁        │
│ (blue circle)           │
│ Receives:               │
│ - Input from x₁ (U)     │
│ - Previous state h₀ (W) │
└───────────┬─────────────┘
            │ V (purple arrow, upward)
            │ --- (dashed arrow, right to h_{t-1})
            v
┌─────────────────────────┐
│ Output: ŷ₁              │
│ (blue circle)           │
└─────────────────────────┘

TIME STEP t=t-1:
┌─────────────────────────┐
│ Input: x_{t-1} (bottom) │
│ (blue circle)           │
└───────────┬─────────────┘
            │ U (red arrow, upward)
            v
┌─────────────────────────┐
│ Hidden state: h_{t-1}   │
│ (blue circle)           │
│ Receives recurrent      │
│ connection from h₁      │
└───────────┬─────────────┘
            │ V (purple arrow, upward)
            │ W (green arrow, right to h_t)
            v
┌─────────────────────────┐
│ Output: ŷ_{t-1}         │
│ (blue circle)           │
└─────────────────────────┘

TIME STEP t=t:
┌─────────────────────────┐
│ Input: x_t (bottom)     │
│ (blue circle)           │
└───────────┬─────────────┘
            │ U (red arrow, upward)
            v
┌─────────────────────────┐
│ Hidden state: h_t       │
│ (blue circle)           │
│ Receives:               │
│ - Input from x_t (U)    │
│ - Previous state        │
│   h_{t-1} (W)           │
└───────────┬─────────────┘
            │ V (purple arrow, upward)
            │ --- (dashed arrow, right to h_L)
            v
┌─────────────────────────┐
│ Output: ŷ_t             │
│ (blue circle)           │
└─────────────────────────┘

TIME STEP t=L (final):
┌─────────────────────────┐
│ Input: x_L (bottom)     │
│ (blue circle)           │
└───────────┬─────────────┘
            │ U (red arrow, upward)
            v
┌─────────────────────────┐
│ Hidden state: h_L       │
│ (blue circle)           │
│ Final hidden state      │
└───────────┬─────────────┘
            │ V (purple arrow, upward)
            v
┌─────────────────────────┐
│ Output: ŷ_L             │
│ (blue circle)           │
└─────────────────────────┘

═══════════════════════════════════════════════════════════════════════════

RNN ARCHITECTURE EXPLANATION:

At each time step t:
  h_t = f(U·x_t + W·h_{t-1} + b_h)
  ŷ_t = g(V·h_t + b_y)

Where:
- x_t: Input at time t
- h_t: Hidden state at time t
- ŷ_t: Output/prediction at time t
- U: Input-to-hidden weight matrix (red arrows)
- W: Hidden-to-hidden (recurrent) weight matrix (green arrows)
- V: Hidden-to-output weight matrix (purple arrows)
- f: Activation function for hidden state (e.g., tanh, ReLU)
- g: Activation function for output (e.g., softmax, sigmoid)
```

Explain why we can consider the hidden state hL as a lossy summary of the sequence x=x_0, x_1, x_2,…,x_L.

