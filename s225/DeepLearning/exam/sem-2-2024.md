## Part A - Multiple Choice Questions (12 questions, 20 marks)

### Question 1 (2 marks)
In the context of Self-Attention mechanisms in transformers, which of the following
statements are correct?
Select one or more:
a. The output of Self-Attention is a weighted sum of values based on attention scores.
b. Self-Attention can only be applied to sequential data like text.
c. The attention scores are computed using the dot product of queries and values.
d. Self-Attention allows each input token to attend to all other tokens in the sequence.
e. Self-Attention computes attention scores by comparing keys and values.
### Question 2 (2 marks)
Given a skip-gram model with vocabulary size 400 and embedding size 200, we consider a pair of target and context words with indices 15 and 10 respectively. Let U and V be two weight matrices connecting input to hidden layers and hidden to output layers. What statements are correct?

Select one or more:
a. Shape of U is [400,200] and shape of V is [200,400]
b. Input to the network is one-hot vector 110 .
c. Shape of U is [400,400] and shape of V is [200,200]
d. The embedding vector h is the column 15 of the matrix U
e. Input to the network is one-hot vector 115 .
f.
The embedding vector h is the row 15 of the matrix U
### Question 3 (2 marks)
What are correct about the multi-head Self-Attention?
Select one or more:
a. The weight matrices
WQ , WK , WV are shared across the heads.
b. We perform each head independently.
c. We concatenate the outputs of each head and input this concatenation to one more linear layer
gain the output of multi-head Self-Attention
WO to
d. We concatenate the outputs of each head and use this concatenation as the output of the multi-head Self-
Attention.
e. Each head has its own
f.
WQ , WK , WV .
The outputs of the heads are conditionally dependent.
### Question 4 (1 mark)
Given an 3D input tensor with shape [30, 64, 64] over which we apply a conv2D with 20
ﬁlters each of which has shape [30, 3, 3], strides [2,2], and padding=1. What is the shape
of the output tensor?
Select one:
a. [20,30, 30]
b. [20, 32, 32]
c. [20, 31, 31]
d. [31, 31, 20]
e. [32, 32, 20]
### Question 5 (2 marks)

Given the loss function 
$J(w) = \frac{1}{1000}\sum^{1000}_{i=1}(w-^2)^2$

where w is the model parameter. We need to solve minw J (w) using stochastic gradient descent with the learning rate η=0.1 . Assume we sample a batch of indices i1 = 1, i2 = 2, i3 = 3, i4 = 4 and at the iteration t, we have wt = 16. What is the value of wt+1 at the next iteration?
Select one:
a. 14.3
b. 14.2
c. 14.1
d. 14
e. 14.4
### Question 6 (2 marks)
Given an implementation of the ResNet as below. Assume that we are feeding a batch [16, 3, 64, 64] to our ResNet. What are the shape of A,B,C,D, E, and F?

```python
class create_ResNet (nn.Module): 
	def __init__(self): 
		super().__init__()
		self.layers = nn.ModuleList([nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3), 
		nn.LazyBatchNorm2d(),
		nn.ReLU(), 
		nn.MaxPool2d(kernel_size=3, stride=2, padding=1), 
		ResnetBlock(64, 2, first_block=True),
		ResnetBlock(128, 2),
		ResnetBlock(256, 2),
		nn.AdaptiveAvgPool2d((1, 1)),
		nn.Flatten(1),
		nn.LazyLinear(10), # nn.Softmax(dim=-1)
])

def forward(self, x):
	for layer in enumerate(self.layers):
		X = layer (X)
	return X
```
Select one:
a. [16,64,16,16], [16,64,16,16], [16,128,8,8], [16,256,4,4], [16,256], [32,10]
b. [16,64,16,16], [16,64,16,16], [16,128,8,8], [16,256,4,4], [16,256,1,1], [16,10]
c. [16,64,16,16], [16,64,8,8], [16,128,4,4], [16,256,2,2], [16,256,1,1], [16,10]
d. Raise an error.
### Question 7 (1 mark)
Consider an image classiﬁcation task with six classes {cat=1, dog=2, lion=3, ﬂower=4,
cow=5, car=6}. Consider an image x. Assume that a Convolutional Neural Network gives a
prediction probabilities f(x)=[0.2, 0.15, 0.05, 0.1, 0.3, 0.2] and categorial ground-truth label
of x is car. What is the cross-entropy loss suffered by this prediction?
Select one:
a. -log 0.2
b. log 0.2
c. -log 0.3
d. log 0.3
e. -log 0.1
### Question 8 (1 mark)
Assume that we have 5 classes in {cat = 1, dog = 2, lion = 3, monkey = 4, car = 5}. Given a
data example x with ground-truth label dog, assume that a feed-forward NN gives discriminative scores to this x as h1 = −3, h2 = 6, h3 = −2, h4 = −1, h5 = 3. What
is the probability to predict x as lion or p(y=lion ∣ x)?
Select one:
a.
$$\frac{e^{-1}}{e^{-3}+e^{6}+e^{-2}+ e^{-1}+e^{3}}$$
b. 1
c. 
$$\frac{e^{1}}{e^{-3}+e^{6}+e^{-2}+ e^{-1}+e^{3}}$$
d.
$$\frac{e^{-2}}{e^{-3}+e^{6}+e^{-2}+ e^{-1}+e^{3}}$$
e.
$$\frac{e^{6}}{e^{-3}+e^{6}+e^{-2}+ e^{-1}+e^{3}}$$
### Question 9 (2 marks)
Given an RNN with three hidden layers of LSTM cells, assume that the sizes of hidden
states on the hidden layers are 25, 35, and 45 respectively and sequence length of input
and hidden layers is 5, and the embedding size of embedding layer is 15. We feed to our
network a mini-batch of shape [2,5]. What are the shape of the output tensor of second
hidden layer (i.e., hidden layer 2) if we consider all hidden states of this layer?

	Hidden 3  
	Hidden 2
	Hidden 1
	Embed
	input:  8  6  8  4  7
	        3  2  3  5  4
Select one:
a. [2, 5, 35]
b. [2, 5, 25]
c. [2, 5, 15]
d. [2, 5, 45]
### Question 10 (1 mark)
Assume that the tensor before the last tensor of a CNN has shape [128, 10 , 32, 32] and
we apply 16 ﬁlters each of which has the shape [10, 5, 5] and strides= [2,2] with padding = 1 to obtain the last tensor. What is the shape of the last tensor?
Select one:
a. [128, 16, 16, 16]
b. [128, 16, 14, 14]
c. [128, 16, 15, 15]
d. [128, 10, 16, 16]
### Question 11 (2 marks)
Given a DL model f(x;θ) parameterized by θ where f(x;θ) represents the prediction
probabilities of x associated with a ground-truth label y∈{1,…,M}, we ﬁnd an adversarial
example by x_{adv} = argmax_{x' ∈Bϵ (x)} l(f (x′ ; θ), y). Which statements are correct?
Select one or more:
a. We maximally decrease the chance to predict x with label y.
b. We maximally increase the chance to predict x with label y.
c. It is an untargeted attack.
d. We maximally increase the chance to predict x with any else label y′≠y.
e. It is a targeted attack.
### Question 12 (2 marks)
Considering Generative Adversarial Networks (GANs), given a distribution of generated examples with probability density function pg and data distribution with probability density function pd , the discriminator D tries to discriminate generated and real examples by minimizing the binary cross-entropy with real examples labeled 1 and fake examples
labeled 0. What are correct for the optimal discriminator D∗ ?
Select one or more:
a.
$$D*(x)=\frac{1}{P_g(x)+1}$$
b.
$$D*(x)=\frac{P_g(x)}{P_g(x)+P_d(x)}$$
c.
$$D*(x)=\frac{P_d(x)}{P_g(x)+1}$$
d.
$$D*(x)=\frac{1}{1+\frac{P_g(x)}{P-D(x)}}$$
## Part B - Short Workout & Knowledge Questions (5 questions, 45 marks)

### Question 13 (10 marks)
Consider a feed-forward neural network as shown in the ﬁgure for spam email detection
with two labels (spam=1 and non-spam=2). Assume that we feed a feature vector x=[-1,0]
with true label y=1 to the network.

$h^2, p=softmax(g^2)$
$W^2=\begin{bmatrix}1 0 \\ 11 \\ 11 \end{bmatrix}, b^2=\begin{bmatrix} -6& -7 \end{bmatrix}$
$\hat{h^1}, h^1=ReLU(\hat{H^1}$
$W^1=\begin{bmatrix}1 & 0 & -1 \\ 1& 1& 1 \end{bmatrix}, b^1=\begin{bmatrix} -2& 0& 1 \end{bmatrix}$
$x=\begin{bmatrix} -1& 0\end{bmatrix}, y=1$

a) What are the formulas and the values of h̄ and h1 ? [2 points]
b) What are the formulas and the values for the logit h2 and the prediction probability p ?
[2 points]
c) What is the predicted label y^ and the cross-entropy loss ℓ for this prediction? Is it a
correct or incorrect prediction? [2 points]
d) What is the derivative∂ℓ
? [1 points]
∂h
e) What is the derivative∂ℓ
? [1 points]
∂W
2
2
f) Assume that we are using Stochastic Gradient Descent with the learning rate η=0.1 to
update W 2 , what is the new weight matrix W 2 ? [2 points]
Please answer the question on your blank piece of paper.
After your exam ﬁnishes, you’ll have extra time to access your phone to scan a QR code and
upload your answer.
Clearly label each page with Student ID and this question number (and sub part if applicable) (for
example, 'Question 7a')
Do not write your Name on it
No. of answer sheets: 2
Page 9 of 16Question 14
10
Assume that we conduct a Convolution Neural Network (CNN) with the conﬁguration as
shown in the below ﬁgure to predict the image dataset Cifar100 with 100 classes. We
feed a batch of images with the shape [128, 3, 64, 64] our CNN. Answer the following
Marks
questions.
14a)
2
Marks
What is the shape of the feature maps [A1, B1, C1, D1]? Show the steps of your answer.
14b)
2
Marks
What is the shape of the feature maps [A2, B2, C2, D2]? Show the steps of your answer.
14c)
2
Marks
What is the shape of the 2D tensor [C,D] if we follow 1: Flatten? What does this 2D tensor
store? Moreover, what is the shape of the weight matrix W in this case?
14d)
2
Marks
What is the shape of the 2D tensor [C,D] if we follow 2: Global Max Pooling? What does
this 2D tensor store? Moreover, what is the shape of the weight matrix W in this case?
14e)
1
Marks
What is the shape of the 2D tensor [E,F]? What does this 2D tensor store?
14f)
1
Marks
Explain how to compute the batch loss for a mini-batch using the 2D tensor [E,F].
Page 10 of 16Question 15
10
Consider a Vision Transformer (ViT) as shown in the following ﬁgure. We input to this ViT
a mini-batch of one image with the resolution of [3,30,30]. We divide this image into 9
equal patches as in the ﬁgure to feed to the ViT. Answer the following questions:
15a)
Marks
2
Marks
What is the shape of one patch?
15b)
3
Marks
Assume that we apply the linear projection to project ﬂattened patches to C-dimensional
(C=768) token embedding and insert the class token to the sequence of tokens. What is
the shape [A,B,C] of the entire sequence of tokens inputted to the Transformer encoder
block? Note that the order is batch size, sequence length and embedding size.
15c)
2
Marks
What is the shape [D,E,F] of the output sequence of tokens? Note that the order is again
batch size, sequence length and embedding size.
15d)
3
Marks
On top of the class token at the output layer, we build up a multi-layered perceptron head
(MLP head) to make prediction. Explain why the class token contains the global
information of the image?
Page 11 of 16Question 16
Read the following code and provide the shape of the tensors h1, h2, h3, h4, h5, h6, h7, h8
if we input x [128, 3, 64, 64] to our CNN
5
Marks
Page 12 of 16Question 17
Assume that we have a RNN with two time-steps as in the ﬁgure. We feed to this RNN a
sentence “I love” to predict a sentiment label y ̂ in the label set {1:positive, 2: negative}.
10
Marks
a) What is the value of the hidden state h0 if we do not use the bias and apply the ReLU
activation function? (3 points)
b) What is the value of the hidden state h1 if we do not use the bias and apply the ReLU
activation function? (3 points)
c) On the top of h1 , we conduct a dense layer with the weight matrix V to predict the label
y^. What are the prediction probabilities and the predicted label y^? (4 points)
Please answer the question on your blank piece of paper.
After your exam ﬁnishes, you’ll have extra time to access your phone to scan a QR code and
upload your answer.
Clearly label each page with Student ID and this question number (and sub part if applicable) (for
example, 'Question 7a')
Do not write your Name on it
No. of answer sheets: 1
Page 13 of 16Part C - Mixed & Handwritten-Answer Questions (4 questions, 35 marks)
Information
Part C has 4 questions, worth 35 marks.
Question 18
Explain why in the deep learning pipeline, we use the valid accuracy for selecting the best
model before evaluating on a separate testing set.
5
Marks
Question 19
Discuss TWO (2) differences and ONE (1) similarity between a convolutional layer and a
max-pooling layer.
5
Marks
Page 14 of 16Question 20
Consider the following DCGAN on the MNIST dataset whose image resolution is [1, 28,
28]. Assume that the noise_dim = 30 (i.e., the dimension of noises is 30).
10
Marks
a) Explain the shape transformation of the tensors in the generator G (i.e.,
self.generator).You only need to focus on the layers nn.Unﬂatten and
nn.Conv2DTranspose for the shape transformation. What is the purpose of this
generator? (2 points)
b) Explain the shape transformation of the tensors in the discriminator D (i.e.,
self.discriminator). You only need to focus on the layers nn.Flatten, nn.Conv2D, and
nn.Linear for the shape transformation. What is the purpose of this discriminator? (2
points)
c) Present and explain the objective function to train this DCGAN including how to update
the generator and discriminator. (3 points)
d) Describe the optimal D∗ and G* at the Nash equilibrium point. Explain why
D∗ (x) = 0.5 for all x at this Nash equilibrium point. (3 points)
Please answer the question on your blank piece of paper.
After your exam ﬁnishes, you’ll have extra time to access your phone to scan a QR code and
upload your answer.
Clearly label each page with Student ID and this question number (and sub part if applicable) (for
example, 'Question 7a')
Do not write your Name on it
No. of answer sheets: 2
### Question 21 (15 marks)
Consider a seq2seq model using the encoder-decoder architecture shown in the ﬁgure
below. Let θe and θd be the parameters for the encoder and the decoder respectively.
Answer the following questions:
INPUTS:  [x₁] [x₂] ... [x_Tx]  <BOS> [y₁] [y₂] ... [yⱼ₋₁]
           │    │         │        │    │    │         │
ENCODER:  h₁ → h₂ → ... → h_Tx    │    │    │         │
           │    │         │        │    │    │         │
           └────┴─────────┼────────┘    │    │         │
                      c = h_Tx          │    │         │
                        │               │    │         │
DECODER:               q₀ → q₁ → ... → qⱼ₋₁ → qⱼ
                        │    │         │      │
                        │    │         │      │
OUTPUTS:              [y₁] [y₂]     [yⱼ₋₁] [yⱼ]

a) Explain the role of the BOS and EOS symbols? (2 points)
b) Explain why we can use the last encoder hidden state hTx as the context vector c to
summarize the source sequence x = [x1 , . . . , xTx ] . (2 points)
c) Let θ = [θe , θd ] be the parameter vector to be learned and D be the training set
consists of multiple pairs of sequences (x, y), write down the maximum log-likelihood
objective function to learn θ. (3 points)
d) Use the product rule, expand the log-likelihood term logP (y|x, θ) where
y = [y1 , … , yTy ] and x = [x1 , … , xTx ] so that it can be computed based on the local
probability P (yj ∣ qj−1 , c, θ) . (3 points)
e) Explain the drawback of the ﬁxed-length and time invariant context vector c ? (2 points)
f) Present how to use the global attention to compute the time varied context vector
ct corresponding to the decoder hidden state qt for overcoming the drawback of the time
invariant context vector c . (3 points)


---

# Part A — Multiple Choice (with quick justifications)

**Q1 (p.4). Self-Attention facts**  
**Correct:** a, d

- a) Output is a weighted sum of **values** (V) using attention weights.
    
- d) In self-attention, each token can attend to **all tokens** in the same sequence.  
    **Incorrect:** b (works beyond strictly sequential data), c (scores use **queries vs keys**, not queries vs values), e (compare **queries and keys**).
    

---

**Q2 (p.4). Skip-gram shapes & one-hot**  
**Correct:** a, b, f

- (U\in\mathbb{R}^{400\times 200}), (V\in\mathbb{R}^{200\times 400}).
    
- Input is one-hot of the **target** word (index 15).
    
- Hidden embedding (h) is the **row 15 of (U)** (multiplying a one-hot row by (U) “selects” that row).  
    (If an option said “column 15 of (U)”: that’s **wrong** here.)
    

---

**Q3 (p.5). Multi-head Self-Attention**  
**Correct:** b, c, e

- b) Heads are computed **independently**.
    
- c) Head outputs are **concatenated**, then projected by (W_O).
    
- e) Each head has its own (W_Q, W_K, W_V).  
    **Incorrect:** a (not shared), d (you still apply (W_O)), f (heads are not conditionally dependent during the per-head computation).
    

---

**Q4 (p.5). Conv2D output shape**  
Input ([30,64,64]), filters: 20 of shape ([30,3,3]), stride ([2,2]), padding (=1).  
Spatial: (\big\lfloor\frac{64-3+2}{2}\big\rfloor + 1=\lfloor 63/2\rfloor+1=31+1=32).  
**Answer:** ([20,32,32]) → option **b**.

---

**Q5 (p.5). One SGD step**  
Per-sample loss ( \tfrac12(w-i)^2 \Rightarrow \nabla = (w-i)).  
Mini-batch ({1,2,3,4}), use average gradient:  
(\bar{g}=\frac{1}{4}\sum (16-i)=\frac{15+14+13+12}{4}=13.5).  
Update (w_{t+1}=16-0.1\cdot 13.5=14.65).

> **Note:** 14.65 isn’t in the options; if the examiner used a different convention (e.g., extra factor 2 or a non-averaged batch), the nearest listed choice would differ. Show this derivation and state the convention you used.

---

**Q6 (p.6). ResNet tensor shapes**  
Without the figure’s exact blocks you pick the option consistent with standard downsampling (×2 per stage) and final flatten to logits. **Answer likely:** **b**. (Explain: input ([16,3,64,64]) → stem → ([16,64,16,16]) then stages ([16,128,8,8]), ([16,256,4,4]), global pool ([16,256,1,1]), FC ([16,10])).

---

**Q7 (p.6). Cross-entropy**  
True label **car** (index 6), predicted prob (=0.2).  
Loss (= -\log 0.2). → option **a**.

---

**Q8 (p.7). Softmax prob for “lion”**  
Scores (h=[-3,6,-2,-1,3]).  
(p(y=\text{lion}\mid x) = \dfrac{e^{-2}}{e^{-3}+e^{6}+e^{-2}+e^{-1}+e^{3}}).  
Pick the option that matches this form. (Usually labeled **c**.)

---

**Q9 (p.7). LSTM layer-2 output shape**  
Batch (=2), seq len (=5), hidden size (layer 2) (=35).  
Output (all time steps): ([2,5,35]). → option **a**.

---

**Q10 (p.8). Last CNN tensor shape**  
Before last: ([128,10,32,32]); 16 filters ([10,5,5]); stride 2; pad 1.  
Spatial: (\lfloor (32-5+2)/2\rfloor+1=\lfloor 29/2\rfloor+1=14+1=15).  
**Answer:** ([128,16,15,15]) → option **c**.

---

**Q11 (p.8). Adversarial example**  
(x_{\text{adv}}=\arg\max_{x'\in B_\epsilon(x)} \ell(f(x';\theta),y)).  
Maximizing loss **decreases** (p(y\mid x')) and **increases** some other class.  
**Correct:** a, c, d. (Untargeted; not b/e.)

---

**Q12 (p.8). Optimal GAN discriminator**  
(D^_(x)=\dfrac{p_d(x)}{p_d(x)+p_g(x)}).  
At the Nash equilibrium (p_g=p_d\Rightarrow D^_(x)=\tfrac12). Pick the option(s) stating this.

---

# Part B — Short Workouts (with key math)

**Q13 (p.9). 2-layer MLP for spam**  
Let (x=[-1,0]). With weights (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}):

- **(a)** (\bar h_1=W^{(1)}x+b^{(1)}); (h_1=\sigma(\bar h_1)) (e.g., ReLU or sigmoid).
    
- **(b)** (h_2=W^{(2)}h_1+b^{(2)}) (logits). For 2-class softmax:  
    (p=\mathrm{softmax}(h_2)_1=\dfrac{e^{h_{2,1}}}{e^{h_{2,1}}+e^{h_{2,2}}}).
    
- **(c)** (\hat y=\arg\max_k h_{2,k}); CE loss (\ell=-\log p) (if (y=1)).
    
- **(d)** (\frac{\partial \ell}{\partial h_2}=\mathrm{softmax}(h_2)-\mathrm{onehot}(y)).
    
- **(e)** (\frac{\partial \ell}{\partial W^{(2)}}=\frac{\partial \ell}{\partial h_2}, h_1^\top).
    
- **(f)** SGD update: (W^{(2)}\leftarrow W^{(2)}-\eta,\frac{\partial \ell}{\partial W^{(2)}}).  
    (Plug the actual numbers from the figure’s weights to compute values.)
    

---

**Q14 (p.10). CNN shapes (CIFAR-100, [128,3,64,64])**  
Use the conv formula (H_{\text{out}}=\big\lfloor \frac{H_{\text{in}}-K+2P}{S}\big\rfloor+1) and similarly for (W).

- **(a)** Provide ([A_1,B_1,C_1,D_1]) by applying the formula layer-by-layer per the figure.
    
- **(b)** Same for ([A_2,B_2,C_2,D_2]).
    
- **(c)** **Flatten path**: tensor ([C,D]=[128, C_\text{last}!\times H!\times W]) stores per-example features; (W\in\mathbb{R}^{(C!HW)\times 100}).
    
- **(d)** **Global max-pool path**: ([C,D]=[128, C_\text{last}]) stores channel-wise maxima; (W\in\mathbb{R}^{C_\text{last}\times 100}).
    
- **(e)** ([E,F]=[128, 100]) stores logits for 100 classes.
    
- **(f)** Batch loss: mean CE over batch, (\tfrac{1}{128}\sum_{i=1}^{128}-\log \mathrm{softmax}(z_i)_{y_i}).  
    (Compute exact numbers from the figure’s kernels/strides.)
    

---

**Q15 (p.11). ViT with 3×3 patches on [3,30,30]**

- **(a)** One patch has shape ([3,10,10]).
    
- **(b)** Flatten each patch to (3\cdot10\cdot10=300), project to (C=768). With class token, seq len (= 1+9=10).  
    **Answer:** ([A,B,C]=[1,10,768]).
    
- **(c)** Transformer encoder preserves ([1,10,768]) (same (B), (L), (C)).
    
- **(d)** The **class token** attends to all patch tokens; after multiple layers, its embedding aggregates **global** information via attention (it collects a weighted mixture of all patch features at each layer).
    

---

**Q16 (p.12). CNN code shapes**  
From the figure’s code:

```
conv1: in 3 → 32, k=3,s=2,p=1      ⇒ h1: [128, 32, 32, 32]
maxpool k=2,s=2                    ⇒ h2: [128, 32, 16, 16]
conv2: 32 → 64, k=3,s=1,p=1        ⇒ h3: [128, 64, 16, 16]
dropout (no shape change)          ⇒ h4: [128, 64, 16, 16]
conv3: 64 → 128, k=3,s=1,p=1       ⇒ h5: [128, 128, 16, 16]
AdaptiveAvgPool2d((1,1))           ⇒ h6: [128, 128, 1, 1]
Flatten                            ⇒ h7: [128, 128]
Linear(128→20)                     ⇒ h8: [128, 20]
```

(Use (\lfloor (H-K+2P)/S\rfloor+1) per conv.)

---

**Q17 (p.13). Tiny RNN, two time-steps “I love”**  
Let embeddings (x_0, x_1), weights (W_{xh}, W_{hh}), no bias, ReLU:

- **(a)** (h_0=\mathrm{ReLU}(W_{xh}x_0)).
    
- **(b)** (h_1=\mathrm{ReLU}(W_{xh}x_1+W_{hh}h_0)).
    
- **(c)** Logits (z=Vh_1), probs (p=\mathrm{softmax}(z)), (\hat y=\arg\max p).  
    (Plug the numeric matrices from the figure to compute values.)
    

---

# Part C — Conceptual / Short-Derivation

**Q18 (p.14). Why pick best on validation, not test?**

- To avoid **test leakage** and optimistic bias. The test set must estimate **generalization** only once, **after** model selection. Validation accuracy drives **hyper-parameter/model** choice; the test set remains untouched until the end.
    

---

**Q19 (p.14). Conv vs Max-pool**

- **Difference 1:** Conv has **learned weights**; max-pool has **no learnable params**.
    
- **Difference 2:** Conv produces **linear combinations** of local neighborhoods (plus nonlinearity); max-pool computes a **fixed reduction** (max).
    
- **Similarity:** Both are **translation-equivariant** local operators that change spatial resolution and operate channel-wise (pool over spatial dims per channel).
    

---

**Q20 (p.15). DCGAN on MNIST [1,28,28]**

- **(a) Generator:** (z\in\mathbb{R}^{30}) → **Unflatten** to a small 3-D seed (e.g., ([N,c_0,h_0,w_0])) → series of **ConvTranspose2d** (stride (>1)) to upsample until ([N,1,28,28]). Purpose: map noise to **realistic images**.
    
- **(b) Discriminator:** Image ([N,1,28,28]) → **Conv2d** (downsample) → **Flatten** → **Linear** → scalar logit. Purpose: distinguish **real vs fake**.
    
- **(c) Minimax objective:**  
    [  
    \min_G\max_D ;; \mathbb{E}_{x\sim p_d}![\log D(x)] + \mathbb{E}_{z\sim p_z}![\log(1-D(G(z)))].  
    ]  
    Update (D) to **increase** both terms; update (G) to **increase** (\log D(G(z))) (or minimize (-\log D(G(z)))).
    
- **(d) Optimal (D^*):** (D^_(x)=\frac{p_d(x)}{p_d(x)+p_g(x)}). At Nash equilibrium (p_g=p_d\Rightarrow D^_(x)=\tfrac12) for all (x).
    

---

**Q21 (p.16). Seq2Seq (Encoder–Decoder)**

- **(a) BOS/EOS:** **BOS** starts decoding; **EOS** marks sequence end.
    
- **(b)** Last encoder hidden state (h_T) summarizes the **source** via recurrent updates, so it’s used as the fixed **context** (c).
    
- **(c) MLE objective:** For dataset (\mathcal{D}={(x,y)}) with (\theta=[\theta_e,\theta_d]),  
    [  
    \max_\theta \sum_{(x,y)\in\mathcal{D}} \log P(y\mid x;\theta).  
    ]
    
- **(d)** Autoregressive factorization with product rule:  
    [  
    \log P(y\mid x;\theta)=\sum_{t=1}^{T_y}\log P(y_t\mid y_{<t},,c,,\theta).  
    ]
    
- **(e)** Drawback of fixed (c): information **bottleneck**—long sequences lose detail; (c) is time-invariant, so decoder can’t focus on different source parts.
    
- **(f) Global attention:** For decoder state (q_t), compute scores (e_{t,i}=a(q_t,h_i)); weights (\alpha_{t,i}=\mathrm{softmax}_i(e_{t,i})); context  
    [  
    c_t=\sum_i \alpha_{t,i}h_i,  
    ]  
    then use (c_t) (time-varying) with (q_t) for predicting (y_t).
    

---

## Handy formulas (put these on your one-pager)

- **Conv2D size:** (H'=\big\lfloor\frac{H-K+2P}{S}\big\rfloor+1,\quad W'=\big\lfloor\frac{W-K+2P}{S}\big\rfloor+1.)
    
- **Cross-entropy:** (\ell=-\log p_{y}).
    
- **Softmax:** (p_k=\dfrac{e^{h_k}}{\sum_j e^{h_j}}).
    
- **Skip-gram shapes:** (U\in\mathbb{R}^{|V|\times d},;V\in\mathbb{R}^{d\times |V|}); one-hot selects a **row** of (U).
    
- **Self-attention:** (\mathrm{Attn}(Q,K,V)=\mathrm{softmax}!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V).
    
- **GAN (D^*):** (\dfrac{p_d}{p_d+p_g}).
    
- **Seq2Seq MLE:** (\sum_t\log P(y_t\mid y_{<t},c,\theta)).
    

If you want, I can turn this into printable flashcards (term → definition/math) or add step-by-step numeric worked solutions for Q13/Q14/Q17 using any specific weights/figures you supply.

## Handy formulas (put these on your one-pager)

- **Conv2D size:** H′=⌊H−K+2PS⌋+1,W′=⌊W−K+2PS⌋+1.H'=\big\lfloor\frac{H-K+2P}{S}\big\rfloor+1,\quad W'=\big\lfloor\frac{W-K+2P}{S}\big\rfloor+1.H′=⌊SH−K+2P​⌋+1,W′=⌊SW−K+2P​⌋+1.
    
- **Cross-entropy:** ℓ=−log⁡py\ell=-\log p_{y}ℓ=−logpy​.
    
- **Softmax:** pk=ehk∑jehjp_k=\dfrac{e^{h_k}}{\sum_j e^{h_j}}pk​=∑j​ehj​ehk​​.
    
- **Skip-gram shapes:** U∈R∣V∣×d,  V∈Rd×∣V∣U\in\mathbb{R}^{|V|\times d},\;V\in\mathbb{R}^{d\times |V|}U∈R∣V∣×d,V∈Rd×∣V∣; one-hot selects a **row** of UUU.
    
- **Self-attention:** Attn(Q,K,V)=softmax ⁣(QK⊤dk)V\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)VAttn(Q,K,V)=softmax(dk​​QK⊤​)V.
    
- **GAN D∗D^*D∗:** pdpd+pg\dfrac{p_d}{p_d+p_g}pd​+pg​pd​​.