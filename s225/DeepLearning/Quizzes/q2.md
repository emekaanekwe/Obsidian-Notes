
### ![[FIT3181_5215-L7-Quiz-solution.pdf]]
The first-order Markov property

![[Pasted image 20250923101214.png|500]]


---

### **Question 1**

**Correct Answer: C**

**The Mathematical Formula/Rule: The First-Order Markov Assumption**
This assumption states that the future is conditionally independent of the past, given the present. Formally, the probability of the next state depends only on the current state, not on the entire history.

$$p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})$$

**Prerequisite Math Knowledge & Refresher**
1.  **Chain Rule of Probability:** A fundamental rule for breaking down the joint probability of a sequence of events.
    $$p(a, b, c) = p(a | b, c) \cdot p(b | c) \cdot p(c)$$
    More generally:
    $$p(x_1, ..., x_n) = p(x_n | x_{1:n-1}) \cdot p(x_{n-1} | x_{1:n-2}) \cdot ... \cdot p(x_2 | x_1) \cdot p(x_1)$$
2.  **Conditional Independence:** The core of the Markov assumption. If $x_t$ is conditionally independent of $x_{1:t-2}$ given $x_{t-1}$, then $p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})$.

**Step-by-Step Justification**
We start with the general chain rule (Option D):
$$p(x_1,...,x_n) = p(x_n | x_{1:n-1}) p(x_{n-1} | x_{1:n-2}) \dots p(x_2 | x_1) p(x_1)$$

Now we apply the **first-order Markov assumption**: $p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})$. This simplifies every conditional term:
*   $p(x_n | x_{1:n-1})$ becomes $p(x_n | x_{n-1})$
*   $p(x_{n-1} | x_{1:n-2})$ becomes $p(x_{n-1} | x_{n-2})$
*   ... and so on.

Substituting these simplified terms back into the chain rule gives us:
$$p(x_1,...,x_n) = p(x_n | x_{n-1}) p(x_{n-1} | x_{n-2}) \dots p(x_2 | x_1) p(x_1)$$

This is exactly **Option C**.

*   **Why not the others?**
    *   A is incorrect; the sum of probabilities over all sequences must be 1, not the probability of one specific sequence.
    *   B is the assumption of total independence, which is much stronger and not realistic for sequential data.
    *   D is the general chain rule *without* applying the Markov assumption.

---

### **Question 2**

**Correct Answer: D**

**The Mathematical Formula/Rule: The Simple RNN Update Equation**
The equation given is:
$$\bar{h}_t = \tanh(h_{t-1}W + x_tU + b)$$

**Prerequisite Math Knowledge & Refresher**
1.  **Matrix Multiplication Dimensions:** For a product `A * B` to be valid, the number of columns in `A` must equal the number of rows in `B`. The resulting matrix has shape `(rows of A, columns of B)`.
2.  **Parameter Counting:** Trainable parameters are the elements of the weight matrices and bias vectors that the model learns.

**Step-by-Step Justification**
Let's identify the dimensions:
*   Input $x_t$ dimension: $D_x = 10$
*   Hidden state $h_t$ dimension: $D_h = 5$

Now, let's break down the equation $h_{t-1}W + x_tU + b$:
1.  **$h_{t-1}W$**: $h_{t-1}$ has shape $(1, 5)$. To multiply with $W$, $W$ must have shape $(5, 5)$. The result is $(1, 5)$.
    *   Parameters in $W$: $5 \times 5 = 25$.
2.  **$x_tU$**: $x_t$ has shape $(1, 10)$. To multiply with $U$, $U$ must have shape $(10, 5)$. The result is $(1, 5)$.
    *   Parameters in $U$: $10 \times 5 = 50$.
3.  **$+ b$**: The bias $b$ is added element-wise to the $(1, 5)$ result. So $b$ must have shape $(1, 5)$.
    *   Parameters in $b$: $5$.

**Total Trainable Parameters:** Parameters(U) + Parameters(W) + Parameters(b) = $50 + 25 + 5 = 80$.

Now, let's check the options:
*   A. $10 \times 5 = 50$ (This is only the size of $U$)
*   B. $10 \times 5 + 5 = 55$ (This is $U$ and $b$, missing $W$)
*   C. $2 \times (10 \times 5 + 5) + 5 \times 5 = 2 \times 55 + 25 = 135$ (This incorrectly doubles the input parameters and adds a redundant term)
*   **D. $10 \times 5 + 5 + 5 \times 5 = 50 + 5 + 25 = 80$ (This matches our calculation: $U + b + W$)**

---

### **Question 3**

**Correct Answer: B and D**

**The Mathematical Formula/Rule: Simple RNN Architecture and Weight Sharing**
The core idea of a simple RNN is the recursive application of the same function at every time step. This involves **weight sharing** across time.

**Prerequisite Math Knowledge & Refresher**
1.  **Weight Sharing in RNNs:** The same parameters ($U$, $W$, $b$) are used at every time step $t$ to combine the new input $x_t$ and the previous hidden state $h_{t-1}$.
2.  **Output Layers:**
    *   **Regression:** Typically uses a linear output layer (no activation function) to predict a continuous value.
    *   **Classification:** Typically uses a softmax output layer to predict a probability distribution over classes.

**Step-by-Step Justification**
Let's evaluate each option:
*   **A. h0=tanh(x0U+b), h1=tanh(h0W+x1V+b)**
    *   **Incorrect.** This uses a different weight matrix $V$ for the input at time $t=1$ instead of reusing $U$. This violates the principle of weight sharing across time in a simple RNN.
*   **B. h0=tanh(x0U+b), h1=tanh(h0W+x1U+b)**
    *   **Correct.** This correctly uses the same input weight matrix $U$ for both $x_0$ and $x_1$, demonstrating weight sharing. The initial hidden state $h_{-1}$ is often assumed to be a zero vector, so $h_0$ is computed from just $x_0$ and the bias.
*   **C. Assume the task is regression, then y~1~=h~1~U+c**
    *   **Incorrect.** For regression, the output is typically $y_1 = h_1V + c$, where $V$ is a *new* set of weights for the output layer. Reusing $U$ (the input-to-hidden weight) for the output is not standard and would constrain the model unnecessarily.
*   **D. Assume the task is classification using softmax function, then y0=softmax(h0V+c)**
    *   **Correct.** This is the standard setup for classification. The hidden state $h_0$ is passed through a linear layer with weights $V$ and bias $c$, and then the softmax function is applied to get a class probability distribution.

---

### **Question 4**

**Correct Answer:**
1.  **Video classification at frame level** -> **C (many-to-many)**
2.  **Sentiment analysis** -> **A (many-to-one)**
3.  **Machine translation** -> **D (many-to-many)**
4.  **Image captioning** -> **B (one-to-many)**

**The Core Concept: RNN Topologies**
RNNs can be configured in different ways based on the input-output sequence structure.

**Justification by Analogy and Definition**
*   **A. Many-to-One:** Multiple inputs, a single output.
    *   **Application: Sentiment Analysis.** You feed in a sequence of words (many) and output a single sentiment score or class (one).
*   **B. One-to-Many:** A single input, multiple outputs.
    *   **Application: Image Captioning.** You input a single image (one) and output a sequence of words forming a caption (many).
*   **C. Many-to-Many (Synced):** Multiple inputs, multiple outputs where each output is aligned to an input at the same time step.
    *   **Application: Video Classification at Frame Level.** You input a sequence of video frames (many) and output a class label (e.g., "walking", "running") for each individual frame (many).
*   **D. Many-to-Many (Unsynced/Encoder-Decoder):** Multiple inputs, multiple outputs where the number of inputs and outputs can differ and are not aligned.
    *   **Application: Machine Translation.** You input a sequence of words in the source language (many). The entire sequence is encoded into a context vector. Then, a decoder generates a sequence of words in the target language (many), which often has a different length.

---

### **Questions 5, 6, 7, & 8**

These questions all follow the same principle regarding tensor shapes in sequential models. We will solve the core concept here.

**The Core Concept: Tensor Shapes in RNNs**
When dealing with mini-batches in RNNs, the output of a layer has the general shape `[batch_size, sequence_length, hidden_size]`.

**Prerequisite Knowledge & Refresher**
1.  **Embedding Layer:** Transforms input indices (shape `[batch_size, sequence_length]`) into dense vectors of size `embed_size`. Thus, its output shape is `[batch_size, sequence_length, embed_size]`.
2.  **RNN Layer (GRU/LSTM):** The output of an RNN layer for a given input sequence is a new sequence of hidden states. If the input is `[batch_size, sequence_length, input_size]`, the output is `[batch_size, sequence_length, hidden_size]`, where `hidden_size` is the size of the RNN's hidden state.

**Step-by-Step Justification for the Network Diagram**
The diagram shows a clear pipeline:
`Input [2, 5]` -> `Embedding Layer (embed_size=10)` -> `Hidden 1 (hidden_size=15)` -> `Hidden 2 (hidden_size=20)` -> `Hidden 3 (hidden_size=25)` -> `Output Layer`

Let's trace the shape through this pipeline:
1.  **Input:** `[batch_size=2, sequence_length=5]`
2.  **After Embedding Layer:** `[2, 5, embed_size=10]`. The embedding layer adds the `embed_size` dimension.
    *   **Q5 Answer: A. [2, 5, 10]**
3.  **After Hidden 1 (GRU with hidden_size=15):** The GRU layer takes the output of the previous layer and transforms it to have a new feature dimension of `hidden_size=15`.
    *   Output Shape: `[2, 5, 15]`
    *   **Q6 Answer: B. [2, 5, 15]**
4.  **After Hidden 2 (GRU with hidden_size=20):** Similarly, it transforms the input to `hidden_size=20`.
    *   Output Shape: `[2, 5, 20]`
    *   **Q7 Answer: C. [2, 5, 20]**
5.  **After Hidden 3 (GRU with hidden_size=25):** Similarly, it transforms the input to `hidden_size=25`.
    *   Output Shape: `[2, 5, 25]`
    *   **Q8 Answer: D. [2, 5, 25]**

---

### **Questions 9, 10, 11, & 12**

These questions are based on the provided PyTorch code. Let's analyze the code step-by-step.

**The Code Analysis:**
1.  `self.emb = nn.Embedding(vocab_size, 64)`: Creates an embedding layer. Output will be `[32, 5, 64]`.
2.  `self.rnn1 = nn.RNN(64, 200, 2, batch_first=True)`: This is a **2-layer** RNN. It takes an input of size 64 and has a hidden size of 200.
    *   `h1, _ = self.rnn1(x)`: By default, this returns the **output features** for the last layer of the RNN for all time steps.
    *   **Shape of h1:** `[batch_size=32, seq_len=5, hidden_size=200]`
    *   **Q9 Answer: B. [32, 5, 200]**
3.  `self.rnn2 = nn.RNN(200, 16, 2, batch_first=True)`: Another **2-layer** RNN. Input size 200, hidden size 16.
    *   `h2, hn2 = self.rnn2(h1)`: `h2` is the output for all time steps.
    *   **Shape of h2:** `[32, 5, 16]`
    *   **Q10 Answer: B. [32, 5, 16]**
    *   `hn2` is the hidden state for the **last time step** of both layers. Its shape is `[num_layers=2, batch_size=32, hidden_size=16]`.
4.  `h3 = h2.reshape(32, -1)`: This flattens the `h2` tensor, combining the sequence and feature dimensions.
    *   **Shape of h3:** `[32, 5 * 16] = [32, 80]`
    *   **Q11 Answer: C. [32, 80]**
5.  `self.linear = nn.Linear(80, 10)`: A linear layer that expects an input of size 80.
    *   `h4 = self.linear(h3)`: The linear layer transforms the feature dimension from 80 to 10.
    *   **Shape of h4:** `[32, 10]`
    *   **Q12 Answer: D. [32, 10]**

---

### **Question 13**

**Correct Answer: A and C**

**The Mathematical Formula/Rule: LSTM Forget Gate**
The forget gate in an LSTM controls how much of the previous cell state (long-term memory) is remembered. Its equation is:
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
where $\sigma$ is the sigmoid function.

**Prerequisite Knowledge & Refresher**
1.  **Sigmoid Function:** Outputs a value between 0 and 1. A value of 0 means "completely forget this," and a value of 1 means "completely remember this." It's perfect for gating mechanisms.
2.  **LSTM Memory Cells:**
    *   **Cell State ($c_t$):** The "long-term memory" of the cell. It runs through the entire chain with only minor linear interactions.
    *   **Hidden State ($h_t$):** The "short-term memory" or "output" of the cell.

**Step-by-Step Justification**
*   **A. We apply sigmoid activation when computing forget gate**
    *   **Correct.** This is defined by the formula $f_t = \sigma(...)$.
*   **B. We apply tanh activation when computing forget gate**
    *   **Incorrect.** The tanh activation is used for the candidate cell state $\tilde{g}_t$, not the forget gate.
*   **C. It controls the proportion of information in long-term memory to be carried forward**
    *   **Correct.** The forget gate $f_t$ is multiplied element-wise with the previous cell state $c_{t-1}$ in the equation: $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{g}_t$. This directly determines what fraction of the long-term memory ($c_{t-1}$) is preserved.
*   **D. It controls the proportion of information in short-term memory to be carried forward**
    *   **Incorrect.** The forget gate acts on the *long-term* cell state ($c_{t-1}$), not the short-term hidden state ($h_{t-1}$).

---

### **Question 14**

**Correct Answer: A and C**

**The Mathematical Formula/Rule: LSTM Output Gate**
The output gate in an LSTM controls how much of the *current* cell state (long-term memory) is read and output as the new hidden state (short-term memory). Its equation is:
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
The new hidden state is then:
$$h_t = o_t \odot \tanh(c_t)$$

**Prerequisite Knowledge & Refresher**
Same as for Question 13, with a focus on the flow from long-term to short-term memory.

**Step-by-Step Justification**
*   **A. We apply sigmoid activation when computing output gate**
    *   **Correct.** This is defined by the formula $o_t = \sigma(...)$.
*   **B. We apply tanh activation when computing output gate**
    *   **Incorrect.** The tanh is applied to the cell state $c_t$ *before* it is gated by the output gate, but the gate itself uses a sigmoid.
*   **C. It controls the proportion of information in current long-term memory to be carried forward to new short-term memory**
    *   **Correct.** The output gate $o_t$ filters the tanh-squashed cell state $\tanh(c_t)$ to produce the new hidden state $h_t$. So, it decides what parts of the *long-term memory* ($c_t$) should be *output* as the new *short-term memory* ($h_t$).
*   **D. It controls the proportion of information in current short-term memory to be carried forward to new long-term memory**
    *   **Incorrect.** This is the role of the **input gate**. The output gate does the reverse: it controls the flow from *long-term* to *short-term* memory.


---



![[FIT3181_5215-L08-Quiz.pdf]]


---

### **Question 1**

**Correct Answer: D**

**The Core Concept: Skip-gram Model Architecture**
In the skip-gram model, we use a **target word** to predict the **context words** surrounding it within a given window.

**Prerequisite Knowledge & Refresher**
*   **Context Window:** A window of size `C` means we look at `C` words to the left and `C` words to the right of the target word.
*   **Skip-gram Input/Output:** For a given target word, the model tries to predict each of the context words within the window. The input is the target word, and the outputs are the context words.

**Step-by-Step Justification**
The sentence is: "the mouse runs away from the cat"
Let's index the words: `[0:'the', 1:'mouse', 2:'runs', 3:'away', 4:'from', 5:'the', 6:'cat']`

A window size of 7 is unusually large and likely a distractor. In practice, the window is centered on the target word. For a standard symmetric window of size `C=3`, the context for "away" (index 3) would be `[1:'mouse', 2:'runs', 4:'from', 5:'the']`. However, a window size of 7 would encompass almost the entire sentence.

Let's evaluate the options based on the skip-gram principle: **Target -> Context**.
*   **A.** Suggests multiple words predict one word. This is the **CBOW** model, not skip-gram.
*   **B.** 'Runs' is the target. Its context for a standard window would not include "the" at position 0. This is incorrect.
*   **C.** Uses two target words. Skip-gram uses a single target word at a time.
*   **D. 'Away' is used to predict 'the', 'mouse', 'runs', 'from', 'the', 'cat'**. This correctly identifies a single target word ('away') and uses it to predict all other words in the large window. This is the correct formulation for skip-gram.

---

### **Question 2**

**Correct Answer: A**

**The Core Concept: CBOW Model Architecture**
The Continuous Bag-of-Words (CBOW) model is the inverse of skip-gram. It uses the **context words** to predict the **target word**.

**Prerequisite Knowledge & Refresher**
*   **CBOW Input/Output:** The input to the model is the sum or average of the context words' vectors. The model's task is to predict the single target word that is missing from the middle of this context.

**Step-by-Step Justification**
Using the same sentence and indexing: `[0:'the', 1:'mouse', 2:'runs', 3:'away', 4:'from', 5:'the', 6:'cat']`

Let's evaluate the options based on the CBOW principle: **Context -> Target**.
*   **A. 'The', 'mouse', 'runs', 'from', 'the', 'cat' are used to predict 'away'**. This correctly identifies the context words (all words except 'away') and uses them to predict the target word 'away'. This is the correct formulation for CBOW.
*   **B.** This describes a skip-gram task (one word predicting many).
*   **C.** This is a mix and doesn't conform to the standard CBOW structure.
*   **D.** This describes a skip-gram task.

---

### **Question 3**

**Correct Answer: A**

**The Core Concept: Analogical Reasoning with Word Embeddings**
Word2Vec embeddings often capture semantic and syntactic relationships vectorially. A classic example is that the vector offset between related words (e.g., man and woman) encodes the relationship itself (e.g., gender).

**Prerequisite Knowledge & Refresher**
*   **Vector Analogies:** The relationship "man is to king as woman is to queen" can be expressed as: `king - man ‚âà queen - woman`. This can be rewritten to find an unknown: `king ‚âà queen - woman + man`.

**Step-by-Step Justification**
We are given the analogy: `? -- prince = queen - princess`

Let's solve for `?`:
`? = queen - princess + prince`

Now, let's interpret the relationships:
*   `queen - princess` captures the concept of royalty and gender (female), but specifically the transition from a younger/less senior female royal to an older/more senior one.
*   Adding `prince` (a male royal) should point us towards the analogous older/more senior male royal.

Following this logic: `(adult female royal) - (young female royal) + (young male royal) = (adult male royal)`
This leads us directly to **King**.

---

### **Question 4**

**Correct Answer: A**

**The Core Concept: One-Hot Encoding**
One-hot encoding is a representation where each word in a vocabulary of size `V` is represented by a binary vector of length `V`, with a single 1 at the word's index and 0s everywhere else.

**Prerequisite Knowledge & Refresher**
*   **Vocabulary Size (V):** The number of unique words. Here, the dictionary has 8 words (`'brown', 'lazy', 'over', 'fox', 'dog', 'quick', 'the', 'jumps'`), so `V=8`.
*   **Index Lookup:** The one-hot vector for a word is defined by its index in the dictionary.

**Step-by-Step Justification**
The dictionary is given as: `{'brown': 0, 'lazy': 1, 'over': 2, 'fox': 3, 'dog': 4, 'quick': 5, 'the': 6, 'jumps': 7}`

The word **'dog'** has index **4**.
Therefore, in an 8-dimensional one-hot vector, the element at position 4 will be 1, and all others will be 0.
`[index0, index1, index2, index3, index4, index5, index6, index7] = [0, 0, 0, 0, 1, 0, 0, 0]`

This matches **Option A**.

---

### **Question 5**

**Correct Answers: B and D**

**The Core Concept: Word2Vec Model Parameters and Hyperparameters**
It's crucial to distinguish between the size of the vocabulary, the size of the embedding, and the context window.

**Prerequisite Knowledge & Refresher**
*   **Dictionary/ Vocabulary Size (N):** The number of unique words in the corpus. This defines the size of the input and output layers of the Word2Vec network.
*   **Embedding Size (d):** The dimension of the dense vector representing each word. This is a key hyperparameter chosen by the user.
*   **Context Window Size (C):** The number of context words to the left and right that the model considers.

**Step-by-Step Justification**
*   **A. Dictionary size N=1 billion**: Incorrect. The dictionary size is the number of *unique words*, which is given as 10k. 1 billion is the number of *sentences*.
*   **B. Dictionary size N=10k**: Correct. This is explicitly stated.
*   **C. Each word will be represented by a vector size [5, 1]**: Incorrect. The vector size is the *embedding size*, which is a hyperparameter. The context window size is 5, but that does not determine the embedding vector's dimensions.
*   **D. The embedding size is unknown with given information**: Correct. The problem statement does not specify the embedding dimension (`d`), so it is unknown.

---

### **Question 6**

**Correct Answers: A, B, and D**

**The Core Concept: Drawbacks of the Vanilla Skip-gram Model**
The standard skip-gram model has computational inefficiencies that are addressed by techniques like Negative Sampling.

**Prerequisite Knowledge & Refresher**
*   **Softmax Computation:** The standard softmax function requires calculating the dot product of the hidden state with every word in the vocabulary, which is an `O(V)` operation. For a large vocabulary (e.g., 10k-1M words), this is very expensive.
*   **Sparse Gradients:** Because the target is a one-hot vector, only the weights for the correct context word receive a strong positive gradient, while all other `V-1` output weights receive a tiny negative gradient. This is an inefficient way to learn.

**Step-by-Step Justification**
*   **A. The softmax output layer is computationally expensive.**: Correct. This is the primary drawback.
*   **B. The values of prediction probabilities are small and hard to distinguish among classes.**: Correct. With a large `V`, the probability mass is spread very thinly, making the correct probability very small and gradients weak.
*   **C. Skip-gram cannot capture the dependency between words.**: Incorrect. Capturing dependencies is its express purpose, and it does so very well.
*   **D. Skip-gram depends on the context window size.**: Correct. The model's performance is sensitive to the choice of this hyperparameter. A small window captures syntactic relationships, while a large window captures more topical/thematic relationships.

---

### **Question 7**

**Correct Answers: C, E, and F**

**The Core Concept: Skip-gram with Negative Sampling (SGNS)**
SGNS transforms the problem from a massive multi-class classification into many binary classification problems, making it computationally efficient.

**Prerequisite Knowledge & Refresher**
*   **New Objective:** For a (target, context) pair, we want to maximize the probability that this is a positive example. For `k` randomly sampled "negative" (target, non-context) pairs, we want to maximize the probability that they are negative.
*   **Sigmoid Function:** This function is perfect for binary classification, outputting a probability between 0 and 1.

**Step-by-Step Justification**
*   **A. The output layer represents the probability over classes and sum of output equal to 1.**: Incorrect. SGNS does not use a softmax over all classes. It uses independent sigmoids for each binary decision.
*   **B. We apply softmax activation function at the output layer.**: Incorrect. SGNS uses sigmoid.
*   **C. We apply sigmoid activation function at the output layer.**: Correct.
*   **D. ...maximize the probabilities to predict those negative words...**: Incorrect. We want to *minimize* (i.e., predict a low probability) for negative pairs.
*   **E. ...minimize the probabilities to predict the pairs of target word and negative words as a positive pair.**: Correct. This is the definition of the negative sample objective.
*   **F. ...maximize the probability to predict the pair of target word and context word as a positive pair.**: Correct. This is the positive sample objective.

---

### **Question 8**

**Correct Answers: B, D, and E**

**The Mathematical Formula/Rule: Skip-gram Objective Function**
The objective function is:
$$J(\theta) = \prod_{t=1}^{T} \prod_{-C \leq j \leq C, j \neq 0} p(w_{t+j} | w_t; \theta)$$

**Prerequisite Knowledge & Refresher**
*   **Symbols:**
    *   `T`: The total length of the corpus (number of words).
    *   `t`: The index of the current **target (center)** word.
    *   `C`: The context window size.
    *   `j`: The offset from the target word to a context word.
*   **Maximum Likelihood Estimation (MLE):** We want to find the parameters $\theta$ that make the observed data (the actual context words) most probable. This is done by **maximizing** $J(\theta)$.

**Step-by-Step Justification**
*   **A. CBOW**: Incorrect. The CBOW objective predicts one word ($w_t$) from a context.
*   **B. Skip-gram**: Correct. This objective predicts multiple context words ($w_{t+j}$) from one target word ($w_t$).
*   **C. j represents for the index of target word**: Incorrect. `j` is the offset.
*   **D. t represents for the index of target word**: Correct.
*   **E. We need to maximize this objective function**: Correct. This is standard MLE.

---

### **Question 9**

**Correct Answers: A and E**

**The Mathematical Formula/Rule: CBOW Objective Function**
The objective function is:
$$J(\theta) = \prod_{t=1}^{T} p(w_t | w_{t-C}, ..., w_{t-1}, w_{t+1}, ..., w_{t+C}; \theta)$$

**Prerequisite Knowledge & Refresher**
*   **Symbols:**
    *   `T`: Total corpus length.
    *   `C`: Half the context window size. The total number of context words is `2C`.
    *   The full window size, including the target word, is `2C + 1`.

**Step-by-Step Justification**
*   **A. CBOW**: Correct. This predicts the target $w_t$ given all context words.
*   **B. Skip-gram**: Incorrect.
*   **C. T is the window size**: Incorrect. `T` is the corpus length.
*   **D. C is the window size**: Incorrect. `C` is half the window size.
*   **E. 2C + 1 is the window size**: Correct. This is the total number of words in the window (center word + `C` left + `C` right).

---

### **Question 10**

**Correct Answers: A, C, and F**

**The Core Concept: Skip-gram Model Architecture and Matrix Dimensions**
Let's define the model components for a vocabulary size `V` and embedding size `d`.

**Prerequisite Knowledge & Refresher**
*   **Input Matrix (`U` or `W_input`)**: Shape `[V, d]`. The embedding lookup table. The hidden layer `h` is obtained by `h = one_hot_input * U`, which is equivalent to selecting the row of `U` corresponding to the target word's index.
*   **Output Matrix (`V` or `W_output`)**: Shape `[d, V]`. Used to compute the scores for each word in the vocabulary being a context word.

**Step-by-Step Justification**
*   **A. Shape of U is [200,100] and shape of V is [100,200]**: Correct. `V=200`, `d=100`.
*   **B. Shape of U is [100,200] and shape of V is [200,100]**: Incorrect. This is transposed.
*   **C. Input to the network is one-hot vector 1~5~.**: Correct. The target word has index 5.
*   **D. Input to the network is one-hot vector 1~10~.**: Incorrect. The context word (index 10) is the target for the output, not the input.
*   **E. The hidden value h is the row 10 of the matrix U**: Incorrect. `h` is the row of `U` corresponding to the *input* (target) word index, which is 5.
*   **F. The hidden value h is the row 5 of the matrix U**: Correct.

---

### **Question 11**

**Correct Answers: B, D, and E**

**The Core Concept: CBOW Model Architecture**
CBOW averages the embeddings of the context words to form the hidden layer.

**Prerequisite Knowledge & Refresher**
*   **Input:** Multiple one-hot vectors for the context words.
*   **Hidden Layer (`h`)**: `h = (1/C) * (embedding_1 + embedding_2 + ... + embedding_C) = average of context word embeddings`. This is equivalent to averaging the corresponding rows of the input matrix `U`.

**Step-by-Step Justification**
*   **A. Shape of U is [100,100] and shape of V is [150,150]**: Incorrect. `U` should be `[V, d] = [100, 150]`. `V` should be `[d, V] = [150, 100]`.
*   **B. Shape of U is [100,150] and shape of V is [150,100]**: Correct.
*   **C. Input to the network is one-hot vector 1~5~.**: Incorrect. The target word (index 5) is the output. The inputs are the context words.
*   **D. Input to the network is ^1~10~+1~20~+1~30~+1~40~^ / 4.**: Correct. This is a mathematical way of saying "the average of the one-hot vectors for indices 10, 20, 30, and 40".
*   **E. The hidden value h is the average of rows 10,20,30,40 of the matrix U**: Correct. This is the core computation of the CBOW hidden layer.
*   **F. The hidden value h row 5 of the matrix U**: Incorrect. That would be the case for skip-gram.

---

### **Question 12**

**Correct Answers: A and E**

**The Core Concept: Interpreting a Skip-gram Computation Graph**
We need to trace the data flow through the provided diagram to calculate the probabilities.

**Prerequisite Knowledge & Refresher**
1.  **Lookup (`h = 1_t U`)**: The one-hot vector for 'quick' (index 6) selects the 6th row of `U`. From the matrix, row 6 is `[0.1, 0.9, 0.9]`. So `h = [0.1, 0.9, 0.9]`.
2.  **Compute Scores (`o = V h`)**: Multiply the `V` matrix (`[d, V] = [3, 8]`) by the hidden vector `h` (`[3,]`). This gives a score vector `o` of length `V=8`.
3.  **Apply Softmax (`p(cw | tw) = softmax(o)`)**: Convert the scores into probabilities.

**Step-by-Step Justification**
*   **A. Vocabulary size is 8 and embedding size is 3.**: Correct. The `U` matrix is `[8, 3]` and `V` is `[3, 8]`.
*   **B. Vocabulary size is 3 and embedding size is 8.**: Incorrect.
*   **Now, let's calculate p1 and p2.**
    *   `h = [0.1, 0.9, 0.9]`
    *   We need the scores for 'the' (index 7) and 'brown' (index 0). This means we need the 7th column and 0th column of `V`, respectively.
    *   **Score for 'the' (index 7)**: `o_7 = V[:, 7] ‚Ä¢ h = (0.5*0.1) + (0.6*0.9) + (0.9*0.9) = 0.05 + 0.54 + 0.81 = 1.40`
    *   **Score for 'brown' (index 0)**: `o_0 = V[:, 0] ‚Ä¢ h = (0.5*0.1) + (0.2*0.9) + (0.7*0.9) = 0.05 + 0.18 + 0.63 = 0.86`
    *   We would need all 8 scores to compute the softmax. The diagram provides them in the `p(cw|tw)` column.
    *   From the diagram: `p(the | quick) = p1 = 0.12` and `p(brown | quick) = p2 = 0.11`.
*   **C. p1 = 0.09, p2 = 0.10.**: Incorrect. These are the probabilities for 'dog' and '?' in the diagram.
*   **D. p1 = 0.45, p2 = 0.77.**: Incorrect. These numbers are present in the `o` (scores) column, not the probability column.
*   **E. p1 = 0.12, p2 = 0.11.**: Correct. This is read directly from the final probability column in the diagram for the correct indices.

---


![[FIT3181_5215-L09-Quiz.pdf]]

---

### **Question 1: Encoder-Decoder Fundamentals**

**Correct Answers: B, C, E**

*   **B. Decoder tries to read from context vector to generate an output sequence.** (Correct)
*   **C. Encoder tries to encode an input sequence to a context vector.** (Correct)
*   **E. Context vector summarizes an input sequence.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **The Encoder-Decoder Paradigm:** This architecture is designed for sequence-to-sequence (seq2seq) tasks like machine translation. The system is split into two specialized components.
2.  **Information Flow:** The input sequence is processed first. Its information is then used to generate the output sequence.

#### Step-by-Step Explanation

*   The **Encoder's** sole job is to process the input sequence (e.g., an English sentence) and create a meaningful representation of it. This representation is the **context vector (c)**. Therefore, **C is correct**.
*   This context vector **c** is, by design, a summary of the entire input sequence. It aims to capture all the salient information needed for the next step. Therefore, **E is correct**.
*   The **Decoder's** job is to take this summary (the context vector **c**) and generate the output sequence (e.g., the French translation) one token at a time. Therefore, **B is correct**.

**Why the others are wrong:**
*   **A:** This is the job of the Decoder, not the Encoder.
*   **D:** This is the job of the Encoder, not the Decoder.
*   **F:** The context vector summarizes the *input* (source), not the target (output) sequence. The target sequence hasn't been generated yet.

---

### **Question 2: Seq2Seq in Machine Translation**

**Correct Answers: C, E**

*   **C. Encoder is a recurrent neural network and decoder is a recurrent neural network.** (Correct, for the original, classic seq2seq model)
*   **E. Context vector could be the last hidden state of encoder.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Recurrent Neural Networks (RNNs):** Before Transformers, RNNs (like LSTMs and GRUs) were the default choice for seq2seq models because their internal state (hidden state) is naturally designed to handle sequential data.
2.  **Hidden State as a Summary:** The hidden state of an RNN at time step *t* is a function of the current input and the previous hidden state. The final hidden state has theoretically processed every element in the sequence, making it a strong candidate for a context vector.

#### Step-by-Step Explanation

*   In the original 2014 seq2seq paper, both the Encoder and Decoder were RNNs. The Encoder RNN processes the input sequence, and its final hidden state is used as the initial context vector. Therefore, **C is correct**.
*   This final hidden state of the Encoder RNN is the most straightforward and common way to initialize the context vector, as it is the last "memory" of the Encoder after reading the entire input. Therefore, **E is correct**.

**Why the others are wrong:**
*   **A & B:** Feed-forward and Convolutional networks (without specific modifications) are not inherently sequential and lack a natural mechanism for a persistent state that summarizes a variable-length sequence, making them poor choices for the original seq2seq model.
*   **D:** The decoder's hidden states are related to generating the *output* sequence, not summarizing the *input*.
*   **F:** The first hidden state of the encoder is usually initialized to zeros and has seen very little of the input sequence, so it is a very poor summary.

---

### **Question 3: Seq2Seq Log-Likelihood Derivation**


**Correct Answers: A, C**

*   **A. In the derivation (1), *c* is viewed as a summary of the sequence *x‚ÇÅ:Tx*.** (Correct)
*   **C. In the derivation (2), *q‚±º‚Çã‚ÇÅ* is viewed as a summary of the sequence *y‚ÇÅ:‚±º‚Çã‚ÇÅ*.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Chain Rule of Probability:** This allows us to break down the joint probability of a sequence into a product of conditional probabilities: $P(y_1, y_2, ..., y_{T_y}) = P(y_1) \cdot P(y_2 | y_1) \cdot ... \cdot P(y_{T_y} | y_1, ..., y_{T_y-1})$.
2.  **Conditional Independence in Seq2Seq:** The core assumption in seq2seq is that the entire input sequence $\mathbf{x}$ is adequately summarized by the context vector $\mathbf{c}$. Furthermore, when generating the next word $y_j$, we assume that all previous words $y_1, ..., y_{j-1}$ are adequately summarized by the decoder's previous hidden state $\mathbf{q}_{j-1}$.

#### Step-by-Step Explanation

*   In derivation (1), the probability of the output $\mathbf{y}$ is conditioned directly on $\mathbf{c}$: $P(\mathbf{y} | \mathbf{x}, \theta) = P(\mathbf{y} | \mathbf{c}, \theta)$. This substitution is only valid if $\mathbf{c}$ is a sufficient representation of the input $\mathbf{x}_{1:T_x}$. Therefore, **A is correct**.
*   In derivation (2), the conditional probability for each token $P(y_j | \mathbf{y}_{1:j-1}, \mathbf{c}, \theta)$ is simplified to $P(y_j | \mathbf{q}_{j-1}, \mathbf{c}, \theta)$. This simplification means we are using the decoder's hidden state $\mathbf{q}_{j-1}$ as a representation (or summary) of all the previously generated words $\mathbf{y}_{1:j-1}$. Therefore, **C is correct**.

**Why the others are wrong:**
*   **B:** *c* summarizes the *input*, not the *target* output.
*   **D:** $\mathbf{q}_{j-1}$ only summarizes the words generated *so far* (up to $j-1$), not the entire future target sequence.
*   **E:** This statement is technically true about how the distribution is *parameterized* (e.g., with a softmax layer on top of a dense network), but it doesn't describe what $\mathbf{q}_{j-1}$ or $\mathbf{c}$ *represent*, which is what the question is asking about in the context of the derivations.

---

### **Question 4: Formulating the Conditional Distribution**

**Correct Answers: A, B, C** (All three diagrams can be used)

#### Prerequisite Knowledge & Refresher

1.  **Parameterizing a Categorical Distribution:** To model $P(y_j | \mathbf{q}_{j-1}, \mathbf{c}, \theta)$, we need a function that takes $\mathbf{q}_{j-1}$ and $\mathbf{c}$ and outputs a probability distribution over the vocabulary. This is typically done by transforming the inputs into a vector of "scores" (logits) and then applying the softmax function.

#### Step-by-Step Explanation

The question is about how to *formulate* or *compute* the conditional distribution. All three diagrams show valid, common neural network architectures for combining the hidden state and context vector to produce an output.

*   **Diagram (a):** A simple concatenation of $\mathbf{q}_{j-1}$ and $\mathbf{c}$, passed through a feed-forward network and softmax. This is a perfectly valid, though simple, way to compute the distribution.
*   **Diagram (b):** The context vector $\mathbf{c}$ is used to *initialize* the decoder's first hidden state. The previous hidden state $\mathbf{q}_{j-1}$ is then a function of this initial state and all previously generated words. This is the classic seq2seq approach.
*   **Diagram (c):** The context vector $\mathbf{c}$ is fed as an *additional input* at every step of the decoder alongside the previous word and the previous hidden state. This is another common and powerful variant.

Since all three are valid design choices for building a seq2seq model that computes $P(y_j | \mathbf{q}_{j-1}, \mathbf{c}, \theta)$, **A, B, and C are all correct**.

---

### **Question 5: The Decoding Process**

**Correct Answers: A, C, E, G**

*   **A. In the phase 1, we feed the input sequence to the encoder to evaluate the context *c* as the last hidden state of the encoder.** (Correct)
*   **C. In the phase 2, we feed BOS symbol the decoder and decode output sequence from this symbol.** (Correct)
*   **E. In the phase 2, we initialize the first hidden state of decoder with the last hidden state of the encoder.** (Correct)
*   **G. In the phase 2, if we use the greedy strategy, at each timestep, we choose the next output item that maximizes the conditional distribution.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Special Tokens (BOS/EOS):** `BOS` (Beginning of Sequence) is a special token used to signal the start of decoding. `EOS` (End of Sequence) signals the end; when the decoder produces this token, generation stops.
2.  **Greedy Decoding:** A simple decoding strategy where at each time step, the word with the highest predicted probability is chosen as the next input.

#### Step-by-Step Explanation

*   **Phase 1 (Encoding):** The input sequence is processed by the encoder. Its final hidden state becomes the context vector **c**. **A is correct**.
*   **Phase 2 (Decoding) Initialization:** The decoder's first hidden state is initialized with the encoder's final hidden state (the context vector **c**). **E is correct**. The first input to the decoder is the `BOS` token. **C is correct**.
*   **Phase 2 (Decoding) Strategy - Greedy:** In greedy decoding, the model does not sample from the distribution. It simply takes the *argmax*: $\hat{y}_j = \arg\max_{y} P(y_j | \mathbf{y}_{1:j-1}, \mathbf{c})$. **G is correct**.

**Why the others are wrong:**
*   **B:** We start with `BOS`, not `EOS`.
*   **D:** The hidden state is initialized with the encoder's *hidden state*, not the last input *item*.
*   **F:** This describes *sampling*, not the *greedy* strategy.

Of course. Let's continue with the rest of the quiz.

---

### **Question 6: Timely Varied vs. Fixed-Length Context**

**Correct Answers: A, E**

*   **A. Fixed-length context is possibly less powerful to capture long input sequences, while timely varied context can provide dynamic and timely adapted context for input sequences.** (Correct)
*   **E. Timely varied context can focus on some input items or words that are more important to generate specific output items or words, while fixed-length context cannot.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Information Bottleneck:** A single, fixed-length context vector forces all information from a potentially long and complex input sequence through a single, fixed-size "bottleneck". This becomes problematic for long sequences.
2.  **Dynamic Relevance:** When generating different words in the output sequence, different parts of the input sequence are usually more relevant. For example, when translating the verb of a sentence, the subject and object might be most important, not the adjectives.

#### Step-by-Step Explanation

*   The fixed-length context vector (from the encoder's final hidden state) must encapsulate *everything* about the input. For long sequences, this is a difficult task and often leads to information loss, especially of details from the beginning of the sequence (a problem related to vanishing gradients in RNNs). Therefore, **A is correct**.
*   Timely varied context is the core idea behind **attention**. It creates a new, customized context vector for *each* step of the decoder. This allows the decoder to "look back" at the entire input sequence and "pay attention" to the most relevant input words for the specific word it is about to generate. Therefore, **E is correct**.

**Why the others are wrong:**
*   **B:** While true that fixed-length context is simpler, this is not an *advantage* of timely varied context; it's a disadvantage in terms of model complexity.
*   **C & D:** Both fixed-length and timely varied context summarize the input sequence. The claim that one cannot or that one is definitively more accurate is false; the key difference is *how* they summarize (statically vs. dynamically).
*   **F:** This is the exact opposite of the truth. Fixed-length context cannot perform this dynamic focusing.

---

### **Question 7: Global Attention**

**Correct Answers: D, F**

*   **D. In the global attention, the time varied context is computed based on all encoder hidden states.** (Correct)
*   **F. In the global attention, the time varied context is a linear combination of all encoder hidden states.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Attention as a Weighted Sum:** The context vector at decoder time step $t$ is a weighted sum of *all* the encoder hidden states: $\mathbf{c}_t = \sum_{i=1}^{T_x} \alpha_{t,i} \mathbf{\bar{h}}_i$.
2.  **Attention Weights (Œ±):** The weights $\alpha_{t,i}$ are computed by a scoring function (e.g., dot product) between the decoder's current state $\mathbf{h}_t$ and *each* encoder state $\mathbf{\bar{h}}_i$, followed by a softmax. This ensures the weights sum to 1.

#### Step-by-Step Explanation

*   "Global" attention means that when calculating the context vector for a decoder step, we consider *every single hidden state* from the encoder. Therefore, **D is correct**.
*   The mechanism for creating the context vector is precisely a weighted sum (a linear combination) of these encoder hidden states, where the weights are the attention scores. Therefore, **F is correct**.

**Why the others are wrong:**
*   **A & C:** These describe *local* attention, which uses a window.
*   **B & E:** Attention is computed using the *encoder* hidden states to inform the *decoder*. Using decoder states for this would not provide any new information about the input.

---

### **Question 8: Local Attention**

**Correct Answers: A, E**

*   **A. In the local attention, the time varied context is computed based on all encoder hidden states in a selective window.** (Correct)
*   **E. In the local attention, the time varied context is a linear combination of all encoder hidden states in a selective window.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Computational Cost of Global Attention:** For very long sequences (e.g., documents), calculating attention over *all* encoder steps for *every* decoder step becomes computationally expensive.
2.  **Local Focus Intuition:** For many tasks, the most relevant information for generating the next output word is found in a small, contiguous segment of the input.

#### Step-by-Step Explanation

*   Local attention is a trade-off. Instead of looking at the entire input sequence, it only looks at a subset (a window) of encoder hidden states centered around a predicted aligned position. Therefore, **A is correct**.
*   Just like global attention, the context vector is formed by a weighted sum. The key difference is that the sum is only over the encoder hidden states that fall within the selected window. Therefore, **E is correct**.

**Why the others are wrong:**
*   **B, C, D, F:** These are either about decoder states or about using *all* encoder states, which defines global attention, not local.

---

### **Question 9: Global Attention Calculation**

**Correct Answer: C**

*   **C. $c_t = 0.2\mathbf{\bar{h}}_1 + 0.5\mathbf{\bar{h}}_2 + 0.2\mathbf{\bar{h}}_3 + 0.1\mathbf{\bar{h}}_4$** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Reading Attention Diagrams:** The values above the arrows (0.2, 0.5, 0.2, 0.1) represent the attention weights ($\alpha_{t,i}$) for the current decoder step $t$.
2.  **Context Vector Formula:** The context vector is always $\mathbf{c}_t = \sum_i \alpha_{t,i} \mathbf{\bar{h}}_i$.

#### Step-by-Step Explanation

*   The diagram clearly shows the weights connecting the decoder state to each encoder state. The highest weight is 0.5 for $\mathbf{\bar{h}}_2$, meaning the second input word is the most important for generating the current output word.
*   The context vector $\mathbf{c}_t$ is computed by multiplying each encoder hidden state by its corresponding attention weight and summing the results. This matches option **C** exactly.

**Why the others are wrong:**
*   **A & B:** While $\mathbf{\bar{h}}_2$ has the highest weight (0.5), the question asks for what is "correct". A and B are interpretations, but C is the definitive, calculable fact from the diagram.
*   **D:** $\mathbf{c}_t$ is a weighted combination of *encoder* states, not the decoder's own state $\mathbf{h}_t$.
*   **E:** This is the wrong order of weights.

---

### **Question 10: Positional Encoding in Transformers**

**Correct Answers: B, D**

*   **B. It helps capture the position of a word/token in a sentence.** (Correct)
*   **D. It is added to the embeddings of words/tokens in a sentence.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Self-Attention is Permutation-Invariant:** The self-attention operation has no inherent notion of order. If you shuffle the input tokens, the output will be the same shuffled set, just transformed. Order information is lost.
2.  **Injecting Positional Information:** To overcome this, we need to explicitly tell the model the position of each token.

#### Step-by-Step Explanation

*   The sole purpose of positional encoding is to provide the model with information about the absolute (and sometimes relative) position of each token in the sequence. Therefore, **B is correct**.
*   The standard method is to create a matrix of positional encodings that has the same dimensions as the word embedding matrix. This matrix is then *added* to the word embeddings before being passed to the first transformer layer. Therefore, **D is correct**.

**Why the others are wrong:**
*   **A:** It deals with the position within a *sentence*, not the position of a sentence in a batch.
*   **C:** It does not produce the word embeddings themselves; that's the job of the initial embedding lookup layer.
*   **E:** The "main signal" is the word embedding. The positional encoding is an additive *modification* to this signal.

---

### **Question 11: Layer Normalization in Transformers**

**Correct Answers: B, D, E**

*   **B. It normalizes the input tensor across the embedding size dimension (i.e., the dimension of d_model).** (Correct)
*   **D. It has the scaling and shifting parameters ùõæ and ùõΩ.** (Correct)
*   **E. It is more effective than Batch Norm for sequential data.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Normalization Goals:** Stabilize and accelerate training by reducing "internal covariate shift". Ensure activations don't become too large or too small.
2.  **Batch Norm vs. Layer Norm:** The difference is in which dimensions are normalized.
    *   **Batch Norm:** Normalizes across the *batch* dimension for each feature. Mean and variance are calculated for each feature across all examples in the batch.
    *   **Layer Norm:** Normalizes across the *feature* dimension for each example. Mean and variance are calculated for each example across all its features.

#### Step-by-Step Explanation

*   For an input tensor of shape `[batch_size, seq_len, d_model]`, Layer Norm computes the mean and standard deviation along the last dimension (`d_model`), independently for each example and each time step. Therefore, **B is correct**.
*   Like Batch Norm, Layer Norm has learnable parameters $\gamma$ (scale) and $\beta$ (shift) to restore the representation power of the network after normalization. Therefore, **D is correct**.
*   For sequences of variable length and small batch sizes (common in NLP), Batch Norm statistics are unstable. Layer Norm's statistics are computed per-example, making it more stable and effective for sequential data. Therefore, **D is correct**.

**Why the others are wrong:**
*   **A:** This describes Batch Norm.
*   **C:** It has parameters ($\gamma$, $\beta$).
*   **F:** This is the opposite of the truth.

---

### **Question 12: Self-Attention Dependencies**

**Correct Answers: B, C**

*   **B. The token embedding $z_i$ is mainly dependent on its previous token embedding $x_i$, but other $x_j (ùëó ‚â† ùëñ)$ also contributes to the computation of $z_i$.** (Correct)
*   **C. More similar $x_j$ is to $x_i$, more contribution it is to the the computation of $z_i$.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Self-Attention is "Self-Referential":** The output for a position is a weighted sum of the *transformations* of *all* positions in the input sequence.
2.  **Similarity Drives Attention:** The attention weight between $x_i$ and $x_j$ is a function of their similarity (via the Query and Key vectors).

#### Step-by-Step Explanation

*   The output $z_i$ is computed as $z_i = \sum_j \alpha_{ij} (x_j W_V)$. Since the sum is over all $j$, *every* input token contributes to $z_i$. However, the weight $\alpha_{ii}$ for the token itself is often significant. Therefore, **B is correct**.
*   The attention score $\alpha_{ij}$ is calculated as $\text{softmax}(\frac{(x_i W_Q)(x_j W_K)^T}{\sqrt{d_k}})$. The dot product $(x_i W_Q)(x_j W_K)^T$ is larger when the Query of $i$ and the Key of $j$ are more similar. A higher score leads to a higher weight $\alpha_{ij}$ and thus a larger contribution from $x_j$ to $z_i$. Therefore, **C is correct**.

**Why the others are wrong:**
*   **A:** This would be true for a simple feed-forward layer, but self-attention is fundamentally designed to mix information from all tokens.
*   **D:** This is the opposite of how attention works.

---

### **Question 13: Self-Attention Layer Mechanics**

**Correct Answers: A, C, D, E, G**

*   **A. We use three weight matrices $W_Q$, $W_K$, $W_V$ to compute $Q$, $K$, $V$ respectively.** (Correct)
*   **C. We rely on $Q$, $K$ to compute the attention scores to store in a matrix $B$ that has shape [L,L].** (Correct)
*   **D. $Q$, $K$ can be considered as two other views of $X$.** (Correct)
*   **E. We apply the softmax function to the attention scores $B$ to gain the attention probabilities $A$ that has shape [L,L].** (Correct)
*   **G. We multiply $A$ and $V$ to obtain the new token/word embeddings $Z = AV$.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Query, Key, Value Analogy:** Think of a search engine.
    *   **Query ($Q$):** What you are looking for (from the current token's perspective).
    *   **Key ($K$):** What each token *has* or *is about*.
    *   **Value ($V$):** The actual *content* or *information* that will be retrieved from each token.
2.  **Matrix Multiplication in Self-Attention:** The entire process is highly parallelizable using matrix operations.

#### Step-by-Step Explanation

*   The input $X$ is projected into three different spaces using three learned weight matrices to create $Q$, $K$, and $V$. **A is correct**.
*   $Q$ and $K$ are different linear projections of the same input $X$, creating two different "views" used for comparison. **D is correct**.
*   The attention scores $B$ are computed as $B = QK^T$. This matrix has shape `[L, L]` where each element $B_{ij}$ is the score between the $i$-th query and the $j$-th key. **C is correct**.
*   To turn scores into probabilities that sum to 1 for each target token (row), we apply softmax: $A = \text{softmax}(B)$. **E is correct**.
*   The final output is the weighted sum of value vectors: $Z = AV$. **G is correct**.

**Why the others are wrong:**
*   **B:** Scores are computed from $Q$ and $K$, not $Q$ and $V$.
*   **F:** We multiply the attention *probabilities* $A$ with $V$, not the raw scores $B$.

---

### **Question 14: Multi-Head Self-Attention**

**Correct Answers: A, C, F**

*   **A. Each head has its own $W_Q$, $W_K$, $W_V$.** (Correct)
*   **C. We perform each head independently.** (Correct)
*   **F. We concatenate the outputs of each head and input this concatenation to one more linear layer $W_o$ to gain the output of multi-head Self-Attention.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **The Power of Ensembles:** Multi-head attention is like having multiple "attention experts". Each head can learn to focus on different types of relationships (e.g., syntactic vs. semantic, long-range vs. short-range).

#### Step-by-Step Explanation

*   In multi-head attention, the input is projected *h* times with *h* different sets of $W_Q^i, W_K^i, W_V^i$ matrices, creating *h* separate heads. **A is correct**.
*   The self-attention computation for each head is performed independently and in parallel. **C is correct**.
*   The outputs of all heads (each a matrix of shape `[L, d_model/h]`) are concatenated into a single matrix of shape `[L, d_model]`. This is then passed through a final linear projection $W_O$ to produce the multi-head output. **F is correct**.

**Why the others are wrong:**
*   **B:** The weights are *not* shared; this is the key to having diverse heads.
*   **D:** The heads are computed independently; their outputs are not conditionally dependent on each other during the forward pass.
*   **E:** We don't just use the concatenation; the final linear layer $W_O$ is a crucial part of the architecture, allowing the model to mix information from the different heads.

---

### **Question 15: Cross-Attention**

**Correct Answers: A, C, F**

*   **A. We use the Cross-Attention to inject the encoder output to the decoder layers.** (Correct)
*   **C. For the Cross-Attention, the decoder input is used to compute $Q$, whereas the encoder output is used to compute $K$, $V$.** (Correct)
*   **F. The Cross-Attention is involved in the computation of decoder output.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Encoder-Decoder Attention:** In a transformer decoder, there are two attention sub-layers: self-attention (on the decoder input) and cross-attention (from decoder to encoder).
2.  **Query Source Dictates the "Perspective":** The entity that produces the Query is the one that is "asking the question" or "attending over" the source that produces the Keys and Values.

#### Step-by-Step Explanation

*   The purpose of cross-attention is to allow each position in the decoder to attend over all positions in the encoder's output. This is how we "inject" the encoded input information into the decoding process. **A is correct**.
*   The decoder has the "active" role of generating the next token. It uses its own state to "query" the encoder's memory. Therefore, the **Decoder** produces the **Queries ($Q$)**. The **Encoder** output, which is the memory we are querying, provides the **Keys ($K$)** and **Values ($V$)**. **C is correct**.
*   The output of the cross-attention layer is a combination of the encoder's information, which is then passed to the decoder's feed-forward network to help predict the next output token. Therefore, it is fundamentally involved in computing the decoder's output. **F is correct**.

**Why the others are wrong:**
*   **B:** It depends on both the decoder input (for Q) and the encoder output (for K, V).
*   **D:** This has the roles of Q and K/V reversed.
*   **E:** Cross-attention is a component of the *decoder* stack, not the encoder.

---

![[FIT3181_5215-L10-Quiz.pdf]]

Of course, Emeka. Let's work through this quiz on Vision Transformers (ViTs) and fine-tuning techniques. We'll follow the same rigorous, concept-focused approach as before.

---

### **Question 1: Self-Attention Properties**

**Correct Answers: A, C, E**

*   **A. With self-attention, we can compute the token embeddings in parallel.** (Correct)
*   **C. For self-attention, the queries (Q), keys (K), and values (V) computed based on source sequences** (Correct)
*   **E. Self-attention can capture intra-sequence dependencies between source sequences** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Parallelizability of Self-Attention:** Unlike RNNs which process tokens sequentially, the self-attention mechanism computes the relationships between all pairs of tokens simultaneously using matrix operations ($QK^T$).
2.  **"Self" Referential Nature:** In *self*-attention, the Query, Key, and Value vectors are all derived from the *same* sequence. It's the sequence attending to itself.
3.  **Intra vs. Inter-sequence:** *Intra*-sequence means within a single sequence. *Inter*-sequence means between two different sequences (e.g., source and target).

#### Step-by-Step Explanation

*   The core self-attention operation $Z = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V$ is a series of matrix multiplications that can be computed all at once for the entire sequence. This makes it highly parallelizable. Therefore, **A is correct**.
*   Since it is *self*-attention, the $Q$, $K$, and $V$ matrices are all linear projections of the same input sequence $X$. Therefore, **C is correct**.
*   The purpose of self-attention is to let each token in a sequence interact with and gather information from every other token in the *same* sequence. This is the definition of capturing *intra*-sequence dependencies. Therefore, **E is correct**.

**Why the others are wrong:**
*   **B:** This describes the sequential nature of RNNs, not self-attention.
*   **D:** This describes *cross-attention* in an encoder-decoder architecture, not *self-attention*.
*   **F:** This also describes *cross-attention*. Self-attention operates on a single sequence.

---

### **Question 2: Cross-Attention Properties**

**Correct Answers: A, D, F**

*   **D. For self-attention, the queries (Q) are computed based on target sequences, while keys (K), and values (V) are computed based on source sequences** (Correct)
*   **F. Self-attention can capture inter-sequence dependencies between source sequences and target sequences** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Cross-Attention Mechanism:** This is the attention mechanism used in the decoder of a transformer to allow the *target* sequence being generated to "attend to" the encoded *source* sequence.
2.  **Query Source Dictates Perspective:** The entity that produces the Queries is the one that is "actively looking" for information.

#### Step-by-Step Explanation

*   In cross-attention, the goal is for the decoder to retrieve relevant information from the encoder's memory. Therefore, the **decoder's hidden states** (from the target sequence) are used to create the **Queries (Q)**. The **encoder's output** (the source sequence representation) is used to create the **Keys (K)** and **Values (V)**. Therefore, **D is correct**.
*   The very purpose of this mechanism is to create a dependency between the source and target sequences, allowing the model to align words across languages (in translation) or find relevant context. This is the definition of capturing *inter*-sequence dependencies. Therefore, **F is correct**.

**Why the others are wrong:**
*   **A, B, C, E:** These all describe properties of *self-attention*, not *cross-attention*.

---

### **Question 3: CNN Properties**

**Correct Answers: A, B, D, E**

*   **A. CNNs cannot capture the global information of images.** (Correct, in early layers)
*   **B. CNN can capture the local information of images.** (Correct)
*   **D. CNNs are locality sensitivity** (Correct)
*   **E. CNNs combine local patterns to learn broader local patterns** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Convolutional Inductive Bias:** The core assumption of CNNs is that of **locality** and **translation equivariance**. A filter's weights are applied to a small local receptive field, making it sensitive to local patterns like edges and corners.
2.  **Hierarchical Feature Learning:** Through stacking convolutional layers, CNNs combine these local patterns into more complex, broader patterns (e.g., edges -> eyes -> faces).

#### Step-by-Step Explanation

*   A single convolutional filter only "sees" a small patch of the image at a time (e.g., 3x3 pixels). Without many down-sampling (pooling) layers, it cannot integrate information from distant parts of the image, limiting its ability to capture *global* context in early layers. Therefore, **A is correct**.
*   This local receptive field is precisely what makes CNNs excellent at capturing *local* information. **B is correct**.
*   "Locality sensitivity" is another way of stating the convolutional inductive bias. **D is correct**.
*   This is the fundamental principle of deep hierarchical learning in CNNs. **E is correct**.

**Why the others are wrong:**
*   **C:** CNNs are very good at capturing spatial relationships *within* their receptive field. Their hierarchical nature allows them to build up an understanding of spatial relationships between objects, though this is less direct than in attention-based models.

---

### **Question 4: Vision Transformer (ViT) Tokenization**

**Correct Answers: A, D, F, G**

*   **A. For ViTs, a token or visual word is a patch of an image.** (Correct)
*   **D. We apply a linear projection to the flattened patches to transform them to token embeddings.** (Correct)
*   **F. We inject the class token to the token embeddings of the patches and learn the class token during training** (Correct)
*   **G. On top of the class token at the final layer, we build up the MLP head to make predictions.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Image as a Sequence:** ViTs treat an image not as a grid but as a sequence of patches, analogous to words in a sentence.
2.  **Class Token ([CLS]):** Inspired by BERT, a special learnable token is prepended to the sequence. Through self-attention, it aggregates information from all other tokens and serves as the global image representation for classification.

#### Step-by-Step Explanation

*   An image is split into fixed-size patches (e.g., 16x16). Each patch is treated as a "visual token". **A is correct**.
*   These flattened image patches are not used directly. They are projected into an embedding space using a trainable linear layer. **D is correct**.
*   The class token is a randomly initialized, learnable vector that is prepended to the patch tokens. Its value is updated via backpropagation during training. **F is correct**.
*   After processing by the transformer encoder, the final state of the class token is used as the input to the MLP head for making the final prediction (e.g., image class). **G is correct**.

**Why the others are wrong:**
*   **B:** A single pixel carries very little semantic information. A patch containing many pixels is a meaningful "visual word".
*   **C:** We project the raw pixel values of the patches into an embedding space first.
*   **E:** The class token is *learned*, not kept fixed.

---

### **Question 5: How ViTs Capture Global Information**

**Correct Answers: C, D**

*   **C. This is because the Multi-head Self-attention layers of the Encoder blocks.** (Correct)
*   **D. The global information is captured in the class token at the final layer because this summarizes the token embeddings at the input layer.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Global Receptive Field of Self-Attention:** From the first layer, each token in a self-attention mechanism can attend to every other token in the sequence. This gives it an immediate, theoretically global receptive field.
2.  **The Role of the [CLS] Token:** The self-attention mechanism allows the class token to attend to all patch tokens. Through the layers, it integrates this information to form a comprehensive summary.

#### Step-by-Step Explanation

*   The multi-head self-attention (MSA) layers are the components that explicitly compute the relationships between all pairs of patches. This is the direct mechanism that allows any patch to influence any other, enabling the capture of global information. **C is correct**.
*   The class token is designed to be a "summary token". Through the MSA layers in all encoder blocks, it gathers contextual information from all other patch tokens. By the final layer, its embedding contains a global representation of the entire image. **D is correct**.

**Why the others are wrong:**
*   **A & B:** The Feed-Forward Network (FFN) and Add & Norm operations are important, but they operate on a *per-token* basis and do not directly facilitate communication *between* tokens. The MSA layer is the key component for global context.
*   **E:** The class token at the *input* layer is just a randomly initialized vector. It has not yet summarized anything.

---

### **Question 6: ViT Training and Robustness**

**Correct Answers: A, C, D**

*   **A. ViTs can naturally capture the global information of images.** (Correct)
*   **C. ViTs can find the long-term dependencies among image patches.** (Correct)
*   **D. We need massive datasets to train ViTs.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Lack of Spatial Inductive Bias:** CNNs have a built-in bias for locality. ViTs have no such bias‚Äîthey must *learn* from data that nearby patches are often related. This makes them more flexible but also more data-hungry.
2.  **Long-Term Dependencies:** Self-attention's global receptive field makes it inherently good at connecting distant parts of an image.

#### Step-by-Step Explanation

*   As explained in Q5, the self-attention mechanism gives ViTs a global receptive field from the start. **A is correct**.
*   "Long-term dependencies" refer to relationships between elements far apart in the sequence. Since self-attention has a direct connection between all tokens, it does not suffer from the vanishing gradient problem that plagues RNNs and can easily model these long-range relationships. **C is correct**.
*   Because ViTs lack the spatial inductive biases of CNNs, they require learning these relationships from scratch. This requires large amounts of data (e.g., JFT-300M) to generalize well, unlike CNNs which can perform well on smaller datasets. **D is correct**.

**Why the others are wrong:**
*   **B:** This is the opposite of the truth. ViTs generally underperform CNNs on small datasets without strong regularization.
*   **E:** The quiz document states "ViTs are *less* robust to patch permutation and occlusion than CNNs". This is because CNNs process the image grid with translation-equivariant filters, making them somewhat robust to local shuffling. ViTs, with their patch-based input and positional encodings, are more sensitive to the structure of the sequence.

---

### **Question 7: Swin Transformer Patch Processing**

**Correct Answers: A, C, F**

*   **A. Swin Transformers employ smaller patches of [3,4,4].** (Correct)
*   **C. Swin Transformers apply a linear projection to flattened patches to gain [C, H/4, W/4].** (Correct)
*   **F. For Swin Transformers, we apply patch merging to down-sample the input shape by two while doubling the depth.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Swin Transformer Goal:** To be a hierarchical vision transformer that is efficient and can serve as a general-purpose backbone for dense prediction tasks (like segmentation, detection), similar to CNNs.
2.  **Hierarchical Structure:** It processes the image at multiple scales, unlike the standard ViT which maintains a single scale.

#### Step-by-Step Explanation

*   Swin Transformers often use a smaller initial patch size (e.g., 4x4) compared to ViT (16x16) to provide a finer-grained initial representation. **A is correct**.
*   With a 4x4 patch and non-overlapping splitting, the number of patches is $(H/4) \times (W/4)$. The linear projection creates a feature map of this spatial dimension. **C is correct**.
*   Patch merging is Swin's equivalent to pooling in CNNs. It groups 2x2 neighboring patches, concatenates their features, and applies a linear layer to reduce the spatial size by 2 and increase the channel size (depth) by 2. **F is correct**.

**Why the others are wrong:**
*   **B:** This is the patch size for the original ViT, not Swin.
*   **D:** This would be the output shape for a ViT with 16x16 patches.
*   **E:** The input shape changes through the stages due to patch merging.

---

### **Question 8: Patch Merging Details**

**Correct Answers: A, E**

*   **A. We merge 2x2 neighbourhood patches, concatenate their embeddings, and then apply a linear projection.** (Correct)
*   **E. If we input the patch merging [C, H/4, W/4], we gain [2C, H/8, W/8]** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Down-sampling Operation:** The goal is to reduce the spatial resolution while increasing the receptive field and the richness (channels) of the features.

#### Step-by-Step Explanation

*   This is the exact procedure for patch merging. It's analogous to a 2x2 pooling but with a learnable linear projection. **A is correct**.
*   Merging 2x2 patches reduces both H and W by a factor of 2 ($H/4 -> H/8$, $W/4 -> W/8$). Concatenating 4 patches of C channels gives $4C$ channels. The subsequent linear projection typically reduces this to $2C$ channels, effectively doubling the depth. **E is correct**.

**Why the others are wrong:**
*   **B:** The linear projection is applied *after* concatenation, not directly to the embeddings.
*   **C & D:** The spatial dimensions (H, W) are reduced, not kept the same.

---

### **Question 9: Window Self-Attention**

**Correct Answers: B, D**

*   **B. We divide all token embeddings into many local windows and then apply the Self-Attention to each local windows independently.** (Correct)
*   **D. The output shape of Window Self-Attention is the same as the input shape.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Computational Complexity of Self-Attention:** Standard self-attention has $O(L^2)$ complexity, where L is the sequence length. For high-resolution images, L is large, making it slow and memory-intensive.
2.  **Window Attention Solution:** By restricting attention to non-overlapping local windows, the complexity becomes $O(M^2 \times L)$, where M is the window size (a fixed constant), making it linear with respect to L.

#### Step-by-Step Explanation

*   This is the definition of windowed self-attention. The image is divided into a grid of non-overlapping windows (e.g., 7x7 patches each), and standard self-attention is computed within each window. **B is correct**.
*   The operation is performed *within* each window and does not change the number of tokens or their order. The output is just a transformed version of the input tokens, so the shape remains the same. **D is correct**.

**Why the others are wrong:**
*   **A:** This describes standard, global self-attention.
*   **C:** The shape is preserved.

---

### **Question 10: Window Self-Attention Properties**

**Correct Answers: A, C**

*   **A. The Window Self-Attention can speed up the standard Self-Attention** (Correct)
*   **C. The Window Self-Attention only allows a token to interact with the ones in the same local window.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Trade-off:** Window attention gains efficiency but loses the global receptive field.

#### Step-by-Step Explanation

*   As explained in Q9, by reducing the computational complexity from quadratic to linear, window attention is significantly faster and more memory-efficient than standard global self-attention. **A is correct**.
*   This is the direct consequence and limitation of the approach. A token in one window has no interaction with tokens in other windows in a given layer. **C is correct**.

**Why the others are wrong:**
*   **B:** It is faster, not slower.
*   **D & E:** These are properties of the *Shifted* Window Self-Attention, which was designed to solve this exact limitation.

---

### **Question 11: Shifted Window Self-Attention**

**Correct Answers: B, C, D, F**

*   **B. The Shifted Window Self-Attention allows a token to interact with the ones in the different local windows.** (Correct)
*   **C. The Shifted Window Self-Attention enables the interaction across local windows.** (Correct)
*   **D. The Shifted Window Self-Attention shifts a local window to right and bottom to become a new local window** (Correct)
*   **F. The output shape of Shifted Window Self-Attention is the same as the input shape.** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Bridging Windows:** The Swin Transformer uses two consecutive transformer blocks in each stage. The first uses regular window partitioning. The second uses a window partition that is shifted by (window_size//2) pixels, creating new windows that cross the boundaries of the old ones.

#### Step-by-Step Explanation

*   By shifting the windows, a token that was in one window in the first block can interact with tokens from neighboring windows in the second block. **B and C are correct** (they are essentially saying the same thing).
*   The specific mechanism to achieve this cross-window connection is to shift the window partitioning grid. **D is correct**.
*   Just like standard window attention, this operation does not change the number or order of tokens; it only changes how they are grouped for attention. Therefore, the output shape is preserved. **F is correct**.

**Why the others are wrong:**
*   **A:** This is the limitation of the *non-shifted* window attention.
*   **E:** The shape remains the same.

---

### **Question 12: Fine-tuning Strategy Philosophy**

**Correct Answers: A, C**

*   **A. We insert additional components to pretrained ViTs that favour the original computation of ViTs and then fine-tune the additional components** (Correct)
*   **C. We insert additional components to pretrained ViTs that favour the original computation of ViTs and then consider the additional components as variables to optimize in optimizers** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Parameter-Efficient Fine-Tuning (PEFT) Goal:** To adapt a large pre-trained model to a new task without fine-tuning all of its parameters, which is computationally expensive and can lead to catastrophic forgetting.

#### Step-by-Step Explanation

*   The core idea of PEFT methods like Prompts, Adapters, and LoRA is to *augment* the pre-trained model with a small number of new, task-specific parameters. The original model's weights are typically kept frozen (or mostly frozen), preserving its original, powerful computation. **A is correct**.
*   These new, additional components (prompts, adapter weights, LoRA matrices) are the only parameters, or the primary parameters, that are updated (optimized) during fine-tuning on the new dataset. **C is correct**.

**Why the others are wrong:**
*   **B:** If you freeze the additional components, they cannot adapt to the new task.
*   **D:** The whole point of these methods is to *avoid* significant modification of the original computation.

---

### **Question 13: Prefix Prompt-Tuning**

**Correct Answer: A**

*   **A. We insert learnable prompts to token embeddings of ViTs and then fine-tune these prompts** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Prompting:** Inspired by NLP, this method prepends a small number of learnable vectors (prompts) to the sequence of input tokens.

#### Step-by-Step Explanation

*   In Visual Prompt Tuning (VPT), a set of learnable parameters (the "prompts") are concatenated with the patch embeddings (and class token) at the *input* of the transformer encoder. Only these prompts and the classification head are fine-tuned, while the rest of the ViT backbone is frozen. **A is correct**.

**Why the others are wrong:**
*   **B & C:** The prompts are inserted into the token/embedding stream at the input stage, not into the FFN or the Q/K/V projection matrices of the self-attention blocks.

---

### **Question 14: Adapters**

**Correct Answer: B**

*   **B. We insert adapters to pointwise networks of ViTs and then fine-tune these adapters** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **Adapter Module:** A small, bottleneck feed-forward network that is inserted *within* a transformer block, typically after the Multi-head Self-Attention (MSA) and/or after the Feed-Forward Network (FFN).

#### Step-by-Step Explanation

*   The most common placement for adapters in a transformer is after the FFN (the "pointwise network" or MLP) within each block. The adapter projects the features down to a low dimension, applies a non-linearity, and projects back up. Only the adapter weights are fine-tuned. **B is correct**.

**Why the others are wrong:**
*   **A & C:** Adapters are not typically inserted at the token embedding level or directly into the Q/K/V matrices.

---

### **Question 15: LoRA (Low-Rank Adaptation)**

**Correct Answer: C**

*   **C. We insert low-ranked matrices to the key, query, and value matrices of ViTs and then fine-tune these low-ranked matrices** (Correct)

#### Prerequisite Knowledge & Refresher

1.  **LoRA's Hypothesis:** The weight update ($\Delta W$) for a pre-trained weight matrix $W$ during adaptation has a low "intrinsic rank".
2.  **Method:** Instead of fine-tuning $W$, LoRA represents the update as $\Delta W = BA$, where $B$ and $A$ are low-rank matrices. $W + \Delta W$ is used for the forward pass, but only $A$ and $B$ are trained.

#### Step-by-Step Explanation

*   In transformers, LoRA is typically applied to the attention projection matrices ($W_Q$, $W_K$, $W_V$) in the self-attention layers. The pre-trained $W$ is frozen, and the low-rank matrices $A$ and $B$ are injected to model the adaptation $\Delta W$. **C is correct**.

**Why the others are wrong:**
*   **A & B:** While theoretically possible, the standard and most effective application of LoRA is to the attention mechanism's weights, not the token embeddings or the FFN layers.

---

This concludes the quiz, Emeka. You're exploring some of the most advanced and impactful areas of deep learning. If any part of these explanations needs further unpacking, just let me know.

---


![[FIT5215-L11-Quiz.pdf]]