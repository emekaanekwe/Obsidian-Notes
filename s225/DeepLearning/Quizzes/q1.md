# Wk 2

![[FIT3181_5215-W2_Quiz.pdf]]


# Wk3

![[FIT3181_5215-L04-Quiz.pdf]]

### Calculate Output Tensor, given 3D tensor
*The same process applies to pooling*
![[Pasted image 20250910100303.png]]

### Calculate the Output Tensor Shape
![[Pasted image 20250910100731.png]]
$$Tensorshape = [CNN[0], Filters, H_{out}, W_{out}]$$

### Tensor Shape from Code

```Python
import torch
import torch.nn as nn

cnn = nn.Sequential(
    nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # Note: stride=2 (not 3)
    nn.ReLU(),
    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
    nn.ReLU()
)

x = torch.rand(1, 3, 224, 224)  # Input shape: [batch_size=1, channels=3, height=224, width=224]
print(cnn(x).shape)
```

Using $H_{out} \ and \ W_{out}$, the output shape before ReLu is [1,32,112,112]
Putting it through again, we get [1,64,112,112]

For **`Conv2d(), ReLu()`**
	input shape = $[1,3,224,224]$, output shape =  $[1*1, 3*32(first), 224/2, 224/2]$

For `BatchNorm(), MaxPool()`
	input shape = $[initial, Conv2d(width, pass2), size/4,size/4]$, output shape =  $[64, 64, 56, 56]$

### Calculate the # of Neurons

$$NeuronNum = Filters \times  W_{out} \times H_{out}$$

### Causes of Overfitting

#### Large Filter + Deep Models + Few Images

### Formal Structure of CNNs
![[Pasted image 20250910103524.png]]

$$\begin{matrix}  Input := [A1,B1,C1,D1] = [Total \ Layers, Preprocess \ Layers, Height, Width] \\ Filter := [F1,F2,F3,F4]=[Total \ Filters, Processed, Height, Width]\\ Pooling(Feature1) := [A2, B2, C2, D2] = [Total \ Features, Feature1 \ Layers \ \#, Width, Height] \\ Pooling(Feature2) := [A3, B3, C3, D3] = [Total \ Features, Feature2 \ Layers \ \#, Width, Height] \\ FC:= [E, F] = [Height, Width (4, 256)] \\ SoftMax := [G, H]=[\# \ of \ Neurons, \# \ of \ Classes ]    \end{matrix} $$

### Row 2 of CNN post-softmax

Assume a tensor shape¬†`[4, 10]`:
    The first dimension (size 4) is the¬†**batch size**¬†(i.e., there are 4 images in the batch).
    The second dimension (size 10) is the¬†**number of classes**¬†(e.g., for a classification task with 10 categories).
Since the tensor is¬†**after softmax**, each row represents the¬†*prediction probabilities*¬†for an image in the batch

### Purpose of Conv2D Tensors

#### 1. Dimension reduction
#### 2. Pattern detection filtering

### Purpose of Pooling Layers

#### 1. Preserve input integrity
#### 2. Reduce input size

### Purpose of Batch Norm Layers

#### 1. Mitigate Overfitting
#### 2. Get the Gaussian distro of training data and testing data




# Wk4

![[FIT3181-5215-L4-Quiz.pdf]]



### FF Networks: Optimization Problems

#### Concerning regularization $\Omega(\theta)$
$$min_ùúÉ ùêΩ( ùúÉ) = ùõ∫ ùúÉ + 1/N \sum^N_{i=1}CE(y_i,f(x;\theta))$$

$\theta$ is the weight and bias at a particular part of the loss
$\Omega (\theta)$ is the regularizer, i.e. sum of the eigenvalues to **prevent over fitting**, **makes the model simpler**, and **iterates the weights towards 0**

#### Concerning the empirical loss $\frac{1}{N} \sum^N_{i=1}CE(y_i,f(x;\theta))$
$$min_ùúÉ ùêΩ( ùúÉ) = ùõ∫ ùúÉ + 1/N \sum^N_{i=1}CE(y_i,f(x;\theta))$$
$f(x;\theta)$ is the prediction Pr, the empirical loss **improves model fitting to training set**, but **could over fit**

### Calculating GD

#### Update Rule
$$Œ∏t+1‚Äã=Œ∏t‚Äã‚àíŒ∑‚ãÖ‚àáf(Œ∏t‚Äã)$$

Assume:        $f(Œ∏)=Œ∏^2‚àí2Œ∏+1$, learning rate: $0.1$, 
**compute** the gradient of¬†$f(Œ∏)$¬†with respect to¬†$Œ∏$:

We can treat the formula like a quadratic equation: $f(x) = x^2+x+c$
	get the derivative
	then **plug back into the update rule**
	calculate the derivative again if needed

### Calculating SGD

Assume: there are 4 indices
#### Update rule
$$Œ∏t+1‚Äã=Œ∏t‚Äã‚àíŒ∑‚ãÖ‚àáf^‚Äã(Œ∏t‚Äã)$$
#### Mini Batch Loss
$$\hat{f}^‚Äã(Œ∏)=\frac{1}{4}‚àë^{4}_{j=1}‚Äã(Œ∏‚àíi_j‚Äã)^2$$
1. plug in the values assumed in the batch loss
2. compute the gradient for $\hat{f}$
	$$‚àáf^‚Äã(Œ∏)=‚Äãf'^‚Äã(Œ∏)=\frac{1}{4}\times\frac{d}{dx}\sum$$
	which requires the derivative of each term. $$(Œ∏‚àí1‚Äã)^2+(Œ∏‚àí2)^2+(Œ∏‚àí3‚Äã)^2+(Œ∏‚àí4)^2$$
plug in the $\theta$ value and compute:$$...(Œ∏=10‚àíx_i)^2...$$
apply the SGD update by plugging in all the values:
$$Œ∏t+1‚Äã=Œ∏t‚Äã‚àíŒ∑‚ãÖ‚àáf^‚Äã(Œ∏t‚Äã)$$

### $minL()$ Optimization Problem

#### Structure
$$min ùúÉ ùêø ùê∑; ùúÉ ‚âî -\frac{1}{ùëÅ}\sum  \Delta  ùëô(ùë•_{i_k}, ùë¶{i_k}; ùúÉ) $$
notice how the updating is **negative** for $x_k$ **batches**

### Gradient and BP via Code
Consider:
```Python
x = torch.tensor([1.0,2.0,3.0], requires_grad=False)
y = torch.tensor([4.0])
W = torch.rand(3,1, requires_grad=True)
b = torch.rand(1, requires_grad=True)

y_hat = torch.matmul(x,W)+b
l = (y_hat - y)**2

# BP
l.bakward(retain_graph=True)

```

1. Computes the gradient of loss for weight **and** bias
2. weight and bias both **store the training data** and **real values**
3. For BP, computes back to **previous weights** and goes **from $\hat{y}$ to x**

### Activation Functions in Code

#### Sigmoid
```Python
 1./(1+np.exp(-x))
```
#### ReLU
```Python
 x*(x>0)
```
#### tanH (hyperbolic)
```Python
 (np.exp(n)-1.)/(np.exp(n)+1.) 
```
#### softmax
```Python
 np.exp(x)/np.sum(np.exp(x))
```

### Calculating Activation Functions

#### Which is ùúï‚Ñé ùúï‚Ñé if ‚Ñé = ùúé(‚Ñé), ‚Ñé is a vector and ùúé is an activation function?
$Diag(\sigma'(\hat{h}))$, or $I$

#### ReLU calculation
Simply reduce the matrix to diag 1
# wk5
![[FIT3181_5215_L05_Quiz.pdf]]



# Wk6
![[FIT3181_5215-L06-Quiz.pdf]]



Of course. Here is a detailed summary of the two quizzes, including step-by-step calculations and explanations for the relevant questions.

### Summary of FIT3181_5215_L05_Quiz.pdf: "Practical Skills in Deep Learning"

This quiz tests fundamental concepts related to training and regularizing deep neural networks.

**Key Topics Covered:** Gradient norms, parameter initialization, vanishing/exploding gradients, underfitting/overfitting, regularization techniques, batch normalization, and data augmentation.

---

**Detailed Question Breakdown:**

**Question 1:** What does it mean if the gradient norm `||g||‚ÇÇ` decreases to 0?
*   **Correct Answer: D. One of A, B, C**
*   **Explanation:** A gradient norm of zero indicates a critical point where the gradient is flat. This critical point could be a local minimum (A), a local maximum (B), or a saddle point (C). Without additional information (like the eigenvalues of the Hessian matrix), we cannot distinguish between them.

**Question 2:** Which activation function is Xavier (Glorot) initialization good for?
*   **Correct Answer: B. Tanh** (and also Sigmoid, though it's less common now)
*   **Explanation:** Xavier initialization is derived assuming linear activation functions with unit derivative. The Tanh function is approximately linear around 0, making it a good fit. While it can be used for Sigmoid, ReLU is better served by He initialization.

**Question 3:** Which activation function is He initialization good for?
*   **Correct Answer: C. ReLU**
*   **Explanation:** He initialization is specifically designed for ReLU (and its variants, like Leaky ReLU) activation functions. It accounts for the fact that ReLU sets half of its inputs to zero, effectively halving the variance compared to a linear function, and adjusts the initial variance accordingly.

**Question 4:** What is the gradient vanishing problem?
*   **Correct Answer: D. Too small gradient at the lower layers of model**
*   **Explanation:** In very deep networks, gradients are calculated via the chain rule. If the derivatives of the activation functions (e.g., Sigmoid) are less than 1, repeated multiplication can cause the gradient to shrink exponentially as it propagates backward to the earlier (lower) layers. This prevents the weights in these layers from updating effectively.

**Question 5:** What is the gradient exploding problem?
*   **Correct Answer: A. Too big gradient at any layer of model**
*   **Explanation:** The opposite of vanishing gradients. If the derivatives involved in the chain rule are large (e.g., with large weights), the gradient can grow exponentially during backpropagation. This causes large, unstable updates to the model parameters, often leading to numerical overflow (NaN values).

**Question 6:** Which layers can help reduce the gradient vanishing problem? (MC)
*   **Correct Answers: E. Batch normalization, F. Skip connection, G. ReLU Activation**
*   **Explanation:**
    *   **Batch Norm (E):** By normalizing activations, it often prevents them from reaching the saturated tails of activation functions like Sigmoid/Tanh where gradients are near zero.
    *   **Skip Connections (F):** As used in ResNet, they provide a shorter, direct path for the gradient to flow backwards, mitigating the vanishing gradient problem in the main convolutional branch.
    *   **ReLU (G):** Its gradient is always 1 for positive inputs, unlike Sigmoid (max gradient 0.25) or Tanh (max gradient 1.0), which helps maintain stronger gradient flow.

**Question 7:** What is correct about underfitting? (MC)
*   **Correct Answers: A, E**
*   **Explanation:**
    *   **A.** Underfitting occurs when the model is too simple (e.g., a linear model) to capture the underlying patterns of complex data.
    *   **E.** Because the model cannot learn the training data well, both training and validation accuracy are low.

**Question 8:** What is correct about overfitting? (MC)
*   **Correct Answers: B, C**
*   **Explanation:**
    *   **B.** Overfitting occurs when a very powerful/complex model (e.g., a large deep net) learns the noise and specific details of the training data instead of the generalizable patterns. This is especially true if the data is simple or small.
    *   **C.** The model performs excellently on the training data but poorly on unseen validation/test data, indicating it has memorized rather than learned.

**Question 9 & 10:** Identifying overfitting and underfitting on MNIST.
*   **Q9 (Overfitting) Correct Answer: A.** A huge gap between high training accuracy (99%) and low testing accuracy (50%) is the classic sign of overfitting.
*   **Q10 (Underfitting) Correct Answers: C, D.** Low accuracy on both the training and test sets (e.g., 70/40 or 30/40) indicates the model has failed to learn the task effectively.

**Question 11:** Which is likely to cause overfitting?
*   **Correct Answer: A. Very big model with few images**
*   **Explanation:** A high-capacity model can easily memorize a small dataset, leading to overfitting. A small model with many images is more likely to underfit.

**Question 12:** Which training is less prone to overfitting?
*   **Correct Answer: B. B is less overfitting than A**
*   **Explanation:** The plot for **B** shows a smaller gap between training and validation accuracy curves (~62% vs 88% for **A**). A smaller gap indicates that the model's performance generalizes better to unseen data, which is the opposite of overfitting.

**Question 13:** Which techniques reduce overfitting? (MC)
*   **Correct Answers: B. Early stopping, C. Adding more data, D. Weight regularization, F. Using Dropout**
*   **Explanation:** These are all standard regularization techniques.
    *   **B.** Stops training before the model starts to overfit.
    *   **C.** Provides more information for the model to learn general patterns.
    *   **D.** (e.g., L1/L2) penalizes large weights, encouraging a simpler model.
    *   **F.** Randomly drops neurons, preventing co-adaptation and making the model more robust.
    *   *Incorrect: A, E, G* (Training longer, adding layers/neurons) increase model capacity and make overfitting *more* likely.

**Question 14:** In Batch Normalization, which are trainable parameters?
*   **Correct Answers: C. Scaling parameter Œ≥, D. Shifting parameter Œ≤**
*   **Explanation:** The minibatch mean (¬µ_B) and standard deviation (œÉ_B) are **statistics** calculated from the data during the forward pass. They are not parameters learned via gradient descent. The parameters Œ≥ (scale) and Œ≤ (shift) are learned to allow the model to represent the identity transform and increase flexibility.

**Question 15:** Which data augmentation should be used?
*   **Correct Answer: C. Horizontally flipping and color shift**
*   **Explanation:** The training set has blue Fords and Chevrolets. The test set shows that brand is the distinguishing feature, not color (the test cars are red). Therefore:
    *   **Color shift** is crucial to teach the model that color is not a relevant feature for class "Ford".
    *   **Horizontal flipping** is a good general augmentation that preserves the object's identity (a car flipped horizontally is still a car).
    *   *Incorrect: Vertical flipping* makes no sense for cars (a vertically flipped car is not a real object). *Center crop* might be useful but doesn't address the core problem of color bias.

---

### Summary of FIT3181_5215-L06-Quiz.pdf: "Advanced Convolutional Neural Networks"

This quiz covers advanced CNN architectures (ResNet), receptive fields, and adversarial machine learning.

**Key Topics Covered:** End-to-end learning vs. traditional pipelines, CNN tensor shapes, receptive fields, ResNet architecture, skip connections, adversarial examples, and adversarial training.

---

**Detailed Question Breakdown:**

**Question 1:** Traditional vs. Deep Learning feature extraction.
*   **Correct Answers: B, C**
*   **Explanation:** In **traditional approaches (C)**, feature extractors (e.g., SIFT, HOG) are designed by hand. The classifier's error signal cannot be used to change these pre-defined features. In **deep learning (B)**, the feature extractor (convolutional layers) and classifier are learned jointly end-to-end using the same error signal.

**Questions 2 & 3:** Calculating tensor shapes in a CNN. This requires step-by-step calculation.

**Explanation of Convolution Output Size Formula:**
The output spatial dimensions (Height, Width) from a convolutional or pooling layer are calculated as:
`Output size = (Input_size - Kernel_size + 2*Padding) / Stride + 1`

Let's trace the dimensions for a single input sample in the batch `[C, H, W] = [3, 32, 32]`:
1.  **Conv2D 1:** `kernel_size=5, stride=2, padding=0`. Uses 100 filters.
    *   `H_out = (32 - 5 + 2*0)/2 + 1 = 27/2 + 1 = 13.5 + 1 = 14.5 -> 14` (floor division)
    *   `W_out = 14`
    *   **Shape B:** `[num_filters=100, H_out=14, W_out=14]` -> `[100, 14, 14]` for one sample. For a batch of 64: `[64, 100, 14, 14]`.
2.  **Pooling Layer:** `pool_size=(2,2), stride=(2,2), padding=1`.
    *   `H_out = (14 - 2 + 2*1)/2 + 1 = (14)/2 + 1 = 7 + 1 = 8`
    *   `W_out = 8`
    *   **Shape C:** `[64, 100, 8, 8]`.
3.  **Fully Connected (FC) Layer:** This layer flattens its input. The input volume is `100 * 8 * 8 = 6400`.
    *   **Shape D:** `[64, 6400]` (The batch dimension is maintained).
4.  **FC to Output:** The FC layer has 7 neurons, and the final output layer has 10.
    *   **Shape E (after 7-neuron FC):** Not explicitly asked, but would be `[64, 7]`.
    *   **Final Output Shape:** `[64, 10]`.

*   **Q2 Correct Answer:** Looks for the option matching our calculations: `[100,3,5,5]` (filters A), `[64,100,14,14]` (B), `[64,100,8,8]` (C), `[64, 100x8x8]` (D), `[64,10]` (E).
*   **Q3 Correct Answer: A.** `[100,3,5,5], [64,100,14,14], [64,100,8,8], [64, 100x8x8], [64, 10]`

**Question 4:** Receptive Fields.
*   **Correct Answers: C, D**
*   **Explanation:** The receptive field of a neuron is the region in the *input* space that affects its value.
    *   **C.** As we add more layers (e.g., conv, pool), each neuron sees a composition of regions from the previous layer, so its receptive field **becomes larger**.
    *   **D.** The value of a neuron is **computationally relevant** only to the pixels within its receptive field. Changes outside this field have no effect on the neuron's value.

**Question 5:** Correct structure of a Residual Block.
*   **Correct Answer: A**
*   **Explanation:** The standard pre-activation ResNet block order is: **Batch Norm -> ReLU -> Conv -> Batch Norm -> ReLU -> Conv**. The skip connection `x` is added to the output of the second convolution, and a final ReLU is applied. Option A matches this structure: `ReLU(Batch Norm(Conv)) -> ReLU(Batch Norm(Conv)) -> + x -> ReLU`.

**Questions 6 & 7:** Calculating output shapes of Residual Blocks. The key is to track the `strides` parameter, as it affects the spatial downsampling and may require a `1x1` conv in the skip connection to match dimensions.

*   **Q6:** `use_1x1conv=False, strides=1`. The input `X` is `[10, 3, 32, 32]`. The block has `num_channels=3`.
    *   `strides=1` means no downsampling. The spatial size remains `32x32`.
    *   `use_1x1conv=False` means the identity `x` is added directly. This requires the output of the convolutions to have the same shape as `x` (`[10, 3, 32, 32]`), which it does.
    *   **Output Shape Y:** `[10, 3, 32, 32]`

*   **Q7:** `use_1x1conv=True, strides=2`. The input `X` is `[10, 3, 32, 32]`. The block has `num_channels=6`.
    *   `strides=2` in the first convolution causes downsampling. Using the output size formula: `(32 - 3 + 2*1)/2 + 1 = (31)/2 + 1 = 15.5 + 1 = 16.5 -> 16` (PyTorch floor division).
    *   The output spatial size after the block is `16x16`.
    *   `use_1x1conv=True` is needed because the number of channels changes from `3` to `6` and the spatial size changes from `32x32` to `16x16`. The `1x1` convolution with `stride=2` in the skip connection handles this projection.
    *   **Output Shape Y:** `[10, 6, 16, 16]`

**Question 8:** Calculating feature map shapes through a ResNet.
*   **Input:** `[32, 3, 64, 64]` (batch, channels, height, width).
*   We need to apply the output size formula step-by-step:
    1.  `Conv2d(64, k=7, s=2, p=3)`: `(64 - 7 + 2*3)/2 + 1 = (63)/2 + 1 = 31.5 + 1 = 32.5 -> 32`. Shape: `[32, 64, 32, 32]`.
    2.  `MaxPool2d(k=3, s=2, p=1)`: `(32 - 3 + 2*1)/2 + 1 = (31)/2 + 1 = 15.5 + 1 = 16.5 -> 16`. Shape: `[32, 64, 16, 16]`. **This is tensor A.**
    3.  `ResnetBlock(64, 2, first_block=True)`: The first block typically uses `strides=1`, so spatial size remains `16x16`. Shape: `[32, 64, 16, 16]`. **This is tensor B.**
    4.  `ResnetBlock(128, 2)`: This block will use `strides=2` in its first sub-block to downsample. Output size: `(16 - 3 + 2*1)/2 + 1 = (15)/2 + 1 = 7.5 + 1 = 8.5 -> 8`. Shape: `[32, 128, 8, 8]`. **This is tensor C.**
    5.  `ResnetBlock(256, 2)`: Again, uses `strides=2` to downsample. Output size: `(8 - 3 + 2*1)/2 + 1 = (7)/2 + 1 = 3.5 + 1 = 4.5 -> 4`. Shape: `[32, 256, 4, 4]`. **This is tensor D.**
    6.  `AdaptiveAvgPool2d((1, 1))`: This layer resizes the spatial dimensions to `1x1`. Shape: `[32, 256, 1, 1]`. **This is tensor E.**
    7.  `Flatten()`: Removes the spatial dimensions. Shape: `[32, 256]`.
    8.  `Linear(10)`: Shape: `[32, 10]`. **This is tensor F.**
*   **Correct Answer: A.** `[32,64,16,16], [32,64,16,16], [32,128,8,8], [32,256,4,4], [32,256,1,1], [32,10]`

**Question 9:** Correct statements about ResNet.
*   **Correct Answers: C, D**
*   **Explanation:**
    *   **C.** The `1x1` convolution in the skip connection is primarily used to adjust the number of channels and/or the spatial dimensions to match the main branch, enabling the element-wise addition.
    *   **D.** This is an accurate description of the ResNet hierarchy.
    *   *Incorrect:*
        *   **A.** The standard pre-activation unit is **Batch Norm -> ReLU -> Conv**. ReLU is *not* followed by Batch Norm.
        *   **B.** While skip connections alleviate vanishing gradients, replacing ReLU with Sigmoid would reintroduce the problem due to Sigmoid's saturating nature, defeating the purpose of the architecture.

**Questions 10 & 11:** Properties of Adversarial Examples.
*   **Q10 Correct Answers: A, D**
    *   **A.** The defining characteristic of an adversarial example is that it is perceptually indistinguishable from the original to a human.
    *   **D.** The model's prediction changes (`argmax` output is different).
*   **Q11 Correct Answers: A, D**
    *   **A., D.** The constraint `||x' - x||‚àû ‚â§ Œµ` defines an L‚àû-norm ball around `x`. The L‚àû-norm is the **maximum absolute change** allowed for any pixel. This small, uniform bound is what ensures the perturbation is imperceptible to humans.

**Question 12:** Untargeted Attack formulation.
*   **Objective:** `x_adv = argmax_{x' ‚àà B_Œµ(x)} L(f(x'; Œ∏), y)`
*   **Correct Answers: B, C, E**
*   **Explanation:** Maximizing the loss `L` w.r.t. the *true label `y`* means we are making the model less confident in the correct answer. This **decreases (B)** the chance of predicting `y` and consequently **increases (C)** the chance of predicting any other incorrect label `y' ‚â† y`. Since the goal is to cause a misclassification to *any* wrong label, it is an **untargeted attack (E)**.

**Question 13:** Targeted Attack formulation.
*   **Objective:** `x_adv = argmin_{x' ‚àà B_Œµ(x)} L(f(x'; Œ∏), y_‚â†)` where `y_‚â†` is a specific target label.
*   **Correct Answers: B, C**
*   **Explanation:** *Minimizing* the loss w.r.t. a specific *target label `y_‚â†`* means we are making the model more confident in that specific wrong label. This **increases (B)** the chance of predicting the target `y_‚â†`. Because the attack aims for a specific pre-determined class, it is a **targeted attack (C)**.

**Question 14:** Adversarial Training.
*   **Correct Answers: B, C, E**
*   **Explanation:** Adversarial training is a robust optimization procedure.
    *   **B.** Instead of standard augmentation, adversarial examples are generated on the fly for each batch using an attack like PGD.
    *   **C.** The model is trained to be robust by learning to correctly classify these adversarial examples.
    *   **E.** The total loss is typically a sum of the loss on clean examples and the loss on adversarial examples (`L = L_clean + L_adv`).


---

# Sasha's Notes
---

# Week 1: Machine Learning Crash Course

## Classifying What Makes a Typical Machine Learning Task

Machine learning is defined as the science and art of programming computers so they can learn from data. More generally, it enables computers to learn without being explicitly programmed. A computer program is said to learn if its performance on a task (T), as measured by a performance measure (P), improves with experience (E).

Machine learning shines in several scenarios:

- Problems with complex, rule-based solutions: When existing solutions require extensive fine-tuning or long lists of rules, ML can simplify the code and often perform better (e.g., spam filters that adapt to new spam patterns automatically).
    
- Intractable problems: For complex problems with no known traditional algorithm (e.g., speech recognition), ML can find a solution by learning from examples.
    
- Fluctuating environments: ML systems can be easily retrained on new data, keeping them up to date with changing patterns.
    
- Gaining insights from data: ML models can be inspected to reveal hidden patterns and correlations in large datasets, a process known as data mining.
    

  

A typical machine learning project workflow involves:

1. Framing the problem: Defining inputs, potential outputs, and their representations (e.g., pixel values for images, probability distributions for outcomes).
    
2. Getting the data: Collecting a significant amount of data for training, validation, and testing.
    
3. Exploring and visualizing the data: Understanding the data's characteristics, identifying quirks, and finding correlations.
    
4. Preparing the data: Cleaning and transforming data for algorithms (e.g., handling missing values, encoding categorical features, feature scaling).
    
5. Selecting and training a model: Choosing an appropriate model and training it on the prepared data.
    
6. Fine-tuning the model: Optimising hyperparameters to improve performance.
    
7. Presenting the solution: Communicating the model's performance and insights.
    
8. Launching, monitoring, and maintaining the system: Deploying the model and ensuring its continued performance in production.
    

However, challenges can arise from "bad data" or a "bad model". These include:

- Insufficient Quantity of Training Data: Too little data can prevent a model from learning effectively.
    
- Nonrepresentative Training Data: If training data does not accurately reflect new cases, the model will generalise poorly.
    
- Irrelevant Features: "Garbage in, garbage out"; the training data must contain relevant features for effective learning. Feature engineering, the process of creating a good set of features, is crucial.
    
- Overfitting the Training Data: Occurs when the model performs well on training data but poorly on new instances due to being too complex for the data. Solutions include simplifying the model, gathering more data, or using regularisation techniques.
    
- Underfitting the Training Data: Occurs when the model is too simple to learn the underlying structure of the data, leading to inaccurate predictions even on training examples. Solutions include selecting a more powerful model, feeding better features, or reducing model constraints.
    

## Explaining What Labels Are and How They Work

In supervised learning, the training set provided to the algorithm includes the desired solutions, called labels. These labels act as the "teacher" for the learning algorithm.

- For a classification task, labels typically represent categories or classes. For example, a spam filter is trained with many emails, each explicitly labelled as "spam" or "ham".
    
- For a regression task, labels (often called targets) are numeric values. For instance, to predict car prices, the system is given examples including car features and their corresponding price targets.
    

The terms "target" and "label" are generally used synonymously in supervised learning, with "target" being more common in regression and "label" in classification. Input features are also referred to as "predictors" or "attributes".

While supervised learning relies on pre-labelled data, other learning types handle data with limited or no labels:

- Unsupervised learning uses unlabelled training data, attempting to find patterns without a teacher (e.g., clustering visitors into groups without prior knowledge of the groups).
    
- Semi-supervised learning deals with datasets that are partially labelled, combining aspects of both unsupervised (e.g., clustering) and supervised learning (e.g., using common labels within clusters).
    
- Self-supervised learning involves generating a fully labelled dataset from a fully unlabelled one. The model learns to predict these generated labels, and then the pre-trained model can be fine-tuned for a different task (e.g., masking part of an image and training the model to recover the original, or predicting masked words in text).
    

## Determining When to Use Regression, Classification, or Generative Models

The choice of model type depends on the nature of the task and the desired output.

### Classification Models

- Used when the goal is to predict a discrete category or class.
    
- Binary Classification: Predicting one of two mutually exclusive classes (e.g., "spam" or "ham", "positive" or "negative" sentiment).
    
- Multiclass Classification: Predicting one of more than two mutually exclusive classes (e.g., handwritten digit recognition, where an image is classified as 0 through 9).
    
- Multilabel Binary Classification: Predicting multiple binary labels simultaneously for a single instance (e.g., classifying an email as both "ham" and "urgent").
    

### Regression Models

- Used when the goal is to predict a continuous numeric value.
    
- Examples include predicting house prices, stock values, or a person's age.
    
- Univariate Regression: Predicting a single numeric value for each instance.
    
- Multivariate Regression: Predicting multiple numeric values at once for each instance (e.g., predicting 2D coordinates and width/height for object localisation).
    

### Generative Models

- These models are primarily discussed in Part II of G√©ron's textbook. They are capable of randomly generating new data that looks very similar to the training data.
    
- While not extensively covered in Part I of G√©ron, they are mentioned as an important aspect of architecture. Buduma's textbook also mentions generative models in the context of autoencoders.
    
- Examples include generating realistic images of faces, new text, or even music.
    
- Architectures like Autoencoders, Generative Adversarial Networks (GANs), and Denoising Diffusion Probabilistic Models (DDPMs) can function as generative models.
    

## How to Translate Classes and Their Discriminative Scores into Concrete Results and Probabilities

Machine learning models translate their internal calculations into predictions and, often, probabilities.

### Decision Function (Discriminative Score)

- For linear classifiers like Perceptrons and Linear SVMs, a decision function is computed (e.g., $\theta^T x = \theta_0 x_0 + \cdots + \theta_n x_n$). If the result is positive, it predicts the positive class; otherwise, it predicts the negative class.
    
- Perceptrons typically do not output class probabilities.
    

### Output Layer Activation Functions for Probabilities

- Sigmoid Activation Function: For binary classification problems, a single output neuron with a sigmoid activation function outputs a value between 0 and 1. This value can be interpreted as the estimated probability of the positive class. For multi-label binary classification, each label has its own sigmoid output neuron.
    
- Softmax Activation Function: For multiclass classification (where an instance belongs to only one class out of three or more), the output layer has one neuron per class and uses the softmax activation function. Softmax ensures that all estimated probabilities are between 0 and 1 and sum up to 1, as the classes are mutually exclusive.
    

### Performance Measures

- Confusion Matrix: A fundamental tool to evaluate a classifier. It counts the number of times instances of class A are classified as class B (e.g., actual "5"s incorrectly classified as "3"s). This provides a detailed breakdown of correct and incorrect classifications for each class.
    
- Precision and Recall: These are critical metrics, especially for binary classifiers:
    

- Precision: The accuracy of positive predictions (e.g., out of all instances classified as "spam", how many were actually "spam").
    
- Recall (Sensitivity or True Positive Rate): The ratio of positive instances that are correctly detected by the classifier (e.g., out of all actual "spam" emails, how many did the filter catch).
    
- There is often a precision/recall trade-off: increasing precision often decreases recall, and vice-versa. A precision/recall curve can visualise this trade-off.
    

- ROC Curve (Receiver Operating Characteristic Curve): Another common tool for binary classifiers that plots the True Positive Rate (recall) against the False Positive Rate (ratio of negative instances incorrectly classified as positive). The Area Under the Curve (AUC) measures the classifier's performance; a perfect classifier has an AUC of 1, while a random classifier has 0.5.
    

## Calculating Cross-Entropy Loss and Its Possible Range

The cross-entropy loss (also known as log loss or x-entropy) is commonly used as a cost function for classification tasks, particularly when the model predicts probability distributions over multiple classes. G√©ron states that it is generally a good choice for classification MLPs. The MLPClassifier in Scikit-Learn minimizes cross-entropy.

While G√©ron's Part I identifies cross-entropy as a suitable loss function for classification, it does not explicitly detail its calculation or possible range. However, it implies its use when predicting probability distributions. In Buduma's text, cross-entropy loss is also mentioned in the context of logistic regression models and globally normalized networks.

Generally, for two probability distributions, P (true) and Q (predicted), the cross-entropy is calculated as $H(P, Q) = -\sum P(x) \log Q(x)$.

- Its minimum possible value is 0, occurring when the predicted probability distribution exactly matches the true distribution.
    
- The maximum value is theoretically infinite as the predicted probability for the true class approaches zero (i.e., when the model is extremely confident in the wrong prediction). In practice, numerical stability measures prevent it from reaching actual infinity.
    

## Differentiating Between KL Divergence, Cross-Entropy Divergence, and Entropy

### Entropy

- In the context of Decision Trees, entropy is used as an impurity measure. It originated in thermodynamics as a measure of molecular disorder.
    
- In Shannon's information theory, it measures the average information content of a message.
    
- A set's entropy is zero when it contains instances of only one class (perfect purity).
    
- Geron's Part I discusses its use as a criterion for splitting nodes in decision trees, contrasting it with Gini impurity.
    

### Cross-Entropy Loss (Cross-Entropy Divergence)

- As mentioned above, G√©ron's Part I introduces cross-entropy as a loss function for classification MLPs that predict probability distributions. It is also referred to as "log loss".
    
- The primary function of cross-entropy in this context is to measure the difference between the predicted probability distribution and the true distribution of labels, guiding the model to improve its predictions during training.
    
- This loss function is essential for models where the output layer uses a Softmax activation for multiclass classification.
    

### KL Divergence (Kullback‚ÄìLeibler Divergence):

- G√©ron's Part I does not explicitly define or discuss KL divergence. This concept is introduced later in G√©ron's textbook, specifically in Part II (Chapter 17), where it is used in the context of Variational Autoencoders (VAEs) as a "latent loss" term to ensure that the learned codings conform to a simple Gaussian distribution.
    
- Buduma's text also mentions KL divergence as a component of the sparsity penalty in sparse autoencoders.
    
- In general, KL divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. It is often used as a measure of "distance" between distributions, though it is not symmetric.
    

# Week 2: History, Principles, and Optimisation

## Introduction to Deep Learning, Its Brief History and Perspectives

### What is Deep Learning?

- Deep learning is an active field of artificial computer intelligence.
    
- It has significantly advanced machine learning through recent breakthroughs, enabling the creation of programs that can learn from data using simple, efficient tools.
    
- At its core, deep learning involves deep neural networks (DNNs), which are highly simplified models of the human cerebral cortex, consisting of stacked layers of artificial neurons.
    
- The term "deep learning" originated from the concept that the overall length of the chain of layers in a feedforward network determines the "depth" of the model. The field studies these DNNs and, more broadly, models with deep computational stacks.
    

### Brief History of Deep Learning

- The fundamental ideas of neural networks have existed for decades.
    
- Early inspirations for artificial neural networks (ANNs) came from the structure of biological neurons in our brains. In 1943, Warren S. McCulloch and Walter Pitts proposed a simple model of an artificial neuron.
    
- The Perceptron, invented by Frank Rosenblatt in 1957, introduced a relatively simple and efficient learning procedure for feature weighting.
    
- Despite these early developments, neural networks were largely abandoned by the late 1990s due to a lack of large datasets, complex software, and insufficient computing power. Training deep neural networks was widely considered impossible.
    
- Revival of Deep Learning: In 2006, Geoffrey Hinton and his team published a pivotal paper demonstrating how to effectively train deep neural networks to achieve state-of-the-art accuracy (>98%) in handwritten digit recognition, coining the term "deep learning". This work revitalised the field, showcasing impressive achievements.
    
- Key Enablers of Modern Deep Learning: The success of deep learning has been driven by several innovations: access to massive labelled datasets (e.g., ImageNet, CIFAR), advancements in hardware such as GPU acceleration, and crucial algorithmic discoveries.
    
- Convolutional Neural Networks (CNNs): Yann LeCun's deep convolutional networks showed promise for image recognition in the 1990s. These networks are biologically inspired by the visual cortex and signal processing, enabling the hierarchical detection of patterns from pixels to object parts. A significant breakthrough occurred in 2012 when Alex Krizhevsky's pioneering use of CNNs achieved an approximate 16% error rate in the ImageNet challenge, marking deep learning's prominence in computer vision.
    
- Recurrent Neural Networks (RNNs): These models, used for sequence processing, were theoretically proven to be universal functional representers (Turing complete) by Kilian and Siegelmann in 1996.
    
- Generative Adversarial Networks (GANs): Introduced by Ian Goodfellow et al. in 2014, GANs involve two competing neural networks (a generator and a discriminator). Yann LeCun regarded it as "the most interesting idea in the last 10 years in machine learning" in 2016.
    
- Memory Augmented Neural Networks (e.g., Neural Turing Machines, DNCs): Graves et al. introduced Neural Turing Machines (NTMs) in 2014, enhancing RNNs with external memory to handle transient storage.
    
- Deep Reinforcement Learning (DRL): A major advancement in 2014 saw DeepMind's Deep Q-Network (DQN) achieve superhuman skill in playing Atari games using only raw pixel inputs. Google's AlphaGo program also exemplifies reinforcement learning.
    

### Perspectives on Deep Learning

- Deep learning models are highly versatile, powerful, and scalable, making them suitable for complex machine learning tasks like image classification, speech recognition, and Go.
    
- They excel in problems where traditional solutions require extensive fine-tuning or are inadequate, and in environments with fluctuating data.
    
- Deep learning can aid human understanding by uncovering hidden patterns and unsuspected correlations in data, a process known as data mining.
    
- In Natural Language Processing (NLP), deep learning aims to build machines that can master written and spoken language, addressing tasks such as text classification, translation, and question answering.
    
- A long-term goal for neural networks is to achieve and explain reasoning processes.
    

## When and Why Does Deep Learning Work?

### When Deep Learning Works Best

- It is particularly effective for complex problems like image recognition, speech recognition, or natural language processing.
    
- Optimal results are often achieved with large datasets, significant computing power, and patience.
    
- Leveraging pretrained neural networks can significantly enhance performance.
    
- It's beneficial when human designers need only to define a broad function family rather than a precise function, allowing the model to learn the specific function itself.
    
- Empirical evidence suggests that greater network depth leads to better generalisation across various tasks.
    

### Why Deep Learning Works (Key Benefits)

- Expressiveness and Parameter Efficiency: Deep neural networks can solve problems previously considered intractable. While a single hidden layer with many neurons can theoretically approximate complex functions, deep networks are more parameter-efficient, achieving better performance with fewer neurons for the same complexity.
    
- Automated Feature Learning (Representation Learning): Each layer in a deep network learns and extracts progressively more relevant features from the input data, automating a process that traditionally required significant human effort. This results in meaningful, dense representations (embeddings) of the data.
    
- Universal Approximation Theorem: A feedforward network with a linear output layer and at least one hidden layer, using any "squashing" activation function (e.g., logistic sigmoid), can approximate any Borel measurable function with any desired non-zero error, given sufficient hidden units.
    
- Hierarchical Composition: Deep models embody the principle that the target function can be understood as a composition of several simpler functions, allowing them to discover underlying factors of variation.
    
- Overcoming Traditional Programming Limitations: Unlike traditional programs that excel at arithmetic and explicit instruction following, machine learning‚Äîespecially deep learning‚Äîlearns from examples, making it superior for tasks like handwriting or speech recognition.
    
- Advanced Optimisation Techniques: Modern optimisers (e.g., Adam), along with techniques like batch normalisation, dropout, and learning rate schedules, have made it feasible to effectively train very deep networks.
    
- Reduced Feature Engineering: For complex problems like Atari games, deep Q-networks (DQNs) can learn directly from raw pixel inputs, eliminating the need for handcrafted features.
    
- Transfer Learning and Unsupervised Pretraining: Reusing layers from models trained on similar tasks (transfer learning) significantly speeds up training and reduces the need for large amounts of labelled data. When similar models aren't available but abundant unlabelled data exists, unsupervised pretraining (e.g., using autoencoders or GANs) can be employed to learn useful representations.
    

## Understand Feedforward Neural Networks (DNN)

### Definition and Core Principles

- Feedforward neural networks (FNNs) are also known as deep feedforward networks or multilayer perceptrons (MLPs).
    
- Their primary objective is to approximate a target function, f*, which maps an input x to an output y (e.g., a classifier maps x to a category y).
    
- These networks define a mapping y = f(x; Œ∏) and learn the optimal parameters Œ∏ to achieve the best function approximation.
    
- Information flows in only one direction‚Äîfrom the inputs through intermediate computations to the output‚Äîwith no feedback connections. This distinct characteristic gives them the "feedforward" designation.
    

### Architecture and Components

- A DNN is constructed from a stack of layers of artificial neurons.
    
- Input Layer: This is the first layer that directly receives the raw input data.
    
- Hidden Layers: These layers are "sandwiched" between the input and output layers. They are crucial because they automatically learn and extract important features from the input data, often referred to as where "most of the magic is happening". Unlike the output layer, the desired behaviour of hidden layers is not directly specified by the training data; the learning algorithm must determine their optimal function. Each element in these vector-valued layers can be considered a "unit" or "neuron," which receives input from other units and calculates its own activation value.
    
- Output Layer: This final layer computes the network's ultimate answer or prediction. Its desired output is directly specified by the training data.
    
- Weights and Biases: Connections between neurons are assigned weights (w or W) that determine the strength of the connection. Additionally, neurons typically have a bias term (b). Collectively, these adjustable values form the parameter vector (Œ∏) of the model.
    
- Linear Perceptrons and Neurons: A single neuron can express models that are more complex than simple linear perceptrons.
    
- Multilayer Perceptrons (MLPs): When an ANN comprises a deep stack of hidden layers, it is termed a Deep Neural Network (DNN). MLPs are widely used for both regression tasks (predicting continuous values, typically with one output neuron for a single value or multiple for multivariate predictions) and classification tasks (predicting categories, using one output neuron for binary classification or multiple for multiclass classification).
    

### Activation Functions (Nonlinearities)

Nonlinearities are critical for enabling neural networks to learn complex relationships. Without them, a network effectively acts as a linear perceptron, severely limiting its ability to learn intricate features. After calculating the weighted sum of inputs (the logit z), an activation function f is applied to produce the neuron's output y = f(z).

  

Common Activation Functions:

- Sigmoid: Often used in regression MLPs for bounded outputs or in binary classification MLPs. However, it can contribute to the vanishing gradient problem.
    
- ReLU (Rectified Linear Unit): Has become a preferred choice for many tasks, especially in computer vision. It is a good default for hidden layers and significantly alleviates vanishing gradients compared to sigmoid or tanh functions.
    
- Softmax: Typically used in output layers to generate a probability distribution over a set of mutually exclusive labels (e.g., classifying handwritten digits 0-9). It is common in multiclass classification MLPs.
    
- Other variants include Leaky ReLU, PReLU, ELU, GELU, Swish, and Mish, each with specific properties. The SELU function, for instance, can promote self-normalisation under certain conditions.
    

### Key Challenges in DNN Training

- Vanishing/Exploding Gradients: This is a major hurdle in training deep neural networks. During backpropagation, gradients can become extremely small (vanishing) or excessively large (exploding), which impedes effective weight updates and learning. Solutions include Glorot and He initialisation, batch normalisation, and gradient clipping.
    
- Overfitting: A critical problem where the model learns the training data too well, leading to poor performance on new, unseen data. More complex models, particularly deep networks with many connections, are susceptible to overfitting. Strategies to combat this include using validation sets, various regularisation techniques (such as L1 and L2 regularisation, and dropout), and early stopping.
    

## Forward Propagation and Computational Graph in Deep Learning

### Forward Propagation

- Forward propagation is the process by which information flows through a neural network from the input layer to the output layer, producing a prediction.
    
- The input data (x) serves as the initial information, which then propagates sequentially through the hidden units of each layer until the final output (≈∑) is generated.
    
- In a layered architecture, the output of each layer h(k) is calculated as a function g(k) of the previous layer's output h(k-1) (or the initial input x for the first layer). This calculation involves multiplying by a weight matrix W(k) and adding a bias vector b(k).
    
- For a single neuron, the process involves computing a logit z (the weighted sum of inputs x and weights w, plus a bias b: z = w·µÄx + b), followed by applying an activation function f to produce the output y = f(z).
    
- In a deep recurrent neural network (RNN), the initial state (e.g., h(init) = 0) is fed into a recurrent neuron along with the first time step's input (x(0)). The neuron computes a weighted sum, applies an activation function (e.g., hyperbolic tangent), yielding the first output (y0), which simultaneously becomes the new state (h0). This iterative process continues for subsequent time steps until the entire sequence is processed. Stacking multiple recurrent layers is common in deep RNNs, with early layers often performing sequence-to-sequence mapping and later layers performing sequence-to-vector transformations.
    

### Computational Graph

- TensorFlow's architecture is built around tensors, which represent data that flows through a computational graph of operations. This graph is a representation of the sequence of computations performed by the neural network.
    
- The graph visually describes how a scalar value, such as the loss for a training example, is computed.
    
- Representing neural networks as computational graphs allows for efficient execution, especially with GPU acceleration for parallel tensor operations, and provides a clear method for model implementation.
    
- For example, in TensorFlow, variables like x (a placeholder for input data), W (weights), and b (biases) define parts of the computational graph. An operation like output = tf.matmul(x, W) + b represents a node in this graph.
    
- Automatic Differentiation (Autodiff): TensorFlow automatically constructs a computation graph that includes the necessary operations for calculating gradients. This feature enables efficient and exact computation of gradients. Reverse-mode autodiff is particularly effective when the function being differentiated has many input variables (e.g., weights and biases) but few outputs (e.g., a single loss value).
    

## Understand Forward Propagation for a Mini-Batch and How to Compute the Batch Loss

### Mini-Batch Processing in Training

- The backpropagation algorithm processes data in mini-batches, typically handling one mini-batch at a time.
    
- In online learning, the system is trained incrementally, with data instances fed sequentially, either individually or in small groups called mini-batches.
    
- The objective is to compute the gradient of the cost function based on a single mini-batch.
    

### Forward Propagation with Mini-Batch Input

- When processing a mini-batch with a simple Multilayer Perceptron (MLP), inputs are organised as a design matrix X (containing multiple examples) and a corresponding vector of labels y.
    
- For example, a layer of hidden features H might be computed as H = max{0, X * W(1)}, where W(1) are the weights of the first layer (simplified here without biases, and using ReLU activation).
    
- Subsequently, predictions for unnormalised log probabilities across classes might be HW(2).
    
- Keras models are designed to efficiently process batches of inputs rather than individual instances.
    
- When a model has multiple inputs (e.g., a Wide & Deep model), the fit() method expects a corresponding pair of input matrices (e.g., (X_train_wide, X_train_deep) for training).
    

### Computing Batch Loss

- The forward propagation process culminates in the calculation of a scalar cost J(Œ∏).
    
- This cost J is derived from the loss function L(≈∑, y) (which quantifies the discrepancy between the model's output ≈∑ and the true target y) and may also include a regulariser Œ©(Œ∏) (to prevent overfitting). The total cost is often expressed as J = L(≈∑, y) + ŒªŒ©(Œ∏).
    
- The primary goal of the training algorithm is to find the parameter values that minimise this cost function.
    

- For classification MLPs, the cross-entropy loss is commonly used.
    
- For regression MLPs, the Mean Squared Error (MSE) is a typical loss function. The Huber loss is also a popular choice, often outperforming the Mean Absolute Error (MAE).
    

- In a custom training loop, the forward pass computes predictions (y_pred = model(X_batch, training=True)). The main loss is then computed (e.g., main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))), and if the model includes regularisation terms (e.g., L1/L2 regularisation), these are added to the main_loss to form the total loss for the batch (loss = tf.add_n([main_loss] + model.losses)).
    

## Understand the General Formulation of Machine Learning Problems as an Optimisation Process

### Machine Learning as an Optimisation Problem

- The core of machine learning involves constructing a model that can learn effectively from data.
    
- This learning process is fundamentally an optimisation process, where the goal is to find optimal values for the model's parameters (such as weights and biases) by iteratively adjusting them until a predefined error or cost function is minimised.
    
- To formulate a machine learning algorithm, one must specify three key elements: a model family, a cost function, and an optimisation procedure.
    
- The primary task of a machine learning engineer is to select an appropriate model and then train it using available data.
    
- During training, the neural network is exposed to a large number of training examples, and its weights are progressively modified to minimise the errors made on these examples.
    
- The ultimate aim is for the trained model to perform well and make accurate predictions on new, unseen instances (generalisation).
    

### Key Components of the Optimisation Process

- Cost Function (Loss Function or Error Function): This mathematical function quantifies the discrepancy between the model's predictions and the true target values. The objective of the optimisation process is to minimise this function. It may be denoted as E (error function), J (cost function), or L (loss function).
    
- Parameters (Weights and Biases): These are the adjustable components within the model that the optimisation algorithm manipulates during training. By modifying these parameters, the model's output changes.
    
- Optimizer: An algorithm or technique designed to improve the model's performance by iteratively adjusting its parameters to reduce the error. Most optimisers in deep learning are based on gradient descent.
    
- Gradient Descent (GD): This is a widely used optimisation algorithm that aims to find the set of weights that minimises the error function. It works by calculating the gradient (which points in the direction of the steepest increase in the error function) at the current parameter values, and then taking a step in the opposite direction (steepest descent).
    

- Variants of Gradient Descent:
    

- Batch Gradient Descent: Uses the entire dataset to compute the gradient and update parameters in each step.
    
- Stochastic Gradient Descent (SGD): Updates parameters after processing each individual training instance.
    
- Mini-batch Gradient Descent: A compromise between Batch GD and SGD, updating parameters after processing small groups of instances (mini-batches).
    

- Learning Rate (Hyperparameter Œ∑): This critical hyperparameter determines the size of the steps taken in the direction of the gradient. A smaller learning rate is typically needed as the optimisation approaches the minimum. The learning rate is multiplied by the gradient to scale the update. Modern optimisers often feature adaptive learning rates, which adjust dynamically during training.
    

### Challenges in Machine Learning Optimisation

- Nonconvexity: Due to the inherent non-linearity of neural networks, most interesting loss functions are nonconvex. This means that iterative, gradient-based optimisers generally aim to find a very low value for the cost function, rather than guaranteeing convergence to the global minimum.
    
- Sensitivity to Initialisation: Optimisation algorithms like SGD, when applied to nonconvex loss functions, are sensitive to the initial values of the parameters. It is crucial to initialise weights to small random values and biases to zero or small positive values. Techniques like Glorot and He initialisation are designed to mitigate these issues.
    
- Overfitting: A common and significant challenge where the model learns the training data too precisely, leading to poor performance on new, unseen data. This is particularly prevalent in complex models. It is addressed through regularisation techniques (e.g., L1 and L2 regularisation, and dropout), the use of validation sets, and early stopping.
    
- Bad Data: Insufficient quantity of training data can severely hinder a model's ability to generalise effectively to new examples.
    
- Data Mismatch: This occurs when the distribution of the training data differs significantly from the data the model encounters in a real-world or production environment.
    

# Week 3: Mastering Convolutional Neural Networks (CNNs)

## Biological Inspiration of Convolutional Neural Networks

- The design of Artificial Neural Networks (ANNs), including CNNs, is deeply inspired by the biological neural networks found in animal brains.
    
- Biological Neurons: These are cells in the brain, composed of a cell body, dendrites (branching extensions receiving inputs), an axon (a long extension sending signals), and synapses (connections to other neurons). Neurons produce short electrical impulses (signals), and when enough neurotransmitters are received, they fire their own impulses.
    
- Layered Structure of the Brain: Biological neurons are organised in vast networks, often in consecutive layers, particularly in the cerebral cortex. This layered processing transforms raw sensory input into conceptual understanding. For instance, the bottommost layer of the visual cortex receives raw visual data, which is then processed by subsequent layers until an object is recognised.
    
- Visual Cortex Studies: Groundbreaking research by David Hubel and Torsten Wiesel revealed that neurons in the visual cortex respond to specific patterns (e.g., vertical, horizontal lines) within small regions of the visual field, known as receptive fields. Furthermore, they found that the visual cortex builds features hierarchically, from simple lines to contours, shapes, and eventually entire objects.
    
- Neocognitron: These studies directly inspired the neocognitron in 1980, which later evolved into what we now call convolutional neural networks. CNNs emerged from this study of the brain‚Äôs visual cortex and have been used in computer image recognition since the 1980s.
    

## Building Blocks and Tensor Transformations in CNNs

CNNs are constructed using several key layers that enable them to process image data efficiently.

- Artificial Neurons: The basic unit of an ANN, analogous to a biological neuron, takes multiple inputs, multiplies them by specific weights, sums them together (producing a logit), and then passes this sum through an activation function to generate an output.
    
- ¬†Convolutional Layers: These are the most important building blocks of a CNN, specifically designed for image processing.
    
- Receptive Fields: Unlike fully connected layers where every neuron connects to every input, neurons in a convolutional layer are connected only to pixels within a small rectangular region of the previous layer, called its receptive field.
    
- Filters (Kernels): These are small trainable weight matrices that act like "pattern detectors." For example, one filter might detect vertical lines, while another detects horizontal lines.
    
- Parameter Sharing: A crucial characteristic is that all neurons within a single feature map share the same filter (weights) and bias term. This dramatically reduces the number of parameters and allows the network to detect the same feature anywhere in the input image, regardless of its position.
    
- Stride (s): This defines the distance between consecutive applications of a filter across the input volume. A stride greater than 1 effectively downsamples the output, reducing its height and width.
    
- Padding (p): Zero padding involves adding rows and columns of zeros around the input image. This is often used to ensure that the output feature map has the same height and width as the input image ("same" padding), preventing information loss at the borders. "Valid" padding applies the filter only where it fully overlaps the input, potentially reducing output size.
    
- Tensor Transformation (3D to n-D):
    

- An input image is represented as a 3D tensor (or "volume") with dimensions: width x height x depth (e.g., a color image might be 200x200x3, where depth 3 corresponds to Red, Green, and Blue channels).
    
- A convolutional layer applies multiple filters (e.g., k filters). Each filter operates across the entire depth of the input volume.
    
- Each filter produces a single 2D feature map (a slice in the output volume). This feature map highlights locations where the specific pattern detected by that filter is present.
    
- The collection of these k feature maps then forms the output volume, which is also a 3D tensor with dimensions: new width x new height x new depth, where the new depth is equal to the number of filters k used in the layer.
    
- For example, if a tf.keras.layers.Conv2D layer has kernels, their shape is [kernel_height, kernel_width, input_channels, output_channels], where output_channels is the number of filters. The output tensor will have a shape [batch_size, output_height, output_width, output_channels].
    

- Activation Layers: After the linear convolution operation, an activation function is applied element-wise to introduce non-linearity. This allows the network to learn complex, non-linear relationships in the data.
    

- ReLU (Rectified Linear Unit): A very popular choice for hidden layers, represented by f(z) = max(0, z).
    

- Variants: LeakyReLU, PReLU, and ELU address some limitations of ReLU.
    

- Softmax: Commonly used in the output layer for multi-class classification tasks. It converts a vector of raw scores (logits) into a probability distribution over mutually exclusive classes, where the probabilities sum to 1.
    

- Pooling Layers: These layers are used to subsample (shrink) the input feature maps, thereby reducing the computational load, memory usage, and the number of parameters. This helps to control overfitting.
    

- Max Pooling: The most common type, it selects the maximum value from a small rectangular region (receptive field) of the input, defined by a pool_size and stride.
    
- Pooling layers do not have learnable weights.
    

- Batch-Normalization (BN) Layers: These layers normalise the inputs to every layer in the network.
    

- Mechanism: They normalise each component of the logit vector (before the activation function) across all examples in a mini-batch by subtracting the mean and dividing by the standard deviation.
    
- Advantages: BN significantly accelerates the training process, makes training more robust to initialisation and variations in input data, and acts as a regularization technique, helping to combat the vanishing/exploding gradients problem. It allows for much higher learning rates.
    

## Advantages of Convolutional Neural Networks (CNNs)

CNNs offer significant advantages, particularly for image data, over traditional fully connected deep neural networks.

- Automatic Feature Learning: CNNs automatically learn to extract useful features from raw input data. Lower layers learn simple features (like edges and textures), and subsequent layers combine these into more complex, high-level representations (like parts of objects or entire objects). This eliminates the need for manual, time-consuming feature engineering.
    
- Exploiting 3D Image Structure: CNNs are designed to directly process images as 3D tensors (width x height x depth), preserving the spatial relationships between pixels. In contrast, vanilla fully connected networks would require flattening the image into a 1D vector, losing crucial spatial information.
    
- Parameter Efficiency and Scalability: Due to local receptive fields and parameter sharing, CNNs have far fewer parameters than fully connected networks for image tasks. This makes them:
    

- More scalable to larger images, avoiding the "intractable increase" in connections and weights seen in vanilla DNNs.
    
- Less prone to overfitting, especially with limited training data.
    

- Shift Invariance: Because filters are shared across the entire input, once a CNN learns to recognise a pattern (e.g., an eye), it can detect that pattern anywhere in the image. This provides a degree of robustness to the exact position of objects within an image.
    
- High Performance: CNNs have achieved superhuman performance on complex visual tasks, such as image classification and object detection.
    

## Popular Datasets for Computer Vision and Major CNN Architectures

The development of CNNs has been propelled by challenging datasets and innovative architectures.

### Popular Computer Vision Datasets

- MNIST: A classic dataset of 28x28 pixel grayscale images of handwritten digits (0-9). It serves as a common benchmark for classification tasks.
    
- Fashion MNIST: A "drop-in replacement" for MNIST, featuring 70,000 grayscale images of 28x28 pixels depicting fashion items across 10 classes. It's considered more challenging than MNIST due to greater class diversity.
    
- CIFAR-10: Consists of 32x32 pixel color images belonging to one of 10 distinct classes. It's a surprisingly hard challenge due to the small image size and subtle distinctions between classes.
    
- ImageNet: A large-scale dataset used in the ILSVRC (ImageNet Large Scale Visual Recognition Challenge), involving classifying images into hundreds or thousands of categories. This challenge has driven much of the progress in CNN architecture.
    

### Most Popular CNN Architectures

- VGGNet (Very Deep Convolutional Networks):
    

- Characterised by its simplicity and depth, stacking many small 3x3 convolutional filters (with stride 1 and "same" padding) repeatedly before a pooling layer.
    
- Increasing the number of convolutional layers before pooling layers allows for richer representations.
    
- It is known for requiring a significant amount of memory during training, often bottlenecked by GPU capacity.
    

- GoogLeNet (Inception Architecture):
    

- Winner of ILSVRC 2014, achieving a top-five error rate below 7%.
    
- Key innovation: Inception modules. These modules allow the network to use parameters very efficiently, having 10 times fewer parameters than AlexNet.
    
- Parallel Processing: An Inception module feeds its input to multiple different convolutional layers in parallel (e.g., using 1x1, 3x3, and 5x5 kernels), enabling it to capture patterns at different scales.
    
- 1x1 Convolutions (Bottleneck Layers): These layers are used within the inception modules to reduce the number of feature maps (dimensionality reduction), which cuts computational cost, reduces parameters, and improves generalisation. They also capture patterns across channels (depth dimension).
    
- The outputs from these parallel paths are then concatenated along the depth dimension to form the module's output.
    
- The full GoogLeNet architecture typically consists of nine inception modules stacked deep, often using ReLU activation functions.
    

- ResNet (Residual Network):
    

- Winner of ILSVRC 2015, known for effectively training very deep networks (e.g., 34 layers).
    
- Key innovation: Residual Learning facilitated by Skip Connections (also called "residual units"). A skip connection directly adds the input of a layer (or a block of layers) to its output.
    
- Problem Solved: In very deep networks, the vanishing gradients problem (where gradients become extremely small during backpropagation, hindering learning) makes training difficult.
    
- Mechanism: By adding the input x to the output of a network block h(x), the block is effectively forced to learn a residual function f(x) = h(x) - x, rather than the complete function h(x). If the optimal function is close to the identity function, learning the small deviation is easier.
    
- Gradient Flow: Skip connections allow gradients to flow more easily through the network, enabling individual layers to make progress even if earlier layers haven't fully learned.
    
- Architecture: ResNets are composed of stacks of residual units, each typically containing two convolutional layers (with batch normalization and ReLU activation, using 3x3 kernels, stride 1, and "same" padding). When the number of feature maps is doubled or spatial dimensions are halved, a 1x1 convolutional layer with stride 2 is used in the skip connection to match the shapes for addition. For example, ResNet-34 is a variation with 34 layers.
    

# Week 4: Backpropagation, Optimisation, and Deep Learning Loss Functions

## How Back-Prop Marries Optimisers to Train Deep Learning Models.

- Backpropagation is the core algorithm behind how neural networks learn. It is a method for efficiently computing the gradient of the cost function with respect to all the model parameters (weights and biases). This process determines how each connection weight and bias should be adjusted to reduce the neural network's error.
    
- Once backpropagation computes these gradients, an optimiser uses them to perform a gradient descent step, iteratively tweaking the parameters to minimise the error. For example, in the backpropagation algorithm for a simple sigmoid function, the backpropagation error multiplies the difference between the prediction and ground truth by the derivative of the sigmoid, and this error is used to update the activity of each synapse.
    
- It is important to understand that backpropagation itself is only the method for computing the gradient; a separate algorithm, such as stochastic gradient descent or other optimisers, is then used to perform the actual learning using this gradient. This combination allows the network's error to gradually drop until it reaches a minimum.
    

## Non-linearity & Non-convexity of Deep Learning Loss Functions

### Non-linear Nature

- To learn complex relationships, neural networks must use neurons that employ some form of non-linearity. Linear neurons alone are limited in their ability to capture these complex patterns.
    
- Deep learning models utilise non-linear activation functions (such as sigmoid, Tanh, ReLU, etc.) in their hidden layers. This non-linearity allows deep feedforward networks with at least one hidden layer to act as universal approximators, meaning they can approximate any continuous function given enough hidden units.
    
- The use of non-linear processing elements enables big neural networks with many layers to compute complicated things.
    

### Non-convex Nature and Optimisation Difficulty

- The non-linearity inherent in neural networks causes most interesting loss functions to become non-convex. This is a significant difference from linear models, where many loss functions result in convex optimisation problems.
    
- For non-convex loss functions, iterative, gradient-based optimisers are used to drive the cost function to a very low value, rather than guaranteed global convergence as with convex optimisation algorithms.
    
- Optimisation in deep learning is sensitive to the initial parameter values. It is important to initialise all weights to small random values, and biases to zero or small positive values.
    
- The process of finding optimal parameters involves searching in an extremely high-dimensional space, which makes the search difficult.
    
- Challenges in optimising deep networks include:
    

- Vanishing/Exploding Gradients: These problems arise when gradients become extremely small or large during backpropagation, hindering effective weight updates.
    
- Local Minima: While potentially exaggerated in practical networks for stochastic gradient descent, the presence of local minima means that optimisers may get stuck in sub-optimal solutions.
    
- Flat Regions and Saddle Points: These areas in the error surface can slow down gradient descent.
    
- Ill-conditioning: Where the error surface is much steeper in some directions than others, making it hard for gradient descent to find the optimal path.
    

## The Back-Prop Formula

Backpropagation is fundamentally an application of the chain rule from calculus. It performs calculations in two main phases:

### Forward Propagation

- The input x is fed forward through the network.
    
- Each neuron computes a weighted sum of its inputs (logit, z), adds a bias term, and then applies an activation function (f) to produce an output (y).
    
- This process continues layer by layer until the network produces a final output ≈∑.
    
- A scalar cost J(Œ∏) (or loss) is then computed by comparing the network's output ≈∑ with the target y.
    

2. Backward Propagation (Backprop):

- The information from the cost J(Œ∏) flows backward through the network to compute the gradient.
    
- Starting from the output layer, the error derivative with respect to the output y_j is calculated, often as ‚àí(t_j ‚àí y_j) for squared error.
    
- This error is then propagated backward through each layer. For a given layer k, the gradient on the layer's output is converted into a gradient on the pre-nonlinearity activation a(k).
    
- The chain rule is applied recursively to determine how the error at a higher layer is affected by changes in weights and activations in a lower layer. For example, the derivative of the error with respect to the activity of a neuron in layer i (‚àÇE/‚àÇy_i) is calculated by accumulating information about how y_i affects the logits of every neuron in the subsequent layer j.
    
- The gradients on weights (W(k)) and biases (b(k)) for each layer k are computed based on these propagated error signals. These gradients indicate the direction and magnitude by which the weights and biases should be adjusted to minimise the cost function.
    
- The derivative of the sigmoid function, y(1-y), is crucial in the backpropagation error for sigmoid neurons, as depicted in the backpropagation error formula. When a neuron is saturated, the derivative of the sigmoid function becomes very small, leading to a small backpropagation error and slow learning.
    
- Computational graphs are often used to formally describe this process, representing variables as nodes and operations as edges, making the application of the chain rule systematic.
    

## Popular optimisers

These optimisers are improvements and refinements of the basic gradient descent (GD) algorithm, which involves repeatedly taking steps in the direction opposite to the gradient of the cost function.

### Stochastic Gradient Descent (SGD)

- Instead of using the entire dataset to compute the gradient (batch gradient descent), SGD computes the gradient and updates parameters using only a small subset of the training data at each step, called a mini-batch.
    
- This leads to more frequent updates and a "noisy" path, which can help escape shallow local minima.
    
- The learning schedule (how the learning rate changes over time) is important to manage the "jumping around" near the minimum.
    

### SGD with Momentum

- This optimiser accelerates SGD by accumulating a "velocity" vector of the gradients from previous steps.
    
- It helps gradient descent navigate flat regions of the error surface and reduces oscillations ("zigzagging") in directions with steep but inconsistent gradients, thus speeding up convergence.
    
- The momentum hyperparameter (often denoted Œº or Œ≤) controls how much previous gradients influence the current update. A value too close to 1 (e.g., 0.99999) can cause the optimiser to overshoot the minimum.
    

### AdaGrad (Adaptive Gradient Algorithm)

- AdaGrad adapts the learning rate for each individual parameter, rather than using a single learning rate for all parameters.
    
- It scales down the learning rate for parameters that have received large gradients (indicating frequent updates) and increases it for parameters with small or infrequent gradients. It sums the squares of all past gradients for each parameter in the denominator of the learning rate update.
    
- This approach is particularly effective for sparse data or problems with highly varying feature frequencies.
    
- A significant drawback is that the accumulated squared gradients in the denominator can cause the learning rate to become very small over time, potentially hindering convergence.
    

### RMSProp (Root Mean Square Propagation)

- RMSProp was developed to address AdaGrad's problem of diminishing learning rates.
    
- Instead of accumulating all past squared gradients, RMSProp uses an exponentially decaying moving average of the squared gradients. This allows it to "forget" distant past gradients, focusing on more recent gradient magnitudes.
    
- This enables the learning rate to adapt based on recent gradients without becoming excessively small too quickly.
    
- It has a decay rate hyperparameter that controls the influence of past squared gradients.
    

### Adam (Adaptive Moment Estimation)

- Adam combines the best features of both momentum and RMSProp.
    
- It computes exponentially decaying averages of both the past gradients (first moment), similar to momentum, and the past squared gradients (second moment), similar to RMSProp.
    
- It also includes bias-correction terms to account for the initial small values of these moving averages.
    
- Adam is widely popular due to its efficiency, good performance across a wide range of tasks, and relative ease of hyperparameter tuning (often, default values work well).
    
- It has beta_1 and beta_2 hyperparameters for the decay rates of the first and second moment estimates, respectively, in addition to a learning rate.
    

# Week 5: Optimisation, Training, and Generalisation

## Deep Learning As a Mathematical Optimization Process

Deep learning fundamentally involves training neural networks, which is an optimization process. The core idea is to find the optimal values for the network's parameters (weights and biases) that minimise a predefined cost function (or error function).

- Cost Function: This function quantifies how well (or poorly) the model performs on the training data.
    

- For regression tasks, the mean squared error (MSE) is a common choice.
    
- For classification tasks, cross-entropy loss is frequently used, penalising the model when it assigns a low probability to the correct target class.
    

- Gradient Descent: This is the generic optimization algorithm employed to iteratively tweak the model parameters to minimise the cost function. It measures the local gradient of the error function with respect to the parameters and takes a "step" in the direction of the steepest descent. The goal is to reach a point where the gradient is zero, indicating a minimum.
    
- Backpropagation: For neural networks, computing this "crazy complicated gradient" is done efficiently using the backpropagation algorithm.
    

- Backpropagation works by propagating the error gradient backwards through the network, from the output layer to the input layer. This information is then used to update each parameter in a gradient descent step.
    
- The algorithm computes how fast the error changes as hidden activities change, and from that, how fast the error changes with respect to individual connection weights.
    

- Non-convexity: A key challenge in training neural networks is that most interesting loss functions are non-convex. This means that gradient descent algorithms are not guaranteed to find the global minimum but rather drive the cost function to a very low value, often converging to a local minimum.
    

## Training Deep Neural Networks

- Weight Initialization: The initialization of connection weights is crucial, especially in deep neural networks, to prevent issues like vanishing or exploding gradients.
    

- Random Initialization: It is important to randomly initialise all hidden layers' connection weights to break symmetry. If all weights and biases were zero, all neurons in a layer would behave identically, making the network ineffective.
    
- Glorot (Xavier) Initialization: Proposed for sigmoid activation functions, it involves drawing weights from a distribution with mean 0 and a specific variance related to the average of the number of input and output connections (fan_avg).
    
- He Initialization: Similar to Glorot, but recommended for ReLU-like activation functions. Weights are drawn from a distribution with mean 0 and variance related to the number of input connections (fan_in).
    
- LeCun Initialization: Proposed in the 1990s, it is equivalent to Glorot initialization when fan_in = fan_out.
    

- Choices of Optimizers: The optimizer determines how the network's weights are updated based on the gradients.
    

- Gradient Descent Variants:
    

- Batch Gradient Descent (BGD): Computes the gradient using the entire training dataset before updating weights. This can be slow for large datasets and is sensitive to saddle points.
    
- Stochastic Gradient Descent (SGD): Estimates the error surface using only a single example at each iteration. This approach can navigate flat regions better but causes the error surface to be dynamic.
    
- Mini-Batch Gradient Descent (MBGD): A compromise between BGD and SGD, computing gradients on small groups of examples (mini-batches). This is widely used in deep learning.
    

- Advanced Optimizers (Adaptive Learning Rate Algorithms): These optimizers dynamically adjust the learning rate during training, improving convergence.
    

- Momentum-Based Optimization: This method accumulates a "velocity" vector of past gradients and uses it to accelerate learning, especially through flat regions and "ravines" in the error surface.
    
- AdaGrad: Adapts the learning rate for each parameter, scaling it down for parameters with large accumulated gradients and up for those with small ones.
    
- RMSProp: An improvement over AdaGrad that decays the accumulated gradients over time, preventing the learning rate from shrinking too quickly.
    
- Adam (Adaptive Moment Estimation): Combines the concepts of momentum and RMSProp by keeping track of both exponentially decaying averages of past gradients (first moment) and past squared gradients (second moment). Adam is a popular choice due to its effectiveness.
    
- AdaDelta: Another adaptive learning rate algorithm.
    

- Learning Rate: This is arguably the most important hyperparameter.
    

- A learning rate that is too small leads to slow convergence, while one that is too large can cause the algorithm to diverge.
    
- Learning rate schedules (e.g., exponential scheduling, 1cycle) involve gradually reducing or adjusting the learning rate during training for better convergence properties.
    

- Batch Size: The number of examples in a mini-batch. Large batch sizes can be processed efficiently by GPUs but may lead to training instabilities and less generalisable models; small batch sizes often result in better models.
    

## Avoiding Overfitting and Regularization

Overfitting occurs when a model performs exceptionally well on the training data but fails to generalise to new, unseen instances. Deep neural networks, with their vast number of parameters, are particularly prone to overfitting.

### Regularization Techniques

These are methods used to constrain the model and reduce its capacity to overfit the training data.

- L1 and L2 Regularization: These techniques add a penalty term to the cost function, discouraging large weights.
    

- L2 Regularization (Weight Decay): Adds a penalty proportional to the square of the weights. It typically leads to diffuse, small weight values.
    
- L1 Regularization: Adds a penalty proportional to the absolute value of the weights. It has the property of leading weight vectors to become sparse (i.e., very close to zero), effectively performing feature selection.
    

- Early Stopping: This involves monitoring the model's performance on a separate validation set during training. When the performance on the validation set stops improving (or starts to worsen), training is halted. This prevents the model from continuing to learn noise in the training data and overfitting.
    
- Dropout Regularization: A powerful technique where, at every training step, random subsets of neurons are "dropped out" (temporarily ignored and set to output 0) from one or more layers (excluding the output layer).
    

- This prevents neurons from relying too heavily on specific inputs or co-adapting too much, forcing the network to learn more robust features.
    
- The dropout rate (p) is a hyperparameter, typically between 10% and 50%.
    
- During inference, dropout is not applied; instead, the connection weights are scaled down by the keep probability (1-p) to compensate for the higher number of active neurons. This is often implemented as inverted dropout, where active neurons' outputs are divided by (1-p) during training, avoiding scaling at test time.
    
- Dropout can slow down convergence but often leads to better models.
    

- Batch Normalization (BN): This technique normalises the inputs to each layer within a mini-batch.
    

- It addresses the problem of internal covariate shift, where the distribution of activations in a deep network changes during training due to updates in previous layers. By normalising activations, BN makes the training process more stable and robust.
    
- Batch Normalization vastly accelerates the training process and often leads to better model performance.
    
- It typically happens before the nonlinearity (activation function) of a layer.
    
- During inference, the mean and standard deviation are estimated using exponentially weighted moving averages of the moments computed during training, rather than on individual mini-batches.
    

## Transfer Learning and Fine-tuning Models in Deep NNs

- Transfer Learning: This is a powerful technique where knowledge gained from training a model on one task is reused for a different, but related, task.
    
- Process: Instead of training a deep neural network from scratch, most of its pre-trained layers (typically the lower layers that learn general features) are reused, and only the top layers are adapted or newly added for the specific task.
    
- Benefits: Transfer learning significantly speeds up training and requires substantially less training data, especially for complex problems like image classification or natural language processing.
    
- Effectiveness: It works best with deep convolutional neural networks (CNNs) because their lower layers learn general feature detectors (like edges or curves) that are useful across many vision tasks.
    
- Applications: ImageNet challenge, where AlexNet pioneered the use of CNNs and revolutionized computer vision.
    
- Fine-Tuning Models: This is the process of further training the reused pre-trained layers, along with any newly added layers, on the new task's dataset.
    
- Selective Training: You can choose to freeze the weights of the pre-trained layers (keep them constant) and only train the new top layers, or you can unfreeze some of the pre-trained layers and fine-tune them with a very small learning rate.
    
- Example: In sentiment analysis, a pre-trained Universal Sentence Encoder can be fine-tuned by setting trainable=True for its KerasLayer, allowing its weights to be tweaked during training for the specific sentiment task.
    
- Unsupervised Pretraining: When labeled training data for a complex task is scarce, but abundant unlabeled data is available, unsupervised pretraining can be employed.
    
- Process: An unsupervised model (e.g., an autoencoder or a Generative Adversarial Network (GAN)) is trained on the large amount of unlabeled data to learn useful representations (embeddings).
    
- Reuse and Fine-tune: The lower layers of this unsupervised model (e.g., the encoder part of an autoencoder or the discriminator of a GAN) are then reused, an output layer for the specific task is added, and the entire network is fine-tuned using the smaller labeled dataset.
    
- Self-supervised Learning: A specific form of unsupervised pretraining where labels are automatically generated from the data itself. For example, masking words in text and training a model to predict the missing words can lead to a model that understands language well.
    
- Word Embeddings: Reusing pre-trained word embeddings (e.g., from Word2Vec framework or powerful language models) instead of one-hot vectors can yield far superior results in Natural Language Processing (NLP) tasks. These embeddings can be learned jointly with the sentiment analysis problem.
    
- Modern Resources: Platforms like TensorFlow Hub offer many pre-trained modules (e.g., sentence encoders) that can be easily integrated and fine-tuned.
    

# Week 6: Architectures, Interpretability, and Robustness

## Architectures of Deep Neural Networks

Deep Neural Networks (DNNs) have seen remarkable advancements through the development of specialised architectures, particularly in computer vision. Two prominent examples are VGGNet and ResNet, which introduced key concepts that shaped the field.

### Convolutional Neural Networks (CNNs) Foundation

- CNNs are a class of neural networks highly effective for image analysis, inspired by the structure of the human visual cortex.
    
- They learn hierarchical representations, building features from pixels to edges, motifs, parts of objects, and ultimately entire objects.
    
- Each layer in a CNN is responsible for learning and building up features from the input data it receives.
    
- Filters (Convolution Kernels): Neurons in a convolutional layer apply small weight matrices (filters) to local regions of the input (receptive fields) to detect specific patterns (e.g., vertical or horizontal lines).
    
- Feature Maps: A layer comprising neurons using the same filter generates a feature map, which highlights areas in the input that strongly activate that filter. Multiple filters lead to multiple feature maps, allowing the network to detect various features simultaneously.
    
- CNNs greatly reduce the need for manual feature selection, which was a cumbersome process in traditional computer vision.
    

### VGGNet

- Developed by the Visual Geometry Group (VGG) at Oxford University, VGGNet was the runner-up in the ILSVRC 2014 challenge.
    
- It features a simple and classical architecture that repeatedly stacks a few convolutional layers (typically 2 or 3, each followed by a ReLU activation) and then a pooling layer.
    
- This pattern is repeated, resulting in networks with a total of 16 or 19 convolutional layers, depending on the specific VGG variant.
    
- A defining characteristic of VGGNet is its use of small 3 √ó 3 filters, but with a large number of them.
    

### ResNet (Residual Network)

- ResNet, developed by Kaiming He et al., won the ILSVRC 2015 challenge.
    
- It was designed to address the problem of vanishing gradients and training difficulties in very deep networks.
    
- The core innovation is the skip connection (or residual connection), which allows the input of a block of layers to be added directly to its output. This creates an "identity shortcut" that helps the gradient flow more easily through the network, allowing for the training of much deeper architectures.
    
- A ResNet can be viewed as a stack of residual units (RUs), where each RU is a small neural network (e.g., two convolutional layers with Batch Normalization and ReLU) with an When the dimensions of the input and output feature maps differ due to downsampling (e.g., changing stride or number of filters), a 1x1 convolutional layer with a stride of 2 and an appropriate number of output feature maps is used in the skip connection to ensure shape compatibility.
    
- Examples include ResNet-34 (34 layers).
    

### Using Pretrained Models and Transfer Learning

- For complex tasks like large image classification, it's common to reuse parts of a pretrained state-of-the-art network rather than training from scratch. This technique is known as transfer learning.
    
- Transfer learning significantly speeds up training and requires substantially less training data.
    
- It is particularly effective with deep convolutional neural networks, as their lower layers learn general feature detectors (like edges or curves) that are useful across many vision tasks.
    
- Libraries like Keras allow easy loading of pretrained models, such as tf.keras.applications.ResNet50(weights="imagenet"), which downloads weights trained on the ImageNet dataset. These models often expect specific input image sizes (e.g., 224x224 pixels for ResNet-50).
    

## Interpretability of Deep NNs in Vision Problems

Interpretability refers to how easily one can inspect and explain a machine learning model's processes and outputs. While deep neural networks excel in performance, their inherent complexity often makes them challenging to interpret, especially in vision tasks.

### Challenges of Interpretability

- Deep models are generally very difficult to interpret due to their nonlinearities and massive number of parameters.
    
- This lack of transparency can hinder their adoption in high-stakes applications, such as medical diagnoses, where a doctor might require an explanation to confirm a model's conclusion.
    

### Insights from Feature Learning

- Hierarchical Features: Convolutional networks learn features in a hierarchical manner. Visualising the filters learned by early convolutional layers often reveals basic feature detectors (e.g., edges, simple curves).
    
- Representation Learning: Higher layers combine these basic features into more complex patterns. The ability of CNNs to learn effective lower-dimensional representations (embeddings) from raw pixel data demonstrates their "spectacular learning capabilities".
    
- Visualizing Embeddings: Techniques like t-SNE can visualize these learned embeddings, showing that images with similar semantic content (e.g., all pictures of boats or butterflies) cluster together in the embedding space, indicating that the network has learned meaningful information about the image content beyond just color.
    

### Explainability through Attention Mechanisms

- Attention mechanisms can significantly enhance the explainability of neural networks.
    
- By highlighting which parts of the input the model focused on when generating an output, attention provides a form of transparency.
    
- This can be a powerful debugging tool; for example, if an image classifier mislabels a dog in the snow as a "wolf," inspecting the attention map might reveal that the model focused heavily on the "snow" rather than the specific features of the "dog," suggesting a bias or a learned correlation.
    
- In some applications, explainability is a legal requirement, such as in systems making critical decisions like loan approvals.
    

### Sparsity for Interpretability

- In architectures like autoencoders, introducing sparsity (encouraging weights to be near zero or limiting the number of active neurons) in the hidden layers can make the learned representations more interpretable.
    
- If individual features learned by an autoencoder affect a smaller fraction of components, it becomes easier to understand how changes in input (e.g., adding strokes to a digit) affect the internal representation.
    

## Robustness and Adversarial Image Attacks.

The deployment of machine learning systems in real-world scenarios necessitates examining their robustness and reliability. A critical aspect of this is adversarial robustness, which deals with the model's resilience to intentionally crafted input perturbations known as adversarial attacks.

### Adversarial Robustness and Examples

- Adversarial Robustness is the ability of a machine learning system to withstand deliberate (test-time) perturbations of its inputs by an adversary intent on fooling the classifier. It exposes fundamental deficiencies in modern machine learning systems, especially those based on deep learning.
    
- Adversarial Examples are inputs that are subtly altered to cause a machine learning model to misclassify them, even though these alterations are often imperceptible to humans.
    
- For instance, a seemingly normal image of a pig can be modified with "airliner noise" to make a classifier confidently predict it as an "airliner".
    
- These vulnerabilities are not unique to deep neural networks but apply to virtually every machine learning algorithm.
    

### Why Adversarial Attacks Work

- Deep CNNs, while highly effective, may not recognise objects in the same way humans do.
    
- They tend to rely more on texture and color rather than the geometrical relationships between objects and their parts that humans use, which offers a more robust, viewpoint-independent recognition. This difference in learning can make CNNs vulnerable to small, humanly imperceptible perturbations.
    
- Neural networks often produce very linear decision functions, whereas for robustness, a model should ideally have more abrupt, "step-function"-like decision boundaries.
    

### Crafting Adversarial Examples (The "Inner Maximization Problem")

Creating adversarial examples involves solving an optimization problem to find minimal perturbations ($\delta$) that lead to misclassification.

1. Lower Bound (Empirical/Gradient-Based Search): The most common strategy involves empirically searching for an adversarial example using local gradient-based methods. This means iteratively adjusting the input in the direction that maximises the loss (or minimises the probability of the correct class).
    
2. Exact Solutions (Combinatorial Optimization): For some specific network architectures and activation functions, the problem can be formulated as a combinatorial optimization problem (e.g., Mixed Integer Programming) and solved exactly. However, this is computationally intensive and does not scale to large models.
    
3. Upper Bound (Convex Relaxations): This strategy involves simplifying the neural network structure into a convex relaxation that is easier to optimise. These methods don't typically produce actual adversarial examples for the original network but can provably certify a network's robustness against certain attack types. They are state-of-the-art for certified robust training.
    

### Defending Against Adversarial Attacks (The "Outer Minimization Problem" / Adversarial Training)

- The challenge is to train classifiers that are robust to adversarial attacks, which is an active research area.
    
- Adversarial Training is a primary defence mechanism. It involves modifying the training process to include adversarial examples. Specifically, for each original training input x, the model is also trained on its adversarial counterpart x + Œ¥ (where Œ¥ is the perturbation) to still correctly classify x.
    

- This typically helps the model resist the same kind of attack it was trained on.
    
- Advanced adversarial training often requires significant computational resources, including GPUs with CUDA.
    

### Transferability of Attacks

Adversarial examples can exhibit transferability, meaning an example crafted to fool one model might also successfully fool another model, even if the models have different architectures or were trained on different datasets. This phenomenon has been used to attack classifiers hosted by major cloud AI platforms.

### Beyond Security

Studying adversarial examples can offer insights into how to build more resilient and robust machine learning algorithms that handle ambiguity and unexpected inputs in a more human-like manner, even outside of direct adversarial scenarios.

**