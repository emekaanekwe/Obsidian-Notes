# Wk 2

![[FIT3181_5215-W2_Quiz.pdf]]


# Wk3

![[FIT3181_5215-L04-Quiz.pdf]]

### Calculate Output Tensor, given 3D tensor
*The same process applies to pooling*
![[Pasted image 20250910100303.png]]

### Calculate the Output Tensor Shape
![[Pasted image 20250910100731.png]]
$$Tensorshape = [CNN[0], Filters, H_{out}, W_{out}]$$

### Tensor Shape from Code

```Python
import torch
import torch.nn as nn

cnn = nn.Sequential(
    nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # Note: stride=2 (not 3)
    nn.ReLU(),
    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
    nn.ReLU()
)

x = torch.rand(1, 3, 224, 224)  # Input shape: [batch_size=1, channels=3, height=224, width=224]
print(cnn(x).shape)
```

Using $H_{out} \ and \ W_{out}$, the output shape before ReLu is [1,32,112,112]
Putting it through again, we get [1,64,112,112]

For **`Conv2d(), ReLu()`**
	input shape = $[1,3,224,224]$, output shape =  $[1*1, 3*32(first), 224/2, 224/2]$

For `BatchNorm(), MaxPool()`
	input shape = $[initial, Conv2d(width, pass2), size/4,size/4]$, output shape =  $[64, 64, 56, 56]$

### Calculate the # of Neurons

$$NeuronNum = Filters \times  W_{out} \times H_{out}$$

### Causes of Overfitting

#### Large Filter + Deep Models + Few Images

### Formal Structure of CNNs
![[Pasted image 20250910103524.png]]

$$\begin{matrix}  Input := [A1,B1,C1,D1] = [Total \ Layers, Preprocess \ Layers, Height, Width] \\ Filter := [F1,F2,F3,F4]=[Total \ Filters, Processed, Height, Width]\\ Pooling(Feature1) := [A2, B2, C2, D2] = [Total \ Features, Feature1 \ Layers \ \#, Width, Height] \\ Pooling(Feature2) := [A3, B3, C3, D3] = [Total \ Features, Feature2 \ Layers \ \#, Width, Height] \\ FC:= [E, F] = [Height, Width (4, 256)] \\ SoftMax := [G, H]=[\# \ of \ Neurons, \# \ of \ Classes ]    \end{matrix} $$

### Row 2 of CNN post-softmax

Assume a tensor shape `[4, 10]`:
    The first dimension (size 4) is the **batch size** (i.e., there are 4 images in the batch).
    The second dimension (size 10) is the **number of classes** (e.g., for a classification task with 10 categories).
Since the tensor is **after softmax**, each row represents the *prediction probabilities* for an image in the batch

### Purpose of Conv2D Tensors

#### 1. Dimension reduction
#### 2. Pattern detection filtering

### Purpose of Pooling Layers

#### 1. Preserve input integrity
#### 2. Reduce input size

### Purpose of Batch Norm Layers

#### 1. Mitigate Overfitting
#### 2. Get the Gaussian distro of training data and testing data




# Wk4

![[FIT3181-5215-L4-Quiz.pdf]]



### FF Networks: Optimization Problems

#### Concerning regularization $\Omega(\theta)$
$$min_𝜃 𝐽( 𝜃) = 𝛺 𝜃 + 1/N \sum^N_{i=1}CE(y_i,f(x;\theta))$$

$\theta$ is the weight and bias at a particular part of the loss
$\Omega (\theta)$ is the regularizer, i.e. sum of the eigenvalues to **prevent over fitting**, **makes the model simpler**, and **iterates the weights towards 0**

#### Concerning the empirical loss $\frac{1}{N} \sum^N_{i=1}CE(y_i,f(x;\theta))$
$$min_𝜃 𝐽( 𝜃) = 𝛺 𝜃 + 1/N \sum^N_{i=1}CE(y_i,f(x;\theta))$$
$f(x;\theta)$ is the prediction Pr, the empirical loss **improves model fitting to training set**, but **could over fit**

### Calculating GD

#### Update Rule
$$θt+1​=θt​−η⋅∇f(θt​)$$

Assume:        $f(θ)=θ^2−2θ+1$, learning rate: $0.1$, 
**compute** the gradient of $f(θ)$ with respect to $θ$:

We can treat the formula like a quadratic equation: $f(x) = x^2+x+c$
	get the derivative
	then **plug back into the update rule**
	calculate the derivative again if needed

### Calculating SGD

Assume: there are 4 indices
#### Update rule
$$θt+1​=θt​−η⋅∇f^​(θt​)$$
#### Mini Batch Loss
$$\hat{f}^​(θ)=\frac{1}{4}∑^{4}_{j=1}​(θ−i_j​)^2$$
1. plug in the values assumed in the batch loss
2. compute the gradient for $\hat{f}$
	$$∇f^​(θ)=​f'^​(θ)=\frac{1}{4}\times\frac{d}{dx}\sum$$
	which requires the derivative of each term. $$(θ−1​)^2+(θ−2)^2+(θ−3​)^2+(θ−4)^2$$
plug in the $\theta$ value and compute:$$...(θ=10−x_i)^2...$$
apply the SGD update by plugging in all the values:
$$θt+1​=θt​−η⋅∇f^​(θt​)$$

### $minL()$ Optimization Problem

#### Structure
$$min 𝜃 𝐿 𝐷; 𝜃 ≔ -\frac{1}{𝑁}\sum  \Delta  𝑙(𝑥_{i_k}, 𝑦{i_k}; 𝜃) $$
notice how the updating is **negative** for $x_k$ **batches**

### Gradient and BP via Code
Consider:
```Python
x = torch.tensor([1.0,2.0,3.0], requires_grad=False)
y = torch.tensor([4.0])
W = torch.rand(3,1, requires_grad=True)
b = torch.rand(1, requires_grad=True)

y_hat = torch.matmul(x,W)+b
l = (y_hat - y)**2

# BP
l.bakward(retain_graph=True)

```

1. Computes the gradient of loss for weight **and** bias
2. weight and bias both **store the training data** and **real values**
3. For BP, computes back to **previous weights** and goes **from $\hat{y}$ to x**

### Activation Functions in Code

#### Sigmoid
```Python
 1./(1+np.exp(-x))
```
#### ReLU
```Python
 x*(x>0)
```
#### tanH (hyperbolic)
```Python
 (np.exp(n)-1.)/(np.exp(n)+1.) 
```
#### softmax
```Python
 np.exp(x)/np.sum(np.exp(x))
```

### Calculating Activation Functions

#### Which is 𝜕ℎ 𝜕ℎ if ℎ = 𝜎(ℎ), ℎ is a vector and 𝜎 is an activation function?
$Diag(\sigma'(\hat{h}))$, or $I$

#### ReLU calculation
Simply reduce the matrix to diag 1