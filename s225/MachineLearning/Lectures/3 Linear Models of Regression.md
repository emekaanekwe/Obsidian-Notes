
---

# **Study Sheet: Linear Models for Regression**

#### **1. Core Concept: Regression vs. Classification**

*   **Regression:** Predicts a **continuous** target value (e.g., house price, age, weight).
*   **Classification:** Predicts a **discrete** class label (e.g., spam/ham, dog/cat).

# Model Structure
## 2. The Linear Regression Model

The model is **linear in its parameters**, not necessarily in its input features. This is the key definition.

*   **General Form:** $y(x, ğ°) = wâ‚€ + wâ‚Ï†â‚(x) + wâ‚‚Ï†â‚‚(x) + ... + w_MÏ†_M(x)$
    *   $ğ° = [wâ‚€, wâ‚, ..., w_M]áµ€$ is the *parameter* or *weight* vector.
    *   $Ï†_j(x)$ are *basis functions* or *feature functions*. They transform the input $x$: $$Ï†_j(x) = x^j$$ for polynomials, $$Ï†_j(x) = exp(\frac{-(x-Î¼_j)Â²}{2sÂ²})$$for Gaussians
    
	 The model's flexibility and ability to fit nonlinear data comes from these (often nonlinear) basis functions, while the learning problem remains tractable due to linearity in `ğ°`.

$y(x,w)=w_0+\sum_{j=0}^{M-1} w_j \phi_j (x) = w^T\phi (x)$
### Vector Form (Compact Notation):

   *   Define a feature vector $Ï†(x) = [1, Ï†â‚(x), Ï†â‚‚(x), ..., Ï†_M(x)]áµ€$. The `1` corresponds to the bias term `wâ‚€`.
   *   The model becomes: $y(x, ğ°) = ğ°áµ€Ï†(x)$

## 3. Learning the Parameters: The Objective Function

*The goal is to find the weight vector* `ğ°` *that best fits the training data* `{x_n, t_n}_{n=1}^N`.

### Sum-of-Squares Error Function
    $$J(ğ°) = (1/2) * Î£_{n=1}^N [t_n - ğ°áµ€Ï†(x_n)]Â²$$
	  This measures the "misfit" between the model's predictions `y(x_n, ğ°)` and the true targets $t_n$

#### Connection to Maximum Likelihood (ML)
  **Assumption:** Targets `t` are generated by the model with additive Gaussian noise: $t = y(x, ğ°) + Îµ$, where $Îµ ~ N(0, ÏƒÂ²)$.
    *   This implies: $p(t | x, ğ°, ÏƒÂ²) = N(t | y(x, ğ°), ÏƒÂ²)$
    *   The **likelihood** of the entire dataset (assuming i.i.d. data) is the product of these probabilities.
    *   **Key Result:** **Maximizing this likelihood is equivalent to minimizing the sum-of-squares error `J(ğ°)`.** This provides a rigorous probabilistic justification for using the squared error.

## 4. Optimizing the Objective: Finding the Best ğ°

### Method 

Set the gradient of the error function `âˆ‡J(ğ°)` to zero and solve for `ğ°`.
    *   $âˆ‡J(ğ°) = 0$ leads to a system of linear equations.

  **1. Closed-Form Solution (The Normal Equation):** $ğ°* = (Î¦áµ€Î¦)â»Â¹Î¦áµ€ğ­$
    *   `Î¦` is the *design matrix* (`N x M`), where *each row is the feature vector* $Ï†(x_n)áµ€$ for a data point.
    *   $ğ­ = [tâ‚, tâ‚‚, ..., t_N]áµ€$ is the *target vector*.
		**Pros:** Exact, one-step solution.
		**Cons:** Computationally expensive for large `M` (number of features). Calculating the matrix inverse $(Î¦áµ€Î¦)â»Â¹$ is an $O(MÂ³)$ operation.

**2. Iterative Solution: Gradient Descent (GD)**
*   Start with a random guess for `ğ°` and iteratively move in the direction of the steepest decrease of $J(ğ°)$.
*   **Update Rule:** $ğ°^{new} = ğ°^{old} - Î· âˆ‡J(ğ°^{old})$
    *   `Î·` is the *learning rate* (step size).
    *   $âˆ‡J(ğ°) = -Î£_{n=1}^N [t_n - ğ°áµ€Ï†(x_n)]Ï†(x_n)$ is the *gradient*.
#### Batch Gradient Descent 
Uses the **entire** training set to compute the gradient for each update. Computationally heavy per step.
##### Stochastic Gradient Descent (SGD) 
A more efficient variant.
    *   **Idea:** Approximate the true gradient using a **single, randomly chosen data point** (or a *mini-batch*).
    *   **Update Rule:** $ğ°^{new} = ğ°^{old} - Î· [t_n - ğ°áµ€Ï†(x_n)]Ï†(x_n)$
    *   **Pros:** Much *faster per update*. Can *escape shallow local minima* (crucial for non-convex problems in deep learning).
    *   **Cons:** *Noisy updates* mean the path to the minimum is not smooth. Requires careful tuning of `Î·`.

## 5. The Problem of Overfitting and Regularization

A model with *high flexibility* (many basis functions) can learn the noise in the training data, leading to poor performance on new data (overfitting).

### Solution 
Add a *regularization term* $Î» Î©(ğ°)$ to the error function to penalize large weight values.
    *   **New Objective:** $JÌƒ(ğ°) = J(ğ°) + Î» Î©(ğ°)$
    *   `Î»` is the *regularization parameter*: controls the trade-off between fitting the data (`J(ğ°)`) and keeping weights small (`Î©(ğ°)`).
    *   `Î©(ğ°)` is the *regularizer*.

*   **Choosing `Î»`:** Use *validation* (e.g., k-fold cross-validation). Train models with different `Î»` values and pick the one that performs best on a held-out validation set.

#### Common Regularizers:
 **L2 Regularization (Ridge Regression):** $Î©(ğ°) = (1/2)||ğ°||â‚‚Â² = (1/2)Î£ w_jÂ²$
        *   **Effect:** Tends to produce models with many small, non-zero weights. The solution remains in closed form: $ğ°* = (Î¦áµ€Î¦ + Î»I)â»Â¹Î¦áµ€ğ­$.
        *   **Properties:** Differentiable, efficient to optimize.
**L1 Regularization (Lasso):** $Î©(ğ°) = ||ğ°||â‚ = Î£ |w_j|$
        *   **Effect:** Tends to produce *sparse models*â€”it drives some weights **exactly to zero**, effectively performing **feature selection**. The features with non-zero weights are deemed the most important.
        *   **Properties:** Not differentiable at 0 (requires `sub-gradient` methods for optimization).

## 6. Key Terminology

*   **Basis/Feature Functions (`Ï†_j(x)`):** Non-linear transformations of the input that provide model flexibility.
*   **Design Matrix (`Î¦`):** Matrix of all feature vectors for all training points.
*   **Maximum Likelihood (ML):** A principle for parameter estimation that justifies the use of the sum-of-squares error.
*   **Gradient Descent (GD):** An iterative optimization algorithm.
*   **Stochastic GD (SGD):** A faster, noisy variant of GD using data subsets.
*   **Overfitting:** Modeling the noise in the training data, leading to poor generalization.
*   **Regularization:** Technique to prevent overfitting by penalizing model complexity.
*   **Ridge Regression:** Linear regression with **L2 regularization**.
*   **Lasso:** Linear regression with **L1 regularization**.

---
**How to Use This Sheet:**
1.  **Understand the Derivation:** Be able to explain why maximizing the likelihood under Gaussian noise leads to minimizing the sum-of-squares error.
2.  **Practice the Math:** Manually compute the closed-form solution `(Î¦áµ€Î¦)â»Â¹Î¦áµ€ğ­` for a tiny dataset (e.g., 2-3 points, 1-2 features).
3.  **Perform GD/SGD by Hand:** For a simple function (e.g., `J(w) = wÂ²`), calculate a few steps of GD and SGD to see how the parameter updates work.
4.  **Contrast L1 vs. L2:** Be able to explain the fundamental difference in the solutions they produce (dense vs. sparse weights) and why this happens.
5.  **Connect Concepts:** Link the idea of model flexibility (number of basis functions) to overfitting and see regularization as the solution.