
---

### **Assignment Overview**  
**Objective**: Assess understanding of model complexity, regularization, and probabilistic models through coding (Python/Jupyter) and theoretical analysis.  
**Submission**: 3 notebooks (1 per section) + PDF exports + oral interview.  
**Key Topics**:  
1. **KNN Regression & Cross-Validation** (Q1-Q3).  
2. **Ridge Regression with SGD** (Q4).  
3. **Logistic Regression vs. Bayes Classifier** (Q5).  

---

### **Section 1: Model Complexity & Selection (Q1-Q3)**  
#### **Question 1: KNN Regressor**  
**Task**:  
- Implement `KnnRegressor` (similar to KNN classifier but predicts mean of neighbors’ values).  
- Use `sklearn.datasets.load_diabetes()`, split 70% training data, and report training/test errors (MSE).  

**Solution Steps**:  
1. **Implement KNN Regressor**:  
   ```python
   from sklearn.base import BaseEstimator
   from sklearn.metrics import mean_squared_error
   import numpy as np

   class KnnRegressor(BaseEstimator):
       def __init__(self, k=5):
           self.k = k

       def fit(self, X, y):
           self.X_train = X
           self.y_train = y
           return self

       def predict(self, X):
           distances = np.sqrt(((self.X_train - X[:, np.newaxis])**2).sum(axis=2))
           nearest_indices = np.argsort(distances, axis=1)[:, :self.k]
           return np.mean(self.y_train[nearest_indices], axis=1)
   ```  
2. **Test Implementation**:  
   ```python
   from sklearn.datasets import load_diabetes
   from sklearn.model_selection import train_test_split

   data = load_diabetes()
   X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.7)
   model = KnnRegressor(k=5).fit(X_train, y_train)
   print("Test MSE:", mean_squared_error(y_test, model.predict(X_test)))
   ```

**Key Points**:  
- Use Euclidean distance (`np.sqrt`).  
- For ties, `np.mean` automatically handles averaging.  

---

#### **Question 2: L-Fold Cross-Validation**  
**Task**:  
1. Implement `LFold` class (similar to `sklearn.model_selection.KFold`).  
2. Use it to find optimal `K` for KNN (1 to 30) on diabetes data, plotting MSE vs. `K` with 95% confidence intervals.  

**Solution Steps**:  
1. **Implement LFold**:  
   ```python
   class LFold:
       def __init__(self, L=5):
           self.L = L

       def split(self, X, y=None):
           indices = np.arange(len(X))
           fold_size = len(X) // self.L
           for i in range(self.L):
               test_mask = (indices >= i * fold_size) & (indices < (i + 1) * fold_size)
               yield indices[~test_mask], indices[test_mask]
   ```  
2. **Find Optimal K**:  
   ```python
   from matplotlib import pyplot as plt

   ks = range(1, 31)
   mse_train, mse_test = [], []
   for k in ks:
       model = KnnRegressor(k=k)
       mse_folds = []
       for train_idx, test_idx in LFold(5).split(X_train):
           model.fit(X_train[train_idx], y_train[train_idx])
           mse_folds.append(mean_squared_error(y_train[test_idx], model.predict(X_train[test_idx])))
       mse_test.append(np.mean(mse_folds))
   plt.errorbar(ks, mse_test, yerr=1.96 * np.std(mse_test) / np.sqrt(5))
   ```  

**Key Points**:  
- **Overfitting**: Low `K` → High variance (small `K` fits noise).  
- **Underfitting**: High `K` → High bias (ignores local patterns).  

---

#### **Question 3: Automatic K Selection**  
**Task**:  
1. Implement `KnnRegressorCV` that internally uses `LFold` to choose best `K` from a list.  
2. Test with nested CV (outer loop evaluates chosen `K`).  

**Solution**:  
```python
class KnnRegressorCV(KnnRegressor):
    def __init__(self, ks=range(1, 21), cv=LFold(5)):
        self.ks = ks
        self.cv = cv

    def fit(self, X, y):
        best_k, best_mse = None, float('inf')
        for k in self.ks:
            mse = 0
            for train_idx, test_idx in self.cv.split(X):
                self.k = k
                self.fit(X[train_idx], y[train_idx])
                mse += mean_squared_error(y[test_idx], self.predict(X[test_idx])))
            if mse < best_mse:
                best_k, best_mse = k, mse
        self.k = best_k
        return super().fit(X, y)
```

**Key Points**:  
- Nested CV avoids data leakage.  
- Report mean `K` selected across outer folds.  

---

### **Section 2: Ridge Regression (Q4)**  
#### **Question 4: Ridge Regression with SGD**  
**Task**:  
1. Derive SGD update for Ridge (L2 penalty).  
2. Implement Ridge Regression using SGD.  
3. Study effect of `λ` on synthetic data (`cos(3πx)` + noise).  

**Solution Steps**:  
1. **Derivation**:  
   - Loss: \( E(w) = \frac{1}{2} \sum (y_i - w^T \phi(x_i))^2 + \frac{\lambda}{2} \|w\|^2 \).  
   - Gradient: \( \nabla E(w) = -\phi(x_i)(y_i - w^T \phi(x_i)) + \lambda w \).  
   - SGD Update: \( w \leftarrow w - \eta (\lambda w - \phi(x_i)(y_i - w^T \phi(x_i))) \).  

2. **Implementation**:  
   ```python
   class RidgeRegression:
       def __init__(self, lambda_=0.1, eta=0.01, epochs=1000):
           self.lambda_ = lambda_
           self.eta = eta
           self.epochs = epochs

       def fit(self, X, y):
           self.w = np.zeros(X.shape[1])
           for _ in range(self.epochs):
               i = np.random.randint(len(X))
               grad = -X[i] * (y[i] - X[i] @ self.w) + self.lambda_ * self.w
               self.w -= self.eta * grad
           return self
   ```  

3. **Effect of `λ`**:  
   - Plot `log(MSE)` vs. `log(λ)` for polynomial features (degree=5).  
   - **Underfitting**: High `λ` → Large bias (weights shrunk too much).  
   - **Overfitting**: Low `λ` → High variance.  

---

### **Section 3: Logistic Regression vs. Bayes (Q5)**  
#### **Question 5: Model Comparison**  
**Task**:  
1. Compare `LogisticRegression` and `BayesClassifier` on `breast_cancer` data.  
2. Plot learning curves (train/test error vs. dataset size `N=5,...,500`).  

**Solution Steps**:  
1. **Implement Bayes Classifier**:  
   ```python
   from sklearn.naive_bayes import GaussianNB
   bayes = GaussianNB().fit(X_train, y_train)
   ```  

2. **Learning Curves**:  
   ```python
   ns = range(5, 501, 5)
   for n in ns:
       X_train, _, y_train, _ = train_test_split(X, y, train_size=n, shuffle=True)
       logistic = LogisticRegression().fit(X_train, y_train)
       bayes = GaussianNB().fit(X_train, y_train)
       # Record errors for each model
   plt.plot(ns, logistic_errors, label="Logistic")
   plt.plot(ns, bayes_errors, label="Bayes")
   ```  

**Key Points**:  
- **Bayes**: Better for small `N` (strong prior assumptions).  
- **Logistic**: Better for large `N` (flexible, data-driven).  

---

### **General Tips**  
1. **Code Structure**:  
   - Use markdown cells for explanations.  
   - Separate sections clearly (`# Question 1`, `## Part I`).  
2. **Interview Prep**:  
   - Be ready to explain your code and derivations.  
   - Practice defining terms like "bias-variance tradeoff".  
3. **Avoid Penalties**:  
   - Submit all files with correct names.  
   - No late submissions!  

---

### **Next Steps**  
1. **Clarify**: Any specific part needing deeper explanation?  
2. **Code**: Need full implementations for any question?  
3. **Theory**: Want to rehearse interview questions?  