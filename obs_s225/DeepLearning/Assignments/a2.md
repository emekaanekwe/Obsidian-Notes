# Part 4: Transformers

## Fixes to Accuracy

### Critical Issues Identified

### 1. **Data Loading Problem**
Your `DataManager` doesn't create proper attention masks for BERT, and the input format doesn't match what your prefix tuning model expects.

**Fix:**
```python
class FixedDataManager(DataManager):
    def manipulate_data(self):
        # Your existing tokenization code...
        # After padding, create attention masks
        self.attention_masks = (self.numeral_data != 0).long()
    
    def train_valid_test_split(self, train_ratio=0.8, test_ratio=0.1):
        # Your existing split code...
        # Create datasets with attention masks
        train_set = torch.utils.data.TensorDataset(
            train_set_data, 
            self.attention_masks[data_indices[:train_size]],  # Add attention masks
            train_set_labels
        )
        # Do same for valid and test sets
```

### 2. **Model Architecture Issues**
Your prefix tuning model has problems with attention mask handling and BERT integration.

**Fix - Simplified Working Version:**
```python
class WorkingPrefixTuning(nn.Module):
    def __init__(self, vocab_size, num_classes, prefix_length=10, embed_dim=768):
        super().__init__()
        self.prefix_length = prefix_length
        self.embed_dim = embed_dim
        
        # Simple embedding layer instead of BERT for now
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # Prefix embeddings
        self.prefix_embeddings = nn.Parameter(
            torch.randn(prefix_length, embed_dim) * 0.02
        )
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=8,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)
        
        # Classifier
        self.classifier = nn.Linear(embed_dim, num_classes)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, input_ids):
        batch_size = input_ids.size(0)
        
        # Get word embeddings
        embeddings = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]
        
        # Add prefix
        prefix_batch = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        combined = torch.cat([prefix_batch, embeddings], dim=1)
        
        # Transformer expects [seq_len, batch_size, embed_dim] for nn.TransformerEncoder
        combined = combined.transpose(0, 1)  # [seq_len, batch_size, embed_dim]
        
        # Create mask for transformer
        seq_len = combined.size(0)
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)
        
        # Pass through transformer
        encoded = self.transformer(combined, mask=mask)
        encoded = encoded.transpose(0, 1)  # Back to [batch_size, seq_len, embed_dim]
        
        # Pooling (use only the prefix tokens for classification)
        pooled = encoded[:, 0, :]  # Use first token (like BERT's [CLS])
        
        # Classification
        logits = self.classifier(self.dropout(pooled))
        return logits
```

### 3. **Training Loop Fix**
Your trainer has issues with loss calculation and evaluation.

**Fix:**
```python
class FixedTrainer:
    def __init__(self, model, criterion, optimizer, train_loader, val_loader):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.train_loader = train_loader
        self.val_loader = val_loader
    
    def fit(self, num_epochs):
        best_acc = 0
        for epoch in range(num_epochs):
            # Training
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (data, masks, labels) in enumerate(self.train_loader):
                data, labels = data.to(device), labels.to(device)
                
                self.optimizer.zero_grad()
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
            
            train_acc = 100. * correct / total
            train_loss = total_loss / len(self.train_loader)
            
            # Validation
            val_acc, val_loss = self.validate()
            
            if val_acc > best_acc:
                best_acc = val_acc
                torch.save(self.model.state_dict(), 'best_model.pth')
            
            print(f'Epoch: {epoch+1:03d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, masks, labels in self.val_loader:
                data, labels = data.to(device), labels.to(device)
                outputs = self.model(data)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        acc = 100. * correct / total
        loss = total_loss / len(self.val_loader)
        return acc, loss
```

### 4. **Quick Working Solution**
Since you need immediate results, here's a minimal working version:

```python
# Quick fix - Use a simpler model first
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, num_classes, embed_dim=256):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=8, dim_feedforward=1024, 
            dropout=0.1, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)
        self.classifier = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        # x: [batch_size, seq_len]
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        encoded = self.transformer(embedded)
        pooled = encoded.mean(dim=1)  # Average pooling
        return self.classifier(pooled)

# Usage
simple_model = SimpleTransformer(
    vocab_size=dm.vocab_size,
    num_classes=dm.num_classes,
    embed_dim=256
).to(device)

# Training with better hyperparameters
optimizer = torch.optim.AdamW(simple_model.parameters(), lr=1e-4, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# Train for more epochs
trainer = FixedTrainer(
    model=simple_model,
    criterion=nn.CrossEntropyLoss(),
    optimizer=optimizer,
    train_loader=dm.train_loader,
    val_loader=dm.valid_loader
)

trainer.fit(num_epochs=100)
```

### 5. **Additional Improvements**

**Data Preprocessing:**
```python
# Ensure proper data normalization
def improve_data_loading():
    # Add this to your DataManager
    print(f"Dataset stats: {len(dm.str_questions)} samples, {dm.num_classes} classes")
    print(f"Class distribution: {np.bincount(dm.numeral_labels)}")
    
    # Check for class imbalance
    if np.min(np.bincount(dm.numeral_labels)) < 10:
        print("Warning: Severe class imbalance detected!")
```

**Hyperparameter Tuning:**
```python
# Better learning rate finder
def find_learning_rate():
    lrs = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]
    # Test each briefly and choose best
```

### **Expected Results After Fixes:**

- **With simple transformer**: 85-90% accuracy
- **With proper BERT integration**: 92-97% accuracy
- **Current issues fixed**: Should jump from 12% to at least 85%

The main problem is that your prefix tuning model isn't properly integrated with BERT and the data loading doesn't match the model's expectations. Start with the simple transformer version first to verify everything works, then gradually add complexity.