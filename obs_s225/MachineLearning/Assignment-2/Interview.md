
# During the interview, you must demonstrate:

### ‚Ä¢ The ability to clearly explain your work for all questions (ie both coding and non-codingquestions), including:

#### ‚Äì The code architecture, eg logic flow, structural decisions, and implementation details

#### ‚Äì A comprehensive understanding of the underlying concepts, algorithms, and techniques applied in your solution for coding and non-coding questions.

#### ‚Ä¢ The capacity to justify your answer/design choices, and the capacity for analysis when presented with **alternative approaches**.

### ‚Ä¢ Here are some sample questions: 
#### (i) Can you explain in simple terms how the Expectation-Maximization (EM) algorithm works for document clustering? 
#### (ii) Based on your plots from Question 2, why does the 3-layer neural network produce a more complex decision boundary compared to the perceptron? 
#### (iii) How does increasing the number of hidden units affect the model‚Äôs capacity and generalization? 
#### (iv) What is the purpose of using an autoencoder in self-taught learning?


---

# ‚úÖ 1. Latent Variables

### üöÄ Simple Definition

Latent variables are **hidden variables** that we don't observe directly but assume exist to explain the observed data.

### üìö Example in Document Clustering

- We observe documents
    
- We **don‚Äôt observe** their cluster labels
    
- Those cluster assignments are latent variables
    

### üß† Interview-friendly explanation

> Latent variables represent hidden structure in data.  
> In document clustering, the topic/cluster assignment for each document is unknown, so it‚Äôs treated as a latent variable.  
> EM helps estimate those hidden assignments and model parameters together.

---

# ‚úÖ 2. EM Algorithm + Hard vs Soft EM

### üöÄ Core Idea

EM solves maximum likelihood problems when data has missing/latent parts.

### üß† Two-step loop

**E-step**: Estimate latent variables (responsibilities) using current parameters  
[  
$\gamma_{nk} = p(z_k=1 | x_n, \theta^{old})$
]

**M-step**: Update parameters using those expectations  
[  
$\theta^{new} = \arg\max_{\theta} \sum_{n,k} \gamma_{nk} \ln p(x_n, z_{nk} | \theta)$
]

### üéØ Why we need EM

> We can't directly maximize the log-likelihood because it contains a log-sum term from latent variables.  
> EM replaces unknown latent assignments with their expected values, making optimization tractable.

---

## Hard vs Soft EM

| Soft EM                                                      | Hard EM                                               |
| ------------------------------------------------------------ | ----------------------------------------------------- |
| Uses **probabilistic** cluster membership (responsibilities) | Assigns each doc to **one cluster** directly (argmax) |
| Expected counts                                              | Hard counts                                           |
| More stable                                                  | Can converge faster but less accurate                 |
| Used for mixture models                                      | Equivalent to K-means in Gaussian case                |

### Interview Script

> Soft EM assigns fractional membership, while Hard EM forces each sample to a single cluster.  
> Soft EM usually converges more smoothly and captures uncertainty; hard EM can be faster but less stable.

---

# ‚úÖ 3. Document Clustering (Multinomial Mixture)

### üöÄ Model assumptions

- Each document generated by one cluster
    
- Within cluster, words follow a multinomial distribution
    

### ‚úÖ Parameters

- $\varphi_k$ (cluster priors)
    
- $\mu_{wk}$ (word probability in cluster k)
    

### ‚úÖ E-step

[  
$\gamma_{nk} = \frac{\varphi_k \prod_{w} \mu_{wk}^{c(n,w)}}{\sum_j \varphi_j \prod_{w} \mu_{wj}^{c(n,w)}}$  
]

### ‚úÖ M-step

[  
$\varphi_k = \frac{1}{N} \sum_n \gamma_{nk}$  
]

[  
$\mu_{wk} = \frac{\sum_n \gamma_{nk} c(n,w)}{\sum_{w'} \sum_n \gamma_{nk} c(n,w')}$  
]

---

# ‚úÖ 4. Perceptron Training

### üöÄ Core idea

Linear classifier:  
[  
$y = \text{sign}(w^T x)$  
]

### üß† Learning rule

[  
$w = w + \eta (y_{true} x) \quad \text{only if misclassified}$  
]

### Key Interview Points

- No output probability ‚Äî it's a rule-based update
    
- Only updates when a mistake happens
    
- Decision boundary must be **linear**
    

---

# ‚úÖ 5. Autoencoders + Self-Taught Learning

### üß† Purpose

Learn **useful representations** from unlabeled data.

### üöÄ 2 phases

1. **Train autoencoder** on unlabeled + labeled data  
    [  
$h = f(Wx)$
$\hat{x} = g(W'h)$
    ]
    
2. Extract hidden features + augment supervised training
    

### Why it works

> Autoencoders extract structure from data.  
> When we add these learned features to the classifier, it improves generalization ‚Äî especially with limited labeled data.

---

# ‚úÖ 6. 3-Layer Neural Networks

### Architecture

Input ‚Üí Hidden ‚Üí Output  
Hidden layer has nonlinear activations (ReLU, tanh)

### Why more powerful than perceptron

> Hidden layers let neural networks learn **non-linear decision boundaries**, enabling them to separate data that perceptrons cannot.

---

# ‚úÖ 7. Effect of Hidden Units on Generalization

|Few Hidden Units|Many Hidden Units|
|---|---|
|Low capacity|High capacity|
|Underfitting risk|Overfitting risk|
|Smooth boundaries|Complex boundaries|

### Interview line

> Increasing hidden units increases capacity.  
> Up to a point it improves accuracy, then it hurts generalization without regularization.

---

# ‚úÖ 8. Explain Your Code ‚Äî Template

**Your interviewer expects:**

‚úî Logic flow  
‚úî Parameter updates  
‚úî Why you chose certain designs  
‚úî Data handling  
‚úî Numerical stability (log-sum-exp trick)

### Suggested script

> I preprocess text into a document-term matrix, apply EM updates iteratively using log-sum-exp for numerical stability, and compare hard vs soft responsibilities.  
> For the perceptron, I test hyperparameters with early stopping.  
> For neural networks, I evaluate different hidden sizes and L2 values, then visualize decision boundaries.  
> For autoencoders, I train on both labeled and unlabeled data, extract bottleneck features, and show that adding them improves classification performance.

---

# üé§ Practice Questions & Answers

| Question                                | Strong Answer                                                                                                                                 |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| "What is a latent variable?"            | A hidden variable that explains the observed data structure, like topic membership in documents.                                              |
| "Why do we need EM?"                    | Because the log-likelihood contains a log-sum over latent variables; EM replaces missing assignments with expectations to optimize tractably. |
| "Difference between perceptron and NN?" | Perceptron is linear; NN uses hidden layers to learn nonlinear boundaries.                                                                    |
| "Why autoencoders?"                     | To learn useful features from unlabeled data; improves generalization in low-label settings.                                                  |
| "Effect of hidden units?"               | Increases capacity; too many causes overfitting without regularization.                                                                       |

---

# ‚úÖ Next Step

Send the three notebook parts, and I will:

- Generate tailored interview bullets per section
    
- Provide ‚Äúexplain this code‚Äù scripts
    
- Give you professor-style practice questions & answers
    
- Evaluate if any part of your logic/explanations needs strengthening
    

üìé **Upload Part 1 when ready**