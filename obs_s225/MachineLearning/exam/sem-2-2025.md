Exam Description 
The total time to complete the exam is 130 minutes and you are expected to complete all questions. Hence, as a rule of thumb, you
should aim to invest no more minutes into a question than two times its marks. For example, a question with 10 marks is designed to
be completed in about 20 minutes. This approach leaves you an additional 10 minutes as buffer time for the overall exam. The exam
is closed book. 
How to use this collection to practice: 
It is fundamentally important that you attempt every question alone without help before you refer to the teaching material, discuss
with your peers, or even look at the separately published reference solutions. If you are like most students (and teachers) this
approach will be scary because it requires you to confront your gaps in knowledge. However, this is exactly why it will lead you to a
good exam performance (very much in contrast to just scanning the solutions). It gives yourself the opportunity to really understand
what you do not understand before looking for help. This way, once you receive the information to fill t he gap, you will connect it
better with the other things you know, causing it to be available to you under exam conditions. 
Elements of Statistical Learning 
1 Classification versus Regression [ 3 marks] 
What is the statistical supervised learning problem on a high level and what is difference between a classification and a regression
problem? Answer in two to three sentences. 
2 Goal of Supervised Learning [3 marks] 
What is the ultimate goal of statistical supervised learning? How are the concepts of training error and test error related to this goal?
Answer in three to four sentences. 
3 Model Selection [6 marks] 
Consider the following scenario: • Alice wants to model the species (‘setosa’, ‘versicolor’, or ‘virginica’) of iris flowers as a function of
four variables (their sepal and petal length and width) • She has collected a dataset of 150 examples • She wants to use the kNN
classifier but she does not know what is a suitable value for k, hence shes want to choose k from the set of candidates {1, 2, ..., 10}
based on the collected data • Finally, she wants a reliable estimate of the performance of the model that has been learned. 
Currently Alice plans to proceed with the following machine learning workflow: 1. Split the data into 10 folds of roughly equal size. 2.
Pick the value of k with the second best average test error across all folds (for each using the remaining folds as training data). In
particular, she plans to use the second best “to avoid overfitting”. 3. Use this test error as the final performance estimate. 
Answer the following two questions: (a) Point out in up to two sentences, what is the most substantial problem with Alice’s proposed
workflow and why. (b) Describe an improved machine learning process that adequately addresses this problem. 
4 Normal Distribution and Maximum Likelihood Estimation [10 marks] 
The (uni-variate) normal distribution is an extremely important distribution describing the behaviour of continuous random variables.
It is parameterised by a mean μ and a standard deviation parameters σ (or more typically by the corresponding variance parameter
σ2 ). Given a dataset {x1 , ... , xN } of independent realisations of a normal random variable X, we can use the principal of maximum
likelihood to find guesses for the unknown parameters. In particular, these guesses have simple closed form solutions. 
Answer each of the following questions with one to two sentences and give mathematical derivations as appropriate. (a) What is the
definition of the normal density function p(x|μ, σ)? What is the key component of the definition that gives rise to the characteristic
bell shape? (b) What is the key idea of the maximum likelihood estimation of the parameters μ and σ, i.e., what is the defining
property of the maximum likelihood estimates μ{ML} and σ{ML} . © How can we derive the closed form solution of the maximum
likelihood estimation for the mean μ? Apply this approach to derive it. (d) How can we derive the closed form solution of the
maximum likelihood estimation for the standard deviation σ? Apply this approach to derive it. 
Linear Regression 
5 Derivation of Squared Error [6 marks] 
For fitting the model parameters w of a linear regression model, we used the approach to minimise the squared error: E(w) = \frac{1
{2}\sum^{N}_{n=1}(t_n-y_n)2 
where {(x1, t1 ), ... , (xN , tn)} is the given training data and yn = \sum^{p}_{i=1}wiφi (xn) are the model predictions. To justify this error
function, we showed that it can be derived as a maximum likelihood parameter estimation for a probabilistic model p(t|x, w). Answereach of the following questions with one to two sentences (including mathematical equations as appropriate). (a) What is the form of
the probabilistic model that we assumed for the regression problem, i.e., how are the target values generated given the input
vectors? (b) What is the likelihood function corresponding to this model? © Why is maximising this likelihood function equivalent to
minimising the squared error function? 
Linear Classification 
6 Logistic Regression [8 marks] 
When using the logistic regression model for binary classification, we model the probability of the positive class (t = 1) given input x
via the sigmoid transformation σ of a linear function w · x of model parameters w. (a) Give the log likelihood function log p(t|x, w) of
the logistic regression model for a single data point (x, t). Hint: We used the fact that we encode the positive class with t = 1 and the
negative class with t = 0 to give a compact formula. (b) As a step towards the gradient descent algorithm for logistic regression,
derive the partial derivative of the negative log likelihood (error function) − log p(t|x, w) with respect to parameter w_i . Derive the
result in individual steps, noting what results you are using (all correct steps give partial marks). © Extend your result from part (b) to
the full gradient of the negative log likelihood when observing a set of n training data points {(x1, t1 ), ... , (xN , tn)}. 
Latent Variable Models 
7 Document Clustering Model [9 marks] 
Suppose we are given a collection of documents D. The data set D is represented as {x1 , x2, x3 , ..., xN } where x_i is a d-dimensional
“count vector” representing the i-th document, based on bag-of-words and with respect to a word vocabulary of size d. We are
interested in fitting a Mixture multinomial model onto this dataset. (a) An individual cluster is described by a vector of word
occurrence probabilities μ where μ_j describes the probability of a word in a document to be the j-th word in the vocabulary. Give a
formula of the probability p(x|μ) of a count vector x given word occurrence probabilities μ and give a brief explanation of the formula
(one to two sentences). Hint: remember that, for simplicity, we assumed the individual counts to be independent. (b) Write down the
“Q-function”, which is the basis of the Expectation-Maximization (EM) algorithm for maximizing the log-likelihood. Notice that you do
not need to write the EM algorithm in this part. © Write down the “hard” as well as the ”soft” Expectation-Maximization (EM)
algorithm for estimating the parameters of the model. If necessary, provide enough explanation to under- stand the algorithm that
you have written. Also briefly explain what is the main difference between hard and soft EM. 
Neural Networks 
8 Forward and Backward Propagation (9 marks) 
Given a neural network f(·) and a dataset D = {(x1, y1), (x2 , y2), ..., (xn , y n)} where xi is a 2-dimensional vector and yi is a scalar value
which represents the target. {w1, w2 , ..., wn } are learnable parameters. h represents a linear unit. For example ti = h_1w_7 +h_2w_8.
The error function for training this neural network is the sum of squared error: E(w)=\frac{1}{2}\sum^{N}_{n=1}(yi-ti)2, 
x1 x2 x3
│ │ │

w1─┼─w2 w3─┼─w4 w5─┼─w6 │ │ │ │ │ ▼ ▼ ▼ ▼ ▼ h1 h2 │ │ w7─┼─────w8─┘ │ │ │ w9 (Bias) ▼ t 
(a) Suppose we have a sample x, where x1 =0.5, x2 =0.6, x3 =0.7. The network parameters are w1 =2, w2 =3 w3 =2, w4 =1.5 w5 =3, w6
=4 w7 =6, w8 =3 Next, let’s suppose the target value y for this example is 4. Write down the forward steps and the prediction error
for this given sample. Hint: you need to write down the detailed computational steps. (b) Given the prediction error in the previous
question, calculate the gradient of w1, namely \frac{∂E}{∂w1}. Please also write down all involved derivatives